{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90b985bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.10 (main, Apr 17 2025, 03:50:21) [Clang 17.0.0 (clang-1700.0.13.3)]\n",
      "NumPy version: 2.2.5\n",
      "pandas version: 2.2.3\n",
      "matplotlib version: 3.10.1\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization libraries\n",
    "from pyfonts import load_font\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "mpl.style.use('ggplot')\n",
    "sns.set_style('white')\n",
    "plt.rcParams['figure.figsize'] = 12,8\n",
    "palette = sns.color_palette(\"viridis\", as_cmap=True)\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "print(\"Python version: {}\". format(sys.version))\n",
    "print(\"NumPy version: {}\". format(np.__version__))\n",
    "print(\"pandas version: {}\". format(pd.__version__))\n",
    "print(\"matplotlib version: {}\". format(mpl.__version__))\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print('-'*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c8db72",
   "metadata": {},
   "source": [
    "### To replicate\n",
    "1. Change the value of `DATA_FOLDER` to the location you will download the sources to.\n",
    "2. Download each source independently using the `File used:` and other information mentioned in this notebook and dataset.\n",
    "3. Run this notebook to process each individual source and compile into one CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bce3d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = '../../data/distributions/check'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6741277c",
   "metadata": {},
   "source": [
    "### Normal Distribution\n",
    "\n",
    "SOCR height and weight dataset from University of Michigan\n",
    "\n",
    "- Source: https://wiki.socr.umich.edu/index.php/SOCR_Data_Dinov_020108_HeightsWeights\n",
    "- Download: https://www.kaggle.com/datasets/burnoutminer/heights-and-weights-dataset\n",
    "- File used: SOCR-HeightWeight.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca3ab64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_FOLDER, 'SOCR-HeightWeight.csv'))\n",
    "\n",
    "df.rename(columns={'Height(Inches)': 'norm_height_inch', 'Weight(Pounds)': 'norm_weight_lbs'}, inplace=True)\n",
    "df = df[['norm_height_inch', 'norm_weight_lbs']]\n",
    "\n",
    "df = df[:25000]\n",
    "\n",
    "df.to_csv(os.path.join(DATA_FOLDER, 'normal_distribution_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ff6dea",
   "metadata": {},
   "source": [
    "### Uniform Distribution\n",
    "\n",
    "Lottery Take 5 Winning Number dataset from Data.gov\n",
    "\n",
    "- Source: https://catalog.data.gov/dataset/lottery-take-5-winning-numbers\n",
    "- Download: https://data.ny.gov/api/views/dg63-4siq/rows.csv?accessType=DOWNLOAD\n",
    "- Terms of Use: https://data.ny.gov/download/77gx-ii52/application/pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcc69887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      uniform_draw_date  uniform_winning_number  uniform_draw_position\n",
      "0            09/24/2020                       2                      1\n",
      "1            09/24/2020                       5                      2\n",
      "2            09/24/2020                      10                      3\n",
      "3            09/24/2020                      15                      4\n",
      "4            09/24/2020                      18                      5\n",
      "...                 ...                     ...                    ...\n",
      "51115        04/21/2025                      13                      1\n",
      "51116        04/21/2025                      26                      2\n",
      "51117        04/21/2025                      29                      3\n",
      "51118        04/21/2025                      30                      4\n",
      "51119        04/21/2025                      37                      5\n",
      "\n",
      "[51120 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "lottery_take_5_df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        DATA_FOLDER, 'Lottery_Take_5_Winning_Numbers__Beginning_1992.csv')\n",
    ")\n",
    "\n",
    "\n",
    "# Function to transform a single row into multiple rows for white balls\n",
    "def transform_row(row):\n",
    "    draw_date = row['Draw Date']\n",
    "    winning_numbers = row['Evening Winning Numbers'].strip().split()\n",
    "    transformed_data = []\n",
    "    for i, number in enumerate(winning_numbers):\n",
    "        transformed_data.append({\n",
    "            'Draw_Date': draw_date,\n",
    "            'White_Ball': int(number),\n",
    "            'Draw_Position': i + 1\n",
    "        })\n",
    "    return transformed_data\n",
    "\n",
    "\n",
    "# Apply the transformation to each row of the DataFrame\n",
    "transformed_list = lottery_take_5_df.apply(transform_row, axis=1).tolist()\n",
    "\n",
    "# Flatten the list of lists into a single list of dictionaries\n",
    "flat_transformed_list = [\n",
    "    item for sublist in transformed_list for item in sublist]\n",
    "\n",
    "# Create a new DataFrame from the flattened list\n",
    "uniform_distribution_df = pd.DataFrame(flat_transformed_list)\n",
    "\n",
    "# uniform_distribution_df.rename(\n",
    "#     columns={\n",
    "#         'Draw_Date': 'lt5_draw_date', 'White_Ball': 'lt5_winning_number',\n",
    "#         'Draw_Position': 'lt5_draw_position',\n",
    "#     }, inplace=True)\n",
    "uniform_distribution_df.rename(\n",
    "    columns={\n",
    "        'Draw_Date': 'uniform_draw_date', 'White_Ball': 'uniform_winning_number',\n",
    "        'Draw_Position': 'uniform_draw_position',\n",
    "    }, inplace=True)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(uniform_distribution_df)\n",
    "\n",
    "uniform_distribution_df = uniform_distribution_df[:25000]\n",
    "\n",
    "# Save this DataFrame to a CSV file\n",
    "uniform_distribution_df.to_csv(\n",
    "    os.path.join(DATA_FOLDER, 'uniform_distribution_data.csv'),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8805be2",
   "metadata": {},
   "source": [
    "### Exponential Distribution\n",
    "\n",
    "The Ultimate Earthquake Dataset From 1990-2023\n",
    "\n",
    "- Source: https://www.kaggle.com/datasets/alessandrolobello/the-ultimate-earthquake-dataset-from-1990-2023\n",
    "- License: [CC0: Public Domain](https://creativecommons.org/publicdomain/zero/1.0/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7636331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             time                           place    status  tsunami  \\\n",
      "66   631184592880  3 km NE of Hitachi-Naka, Japan  reviewed        0   \n",
      "245  631297597730      40 km ENE of Nemuro, Japan  reviewed        0   \n",
      "346  631356818930       50 km SE of Hasaki, Japan  reviewed        0   \n",
      "480  631455398380     73 km ESE of Hitachi, Japan  reviewed        0   \n",
      "544  631495557190      252 km SSE of ?yama, Japan  reviewed        0   \n",
      "\n",
      "     significance   data_type  magnitudo   state  longitude  latitude  depth  \\\n",
      "66            354  earthquake        4.8   Japan    140.568    36.417   67.9   \n",
      "245           326  earthquake        4.6   Japan    146.063    43.420   73.4   \n",
      "346           284  earthquake        4.3   Japan    141.272    35.457   10.0   \n",
      "480           298  earthquake        4.4   Japan    141.408    36.342   55.1   \n",
      "544           416  earthquake        5.2   Japan    138.821    32.381  247.7   \n",
      "\n",
      "                                 date  \n",
      "66   1990-01-01 09:03:12.880000+00:00  \n",
      "245  1990-01-02 16:26:37.730000+00:00  \n",
      "346  1990-01-03 08:53:38.930000+00:00  \n",
      "480  1990-01-04 12:16:38.380000+00:00  \n",
      "544  1990-01-04 23:25:57.190000+00:00  \n",
      "     exp_earthquake_number  exp_time_since_last_earthquake_seconds  \\\n",
      "245                      2                               113004.85   \n",
      "346                      3                                59221.20   \n",
      "480                      4                                98579.45   \n",
      "544                      5                                40158.81   \n",
      "695                      6                                75909.94   \n",
      "\n",
      "     exp_magnitude  \n",
      "245            4.6  \n",
      "346            4.3  \n",
      "480            4.4  \n",
      "544            5.2  \n",
      "695            4.3  \n"
     ]
    }
   ],
   "source": [
    "earthquake_df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        DATA_FOLDER, 'Eartquakes-1990-2023.csv')\n",
    ")\n",
    "\n",
    "earthquake_df = earthquake_df.loc[earthquake_df['state'] == ' Japan']\n",
    "print(earthquake_df.head())\n",
    "\n",
    "# Convert the 'time' column to datetime objects\n",
    "earthquake_df['time'] = pd.to_datetime(earthquake_df['time'], unit='ms')\n",
    "\n",
    "# Sort the DataFrame by time\n",
    "earthquake_df = earthquake_df.sort_values(by='time')\n",
    "\n",
    "# Calculate the time difference between consecutive earthquakes in seconds\n",
    "time_diff = earthquake_df['time'].diff().dt.total_seconds()\n",
    "\n",
    "# Create the 'Time_Since_Last_Earthquake_Seconds' column\n",
    "earthquake_df['Time_Since_Last_Earthquake_Seconds'] = time_diff\n",
    "\n",
    "# Create a sequential 'Earthquake_Number' column as the dependent variable\n",
    "earthquake_df['Earthquake_Number'] = range(1, len(earthquake_df) + 1)\n",
    "\n",
    "earthquake_df['Magnitude'] = earthquake_df['magnitudo']\n",
    "\n",
    "# Select the two columns we need for the exponential distribution example\n",
    "exponential_df = earthquake_df[[\n",
    "    'Earthquake_Number', 'Time_Since_Last_Earthquake_Seconds', 'Magnitude']].dropna()\n",
    "\n",
    "exponential_df.rename(\n",
    "    columns={\n",
    "        'Earthquake_Number': 'exp_earthquake_number', 'Time_Since_Last_Earthquake_Seconds': 'exp_time_since_last_earthquake_seconds',\n",
    "        'Magnitude': 'exp_magnitude',\n",
    "    }, inplace=True)\n",
    "\n",
    "# Display the first few rows of the resulting DataFrame\n",
    "print(exponential_df.head())\n",
    "\n",
    "exponential_df = exponential_df[:25000]\n",
    "\n",
    "# Save this DataFrame to a new CSV file\n",
    "exponential_df.to_csv(\n",
    "    os.path.join(\n",
    "        DATA_FOLDER, 'exponential_distribution_data.csv'),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f83eda",
   "metadata": {},
   "source": [
    "### Poisson Distribution\n",
    "\n",
    "Schochastics's Football (Soccer) Dataset\n",
    "\n",
    "- Source: https://github.com/schochastics/football-data\n",
    "- Download: https://github.com/schochastics/football-data/archive/refs/tags/v0.1.0.zip\n",
    "- File used: football-data-0.1.0/data/results/spain.csv\n",
    "- License: [Open Data Commons Attribution License](https://opendatacommons.org/licenses/by/1-0/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f5a93a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        DATA_FOLDER, 'spain.csv')\n",
    ")\n",
    "\n",
    "df = df[['home', 'away', 'date', 'gh', 'ga']]\n",
    "\n",
    "# df.rename(columns={'home': 'home_team', 'away': 'away_team', 'gh': 'goals_home', 'ga': 'goals_away'}, inplace=True)\n",
    "df.rename(columns={\n",
    "    'home': 'poisson_home_team', 'away': 'poisson_away_team',\n",
    "    'gh': 'poisson_goals_home', 'ga': 'poisson_goals_away',\n",
    "    'date': 'poisson_date'\n",
    "}, inplace=True)\n",
    "\n",
    "df = df[:25000]\n",
    "\n",
    "df.to_csv(\n",
    "    os.path.join(\n",
    "        DATA_FOLDER, 'poisson_distribution_data.csv'),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517341b2",
   "metadata": {},
   "source": [
    "### Log Normal Distribution\n",
    "\n",
    "DC Public Employee Salary Dataset\n",
    "\n",
    "- Source 1: https://catalog.data.gov/dataset/dc-public-employee-salary \n",
    "- Source 2: https://opendata.dc.gov/datasets/DCGIS::dc-public-employee-salary/explore\n",
    "- Download: https://opendata.dc.gov/api/download/v1/items/c9a03cab565b44849bcfc57e63fd3591/csv?layers=35\n",
    "- License: [CC BY 4.0 License](https://creativecommons.org/licenses/by/4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "934d20e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  FIRST_NAME  LAST_NAME                       JOBTITLE DESCRSHORT GRADE  \\\n",
      "0      Scott     Pitzer  Facilities Systems Specialist        DBH    12   \n",
      "1    Pauline     Oboite                          NURSE        DBH    05   \n",
      "2       Dawn       Fong   Special Projects Coordinator        DCG    12   \n",
      "3      Sonya  Kingsland   Behavioral Health Technician        DBH    08   \n",
      "4    Octavia   Fletcher   Behavioral Health Technician        DBH    08   \n",
      "\n",
      "   COMPRATE HIREDATE_STRING GVT_TYPE_OF_APPT  OBJECTID     employee_name  \n",
      "0   90805.0      1987/10/01         CS - Reg         1      Scott Pitzer  \n",
      "1   76510.0      2000/02/14         CS - Reg         2    Pauline Oboite  \n",
      "2   90805.0      2020/03/16        CS - Term         3         Dawn Fong  \n",
      "3   70600.0      1988/06/07         CS - Reg         4   Sonya Kingsland  \n",
      "4   70600.0      1995/09/18         CS - Reg         5  Octavia Fletcher  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        DATA_FOLDER, 'DC_Public_Employee_Salary.csv')\n",
    ")\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df = df.loc[df['COMPRATE'] > 5000]\n",
    "\n",
    "df['employee_name'] = df['FIRST_NAME'] + ' ' + df['LAST_NAME']\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "df = df[['employee_name', 'JOBTITLE', 'COMPRATE']]\n",
    "# df.rename(columns={'COMPRATE': 'annual_compensation', 'JOBTITLE': 'job_title'}, inplace=True)\n",
    "df.rename(columns={\n",
    "    'employee_name': 'ln_employee_name', 'COMPRATE': 'ln_annual_compensation', 'JOBTITLE': 'ln_job_title'\n",
    "}, inplace=True)\n",
    "\n",
    "df = df[:25000]\n",
    "\n",
    "df.to_csv(\n",
    "    os.path.join(\n",
    "        DATA_FOLDER, 'log_normal_distribution_data.csv'),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dc849e",
   "metadata": {},
   "source": [
    "### Gamma Distribution\n",
    "\n",
    "MQTT Traffic Dataset\n",
    "\n",
    "- Source: https://www.kaggle.com/datasets/cnrieiit/mqttset\n",
    "- Download: `mqttdataset_reduced.csv` within `Data/FINAL_CSV/`\n",
    "- License: [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/)\n",
    "- Attribution: [Vaccari, I.; Chiola, G.; Aiello, M.; Mongelli, M.; Cambiaso, E. MQTTset, a New Dataset for Machine Learning Techniques on MQTT. Sensors 2020, 20, 6578](https://www.mdpi.com/1424-8220/20/22/6578/htm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "860a547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        DATA_FOLDER, 'mqttdataset_reduced.csv')\n",
    ")\n",
    "\n",
    "df = df.loc[df['tcp.len'] > 0]\n",
    "\n",
    "df = df[['tcp.time_delta', 'tcp.len']]\n",
    "\n",
    "df.rename(columns={\n",
    "    'tcp.time_delta': 'gamma_time_delta', 'tcp.len': 'gamma_data_length'\n",
    "}, inplace=True)\n",
    "\n",
    "df = df[:25000]\n",
    "\n",
    "df.to_csv(\n",
    "    os.path.join(\n",
    "        DATA_FOLDER, 'gamma_distribution_data.csv'),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ecd6fd",
   "metadata": {},
   "source": [
    "### Beta Distribution\n",
    "\n",
    "College Completion Dataset\n",
    "\n",
    "- Source: https://www.kaggle.com/datasets/thedevastator/boost-student-success-with-college-completion-da?select=cc_state_sector_grads.csv\n",
    "- Download: `cc_state_sector_grads.csv`\n",
    "- License: [CC0 1.0 Universal (CC0 1.0)](https://creativecommons.org/publicdomain/zero/1.0/)\n",
    "- Attribution: https://data.world/databeats/college-completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "042af2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26694\n",
      "    beta_state  beta_year beta_gender  beta_completion_100_rate\n",
      "324    Arizona       2011           B                     0.094\n",
      "325    Arizona       2011           B                     0.170\n",
      "326    Arizona       2011           B                     0.072\n",
      "327    Arizona       2011           B                     0.111\n",
      "328    Arizona       2011           B                     0.131\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        DATA_FOLDER, 'cc_state_sector_grads.csv')\n",
    ")\n",
    "\n",
    "df.rename(\n",
    "    columns={\n",
    "        'state': 'cc_state', 'year': 'cc_year', 'gender': 'cc_gender',\n",
    "        'grad_100_rate': 'cc_completion_100_rate'\n",
    "    }, inplace=True)\n",
    "\n",
    "\n",
    "df = df.loc[df['cc_state'].isin(\n",
    "    [\n",
    "        'California', 'Texas', 'Florida', 'New York', 'Pennsylvania',\n",
    "        'Illinois', 'Ohio', 'Georgia', 'North Carolina', 'Michigan',\n",
    "        'New Jersey', 'Virginia', 'Washington', 'Arizona', 'Tennessee'\n",
    "    ]\n",
    ")]\n",
    "df = df[['cc_state', 'cc_year', 'cc_gender', 'cc_completion_100_rate']]\n",
    "\n",
    "df['cc_completion_100_rate'] = df['cc_completion_100_rate'] / 100\n",
    "\n",
    "df.rename(\n",
    "    columns={\n",
    "        'cc_state': 'beta_state', 'cc_year': 'beta_year', 'cc_gender': 'beta_gender',\n",
    "        'cc_completion_100_rate': 'beta_completion_100_rate'\n",
    "    }, inplace=True)\n",
    "\n",
    "print(len(df))\n",
    "df = df[:25000]\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "df.to_csv(\n",
    "    os.path.join(\n",
    "        DATA_FOLDER, 'beta_distribution_data.csv'),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59bcc1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_df = pd.read_csv(os.path.join(DATA_FOLDER, 'normal_distribution_data.csv'))\n",
    "uniform_df = pd.read_csv(os.path.join(DATA_FOLDER, 'uniform_distribution_data.csv'))\n",
    "exponential_df = pd.read_csv(os.path.join(DATA_FOLDER, 'exponential_distribution_data.csv'))\n",
    "poisson_df = pd.read_csv(os.path.join(DATA_FOLDER, 'poisson_distribution_data.csv'))\n",
    "ln_df = pd.read_csv(os.path.join(DATA_FOLDER, 'log_normal_distribution_data.csv'))\n",
    "gamma_df = pd.read_csv(os.path.join(DATA_FOLDER, 'gamma_distribution_data.csv'))\n",
    "beta_df = pd.read_csv(os.path.join(DATA_FOLDER, 'beta_distribution_data.csv'))\n",
    "\n",
    "final_df = pd.concat(\n",
    "    [\n",
    "        normal_df, uniform_df, exponential_df, poisson_df, ln_df, gamma_df, beta_df\n",
    "    ], axis=1\n",
    ")\n",
    "\n",
    "final_df.to_csv(os.path.join(DATA_FOLDER, 'real_world_distributions.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

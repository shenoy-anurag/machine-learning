{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.8.0, Keras version: 2.7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "print(\"Tensorflow version: {}, Keras version: {}\".format(tf.__version__, keras.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import pickle\n",
    "import string\n",
    "import requests\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.layers import Input, Dense, GRU, LSTM, Embedding\n",
    "from tensorflow.python.keras.optimizers import adam_v2\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from mosestokenizer import MosesDetokenizer\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/anurags/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/anurags/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "detokenizer = MosesDetokenizer('en')\n",
    "translator = str.maketrans('', '', string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'further', 'yourselves', 'doing', 'he', 'mustn', 'them', 'your', 'having', 'don', 'doesn', 'am', \"doesn't\", 'my', 'won', 'theirs', 'down', 'all', 'didn', 'more', \"wasn't\", 'other', 'while', \"it's\", 'our', 'being', 'by', 'does', 'so', \"shouldn't\", 'isn', 'against', 'just', 'themselves', 'yourself', 'an', 'than', 'most', 'or', 'll', 'below', 'we', 'again', 'only', 'should', 'shan', 'above', 've', \"aren't\", 'herself', 'him', 'ourselves', \"needn't\", 'ours', 'yours', 'himself', 'haven', \"don't\", 'out', 'had', 'each', 'hers', 'wasn', 'and', \"wouldn't\", 'same', 'between', 'these', \"you'll\", 'they', 'needn', 'own', 'very', 'over', 'wouldn', 'both', 'm', \"mightn't\", 'did', 'is', 's', 'ma', 'o', \"you'd\", 'few', 'now', 'any', 'but', 'after', \"mustn't\", 'weren', 'from', 'as', 'this', 'myself', 'some', 'a', \"weren't\", \"didn't\", 're', \"couldn't\", 'her', 'has', 'mightn', 'aren', \"you've\", 'until', 'its', \"hasn't\", 'because', 'that', \"haven't\", 'his', 'ain', 'when', 'd', 'she', 'couldn', 'it', 'hasn', 'was', \"won't\", 'their', 't', 'nor', \"shan't\", 'hadn', 'shouldn', \"she's\", 'were', 'such', \"you're\", \"hadn't\", 'if', 'itself', 'those', 'me', 'been', 'through', 'into', \"should've\", 'during', 'whom', \"that'll\", 'then', 'under', \"isn't\", 'once', 'there'}\n"
     ]
    }
   ],
   "source": [
    "# Reduce stopwords list by excluding these words from the list.\n",
    "not_stopwords = [\n",
    "    'no', 'not', 'to', 'i', 'be', 'how', 'are', 'r', 'y', 'you', 'the', 'do', 'what', 'up', 'can', 'for',\n",
    "    'of', 'where', 'have', 'who', 'on', 'with', 'which', 'in', 'about', 'at', 'will', 'here', 'too', 'off',\n",
    "    'before', 'why'\n",
    "]\n",
    "\n",
    "NLTK_STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def get_stopwords(stopwords, not_stopwords):\n",
    "    for word in not_stopwords:\n",
    "        stopwords.discard(word)\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "custom_stopwords = get_stopwords(NLTK_STOPWORDS, not_stopwords=not_stopwords)\n",
    "\n",
    "print(custom_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/Intent/intents.yml\n"
     ]
    }
   ],
   "source": [
    "DATA_FOLDER = \"./data\"\n",
    "\n",
    "dataset_url = \"https://www.kaggle.com/elvinagammed/chatbots-intent-recognition-dataset/\"\n",
    "\n",
    "# INTENT_FILE_NAME = \"Intent.json\"\n",
    "INTENT_FILE_NAME = \"intents.yml\"\n",
    "\n",
    "INTENT_DATA_FOLDER = os.path.join(os.path.join(DATA_FOLDER, \"Intent\"))\n",
    "INTENT_FILE_PATH = os.path.join(INTENT_DATA_FOLDER, INTENT_FILE_NAME)\n",
    "print(INTENT_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'intent': 'greet',\n",
       "  'examples': \"- hey\\n- hello\\n- hi\\n- hello there\\n- good morning\\n- good evening\\n- moin\\n- hey there\\n- let's go\\n- hey dude\\n- goodmorning\\n- goodevening\\n- good afternoon\\n- Hellllooooooo\\n- howdy\\n- hey bot\\n- heya\\n- Hallo\\n- hello?\\n- ayyyy whaddup\\n- heyo\\n- helloooo\\n- hellooo\\n- hello sweatheart\\n- hiihihi\\n- hey there\\n- yoo\\n- hello sweet boy\\n- whats up\\n- Hei\\n- hello\\n- I said, helllllloooooO!!!!\\n- good evening\\n- hi there it's me\\n- hi friend\\n- jop\\n- hi\\n- hi there\\n- what up\\n- hii\\n- hello it is me again\\n- jojojo\\n- hey let's talk\\n- hey, let's talk\\n- heyho\\n- hiii\\n- Whats up my bot\\n- Heya\\n- hey dude\\n- Well hello there\\n- hello friend\\n- Hey\\n- greetings\\n- hello everybody\\n- hello is anybody there\\n- good afternoon\\n- hello robot\\n- hallo\\n- hi?\\n- hola\\n- yo\\n- heeey\\n- hi hi\\n- hey\\n- hey hey\\n- hello there\\n- hi\\n- hi there\\n- hey bot!\\n- hi pal!\\n- hi folks\\n- Hey man\\n- Hi\\n- Yo!\\n- Howdy\\n- Yo!\\n- Howdy\\n- Hiya\\n- Hello!\\n- Hola!\\n- Hi\\n- Hey\\n- Hi bot\\n- Hey bot\\n- Hello\\n- hi again\\n- hi Mister\\n- good morning\\n\"},\n",
       " {'intent': 'goodbye',\n",
       "  'examples': '- cu\\n- good by\\n- cee you later\\n- good night\\n- bye\\n- goodbye\\n- have a nice day\\n- see you around\\n- bye bye\\n- see you later\\n'},\n",
       " {'intent': 'good_day',\n",
       "  'examples': \"- have a nice day\\n- good day to you\\n- have a good one!\\n- G'day mate!\\n- it is a good evening\\n- it is a good day today\\n\"},\n",
       " {'intent': 'mood_great',\n",
       "  'examples': '- perfect\\n- great\\n- amazing\\n- feeling like a king\\n- wonderful\\n- I am feeling very good\\n- I am great\\n- I am amazing\\n- I am going to save the world\\n- super stoked\\n- extremely good\\n- so so perfect\\n- so good\\n- so perfect\\n'},\n",
       " {'intent': 'mood_unhappy',\n",
       "  'examples': \"- my day was horrible\\n- I am sad\\n- I don't feel very well\\n- I am disappointed\\n- super sad\\n- I'm so sad\\n- sad\\n- very sad\\n- unhappy\\n- not good\\n- not very good\\n- extremly sad\\n- so saad\\n- so sad\\n\"}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "intent_data = None\n",
    "with open(INTENT_FILE_PATH, 'r') as f:\n",
    "    # The FullLoader parameter handles the conversion from YAML\n",
    "    # scalar values to Python the dictionary format\n",
    "    intent_data = yaml.full_load(f)\n",
    "    intent_data = intent_data[\"intents\"]\n",
    "\n",
    "display(intent_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1433\n",
      "           text intent\n",
      "0           hey  greet\n",
      "1         hello  greet\n",
      "2            hi  greet\n",
      "3   hello there  greet\n",
      "4  good morning  greet\n"
     ]
    }
   ],
   "source": [
    "train_data = {'text': [], 'intent': []}\n",
    "\n",
    "for i in range(len(intent_data)):\n",
    "    lines = intent_data[i][\"examples\"]\n",
    "    lines = lines.split(\"\\n\")\n",
    "    lines = [l.strip(\"- \").strip(\"''\") for l in lines if l.strip()]\n",
    "    intent_lines = [intent_data[i][\"intent\"] for _ in range(len(lines))]\n",
    "    train_data[\"text\"].extend(lines)\n",
    "    train_data[\"intent\"].extend(intent_lines)\n",
    "\n",
    "train_data = pd.DataFrame(train_data)\n",
    "print(len(train_data))\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_data['text'].tolist()\n",
    "y_train = train_data['intent'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    return text.translate(translator)\n",
    "\n",
    "\n",
    "def tokenize_sent(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "\n",
    "def detokenize_sent(tokenized_text):\n",
    "    return detokenizer(tokenized_text)\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = text.lower()\n",
    "    tokenized_text = tokenize_sent(text)\n",
    "    cleaned_text = [word for word in tokenized_text if not word in NLTK_STOPWORDS]\n",
    "    return ' '.join(cleaned_text)\n",
    "\n",
    "\n",
    "def remove_custom_stopwords(text, stopwords):\n",
    "    text = text.lower()\n",
    "    tokenized_text = tokenize_sent(text)\n",
    "    cleaned_text = [word for word in tokenized_text if not word in stopwords]\n",
    "    return ' '.join(cleaned_text)\n",
    "\n",
    "\n",
    "def lower_case_text(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING = 'pre'\n",
    "TEXT_LEN_LIMIT = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey', 'hello', 'hi', 'hello', 'good morning']\n"
     ]
    }
   ],
   "source": [
    "x_train_processed = []\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "    # lower case the text\n",
    "    text = lower_case_text(x_train[i])\n",
    "    # remove punctuation\n",
    "    text = text.translate(translator)\n",
    "    # remove stopwords\n",
    "    text = remove_custom_stopwords(text, custom_stopwords)\n",
    "\n",
    "    if not text:\n",
    "        text = x_train[i]\n",
    "    # process the text using Spacy's language model \"en_core_web_sm\".\n",
    "    doc = nlp(text)\n",
    "    # lemmatization\n",
    "    text = [token.lemma_ for token in doc]\n",
    "    # detokenize text\n",
    "    text = detokenize_sent(text)\n",
    "\n",
    "    x_train_processed.append(text)\n",
    "\n",
    "print(x_train_processed[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for i in range(len(x_train_processed)):\n",
    "    words = word_tokenize(x_train_processed[i])\n",
    "    all_words.extend(words)\n",
    "\n",
    "all_words = list(set(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_intents = len(set(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 1, 'to': 2, 'you': 3, 'the': 4, 'what': 5, 'want': 6, 'be': 7, 'bot': 8, 'how': 9, 'newsletter': 10, 'do': 11, 'about': 12, 's': 13, 'who': 14, 'm': 15, 'can': 16, 'for': 17, 'name': 18, 'time': 19, 'know': 20, 'chatbot': 21, 'not': 22, 'like': 23, 'good': 24, 'in': 25, 'of': 26, 'thank': 27, 'tell': 28, 'build': 29, 'up': 30, 'talk': 31, 'have': 32, 'no': 33, 'please': 34, 'learn': 35, 'give': 36, 'company': 37, 'weather': 38, 'hi': 39, 'go': 40, 'help': 41, 'get': 42, 'one': 43, 'sign': 44, 'subscribe': 45, 'bye': 46, 'day': 47, 'yes': 48, 'stop': 49, 'with': 50, 'hey': 51, 'would': 52, 'work': 53, 'feedback': 54, 'hello': 55, 'today': 56, 'human': 57, 'manager': 58, 'developer': 59, 'make': 60, 'will': 61, 'email': 62, 'on': 63, 'why': 64, 'use': 65, 'cool': 66, 'speak': 67, 'let': 68, 'ok': 69, 'nice': 70, 'sure': 71, 'd': 72, 'start': 73, 'great': 74, 'real': 75, 'else': 76, 'create': 77, 'call': 78, 'learning': 79, 'see': 80, 'accept': 81, 'machine': 82, 'which': 83, 'akela': 84, 'improve': 85, 'dialogue': 86, 'later': 87, 'sad': 88, 'support': 89, 'million': 90, 'engineer': 91, 'language': 92, 'management': 93, 'lead': 94, 'intent': 95, 'suggest': 96, 'robot': 97, 'course': 98, 'absolutely': 99, 'thing': 100, 'exit': 101, 'person': 102, 'wan': 103, 'na': 104, 'customer': 105, 'sale': 106, 'euro': 107, 'place': 108, 'boss': 109, 'project': 110, 'tool': 111, 'ture': 112, 'oov': 113, 'need': 114, 'deep': 115, 'natural': 116, 'test': 117, 'signup': 118, 'add': 119, 'evening': 120, 'friend': 121, 'perfect': 122, 'feel': 123, 'here': 124, 'oh': 125, 'ai': 126, 'think': 127, 'u': 128, 'someone': 129, 'everything': 130, '1': 131, 'ice': 132, 'dev': 133, 'at': 134, 'year': 135, 'idea': 136, 'right': 137, 'where': 138, 'anything': 139, 'entity': 140, 'generative': 141, 'nltk': 142, 'retention': 143, 'retrieval': 144, 'spacy': 145, 'tensorflow': 146, 'list': 147, 'morning': 148, 'goodbye': 149, 'mate': 150, 'amazing': 151, 'ask': 152, 'really': 153, 'alright': 154, 'yet': 155, 'never': 156, 'quit': 157, 'action': 158, 'touch': 159, 'service': 160, 'team': 161, 'send': 162, 'business': 163, '5': 164, 'none': 165, 'insurance': 166, 'health': 167, 'english': 168, 'people': 169, 'linda': 170, 'life': 171, 'inr': 172, 'we': 173, 'product': 174, 'new': 175, 'suggestion': 176, 'invent': 177, 'could': 178, 'first': 179, 'kera': 180, 'nlp': 181, 'nlu': 182, 'receive': 183, 'nl': 184, 'howdy': 185, 'sweet': 186, 'well': 187, 'yo': 188, 'around': 189, 'super': 190, 'chat': 191, 'yep': 192, 'mind': 193, 'nah': 194, \"'day\": 195, 'idiot': 196, 'fuck': 197, 'also': 198, 'cute': 199, 'picture': 200, 'big': 201, 'researcher': 202, 'old': 203, '10': 204, 'head': 205, 'buck': 206, 'acme': 207, 'full': 208, 'stack': 209, 'money': 210, 'distance': 211, 'marketing': 212, 'mop': 213, 'base': 214, 'alex': 215, 'dollar': 216, 'ceo': 217, 'mail': 218, 'york': 219, 'philipp': 220, \"'go\": 221, 'generate': 222, 'creator': 223, 'current': 224, 'berlin': 225, 'option': 226, 'processing': 227, 'understanding': 228, 'configure': 229, 'subscription': 230, 'dope': 231, 'join': 232, 'registration': 233, 'type': 234, 'dude': 235, 'afternoon': 236, 'heya': 237, 'hallo': 238, 'boy': 239, 'say': 240, 'hola': 241, 'man': 242, 'night': 243, 'world': 244, 'simple': 245, 're': 246, 'artificial': 247, 'intelligence': 248, 'guess': 249, 'y': 250, 'fine': 251, 'change': 252, 'hm': 253, 'definitely': 254, 'agree': 255, 'affirmative': 256, 'udo': 257, 'k': 258, 'nein': 259, 'wrong': 260, 'decide': 261, 'decline': 262, 'mean': 263, 'either': 264, 'execution': 265, 'moron': 266, 'piece': 267, 'shit': 268, 'hate': 269, 'stupid': 270, 'anyone': 271, 'founder': 272, 'care': 273, 'check': 274, 'news': 275, 'much': 276, 'cheer': 277, 'lot': 278, 'advocate': 279, 'juste': 280, 'jimmy': 281, 'cream': 282, 'transformer': 283, '200': 284, 'quid': 285, 'sentient': 286, 'glibber': 287, 'glitter': 288, '60': 289, 'choose': 290, 'car': 291, 'actually': 292, 'brand': 293, '50': 294, 'scientist': 295, 'own': 296, 'freelancer': 297, 'something': 298, 'extract': 299, 'amount': 300, 'long': 301, 'cube': 302, 'promote': 303, 'turtle': 304, 'function': 305, 'department': 306, 'animal': 307, 'conversational': 308, 'assistant': 309, 'python': 310, 'max': 311, 'mei': 312, 'tom': 313, 'miller': 314, '2': 315, 'jim': 316, 'different': 317, 'plan': 318, 'microsoft': 319, 'per': 320, 'abcd': 321, '100k': 322, '150000': 323, 'data': 324, 'bcg': 325, 'maxmeierfirmade': 326, 'botfanbotscom': 327, 'elise': 328, 'biz': 329, 'co': 330, 'kg': 331, 'budget': 332, 'ali': 333, 'park': 334, '200k': 335, 'eisenkleber': 336, 'okay': 337, 'treat': 338, 'opinion': 339, 'model': 340, 'focus': 341, 'owner': 342, 'design': 343, 'may': 344, 'set': 345, 'india': 346, 'beautiful': 347, 'tomorrow': 348, 'quite': 349, \"'the\": 350, 'sun': 351, 'explain': 352, 'show': 353, 'kind': 354, 'custom': 355, 'conversation': 356, 'setup': 357, 'love': 358, 'subscribtion': 359, 'lets': 360, 'try': 361, 'website': 362, 'site': 363, 'moin': 364, 'goodmorne': 365, 'goodevening': 366, 'hellllooooooo': 367, 'ayyyy': 368, 'whaddup': 369, 'heyo': 370, 'helloooo': 371, 'hellooo': 372, 'sweatheart': 373, 'hiihihi': 374, 'yoo': 375, 'hei': 376, 'helllllloooooo': 377, 'jop': 378, 'hii': 379, 'jojojo': 380, 'heyho': 381, 'hiii': 382, 'greeting': 383, 'everybody': 384, 'anybody': 385, 'heeey': 386, 'pal': 387, 'folk': 388, 'hiya': 389, 'mister': 390, 'cu': 391, 'cee': 392, 'gday': 393, 'king': 394, 'wonderful': 395, 'save': 396, 'stoke': 397, 'extremely': 398, 'horrible': 399, 'disappoint': 400, 'unhappy': 401, 'extremly': 402, 'saad': 403, 'ar': 404, 'cuz': 405, 'question': 406, 'lol': 407, 'skynet': 408, 'indeed': 409, 'sound': 410, 'correct': 411, 'confirm': 412, 'amayze': 413, 'jo': 414, 'okey': 415, 'dokey': 416, 'behave': 417, 'ja': 418, 'yup': 419, 'yas': 420, 'without': 421, 'doubt': 422, 'coolio': 423, 'yay': 424, 'yop': 425, 'awesome': 426, 'yeah': 427, 'ttyl': 428, 'byebye': 429, 'goodnight': 430, 'ya': 431, 'toodleoo': 432, 'got': 433, 'farewell': 434, 'ciao': 435, 'catch': 436, 'byyye': 437, 'slay': 438, 'tlak': 439, 'address': 440, 'nevermind': 441, 'deny': 442, 'neither': 443, 'afraid': 444, 'way': 445, 'sorry': 446, 'sir': 447, 'maam': 448, 'restart': 449, 'g': 450, 'neckbeard': 451, 'f': 452, 'dumbass': 453, 'smarter': 454, 'put': 455, 'that': 456, 'annoying': 457, 'gim': 458, 'proper': 459, 'handoff': 460, 'agent': 461, 'forward': 462, 'forum': 463, 'link': 464, 'ill': 465, 'bro': 466, 'bunch': 467, 'herbert': 468, 'shiba': 469, 'tylerthematemanclubmatecom': 470, 'tyler': 471, 'construction': 472, 'worker': 473, 'club': 474, 'memecom': 475, 'herbertgmailcom': 476, 'shibashibacom': 477, 'chief': 478, 'lemonade': 479, 'officer': 480, '500': 481, 'half': 482, 'ol': 483, 'hielisede': 484, 'justejustecom': 485, 'ullegmxde': 486, 'csi': 487, 'ml': 488, 'saving': 489, 'account': 490, 'sam': 491, 'nothing': 492, 'tamedmousemicerevolutionfr': 493, 'volodimir': 494, 'voldemarich': 495, 'idk': 496, 'junkjunkcom': 497, 'jpcom': 498, 'alacmeorg': 499, 'al': 500, 'capone': 501, '£50k': 502, 'hr': 503, 'stuff': 504, 'nonenonecouk': 505, 'p': 506, 'datum': 507, 'my': 508, 'meyoude': 509, 'akelaistcoolschwabenländlede': 510, '120000': 511, 'spam': 512, 'pick': 513, 'nose': 514, 'busy': 515, \"'in\": 516, 'date': 517, 'message': 518, 'interested': 519, 'ordinal': 520, 'duration': 521, 'emailemailcom': 522, 'neutron': 523, 'industry': 524, '123akelacom': 525, 'akelaakelacom': 526, 'akelas': 527, 'billion': 528, 'bank': 529, 'alan': 530, 'impress': 531, 'badass': 532, 'tester': 533, 'promotion': 534, 'projject': 535, 'james': 536, 'validemailonede': 537, 'validoneemail': 538, 'unemployed': 539, 'spend': 540, '500000000': 541, 'wolf': 542, 'akilla': 543, 'sell': 544, 'salesman': 545, 'factory': 546, 'propella': 547, 'innvoation': 548, 'job': 549, 'responsible': 550, 'innovation': 551, 'frontend': 552, 'foobarcom': 553, '10000000': 554, 'mrmopmopsapp': 555, 'janitor': 556, 'lindalindalinda': 557, 'favorite': 558, 'color': 559, 'employee': 560, 'book': 561, 'meeting': 562, 'room': 563, 'ralph': 564, 'white': 565, 'ann': 566, 'snyder': 567, 'victoria': 568, 'mcmillan': 569, 'denise': 570, 'perry': 571, 'bob': 572, 'geldorf': 573, 'susan': 574, 'catterfeld': 575, 'taylor': 576, 'shwe': 577, 'meredith': 578, 'grey': 579, 'karev': 580, 'consult': 581, 'potential': 582, 'emayl': 583, 'yolo': 584, 'yolode': 585, '50k': 586, 'mopbot': 587, 'mr': 588, 'lorettastrawberryicecome': 589, 'strawberry': 590, 'philip': 591, 'bosch': 592, 'faq': 593, 'hellohellocom': 594, '10000k': 595, 'adelegmxcom': 596, 'marc': 597, 'trillion': 598, 'developerjobfunctionfull': 599, 'moabityogacom': 600, 'moabit': 601, 'yoga': 602, 'studio': 603, 'oli': 604, '100000k': 605, '10000': 606, '400': 607, 'trilion': 608, 'philippsuperphilippphilipp': 609, 'mln': 610, 'loretta': 611, 'meier': 612, 'mgmt': 613, 'schlabberjimmyglibberglitteredu': 614, '100': 615, '200000': 616, '50000': 617, 'thousand': 618, 'moment': 619, '75000150000': 620, 'estimation': 621, '10k': 622, 'bayer': 623, 'daimler': 624, 'uber': 625, 'small': 626, 'tech': 627, 'apple': 628, 'clue': 629, 'sap': 630, '50000000': 631, 'ibm': 632, 'bout': 633, '4000000': 634, '500000': 635, '123': 636, '1234': 637, 'a2b3c4d5': 638, 'abcde': 639, 'efgh': 640, '45113': 641, '4612': 642, '43499': 643, '240kyear': 644, 'usd': 645, '250000': 646, 'tmobile': 647, 'vodafone': 648, 'accenture': 649, 'chengmingaliyuncom': 650, 'solomq122qqmailcom': 651, 'analyst': 652, 'growth': 653, 'cto': 654, 'sislawawaindiacom': 655, 'alexanderdenkertuberlinde': 656, 'nerdstanfordedu': 657, 'saswatkarharrediffmailcom': 658, 'mckinsey': 659, 'germany': 660, 'brazil': 661, 'digital': 662, 'venture': 663, 'stanford': 664, 'university': 665, 'research': 666, 'group': 667, 'centre': 668, 'ubc': 669, 'vancouver': 670, 'canada': 671, 'coo': 672, 'markjobsibmcom': 673, 'khardikkmosubolnetin': 674, 'mikemillersalesapplecom': 675, 'stephanywhitemicrosoftcom': 676, 'julianfrankhotmailcom': 677, 'santaklausgooglemailcom': 678, '4': 679, 'halpert': 680, 'udoai': 681, 'lindayoloyolode': 682, 'problem': 683, 'solve': 684, 'helvetia': 685, 'elisegmailcom': 686, 'system': 687, 'flatter': 688, 'every': 689, '90k': 690, 'donezo': 691, 'serve': 692, 'butter': 693, 'deve': 694, '300k': 695, 'number': 696, 'kevinyolooozde': 697, '200000000': 698, 'lindamailcom': 699, 'zendesk': 700, 'jacqueline': 701, '123gmailcom': 702, 'udoudoai': 703, 'heykldpeffesfokenoinwf': 704, 'master': 705, 'desaster': 706, 'wurst': 707, '0': 708, 'woman': 709, 'dispenser': 710, '500k': 711, 'develope': 712, 'generation': 713, '100000': 714, 'yespleaseyescom': 715, 'scalable': 716, 'propelladaskapitalde': 717, 'nyt': 718, 'foudner': 719, 'lindalindacom': 720, 'success': 721, 'freya': 722, 'alinytimescom': 723, 'chocolate': 724, 'bbc': 725, 'helphelpcom': 726, 'akelaphilippcom': 727, 'testtestcom': 728, 'software': 729, 'john': 730, 'smith': 731, 'jenny': 732, 'douglas': 733, 'klaus': 734, 'klausson': 735, 'race': 736, 'driver': 737, 'bolschewistische': 738, 'kurkapelle': 739, 'schwarzrot': 740, 'ifuckrobots666applecom': 741, 'saswat': 742, 'self': 743, 'emplaye': 744, '2000k': 745, 'klausimausiapplecom': 746, 'ulrikovitcheisenklebereisenkleberlimitedcokgcom': 747, 'limited': 748, 'ulrikovitch': 749, 'designer': 750, 'pizza': 751, 'operation': 752, 'vladimir': 753, 'scrap': 754, '123skdvfvsdj': 755, 'reddit': 756, '20k': 757, 'philippthephilippcompanycom': 758, 'joeykoolmanconsultingcom': 759, 'load': 760, 'myemailgmailcom': 761, 've': 762, 'hang': 763, \"'up\": 764, \"'new\": 765, \"'everything\": 766, \"'life\": 767, 'whazzup': 768, 'sup': 769, 'ahoy': 770, 'matey': 771, 'chatting': 772, 'take': 773, 'shall': 774, 'listen': 775, 'classification': 776, 'ability': 777, 'chitchat': 778, 'improvement': 779, 'share': 780, 'produce': 781, 'fabricate': 782, 'behind': 783, 'legal': 784, 'programmer': 785, 'builder': 786, 'yoi': 787, 'trouble': 788, 'bring': 789, 'existence': 790, 'since': 791, 'age': 792, 'pardon': 793, 'might': 794, 'hour': 795, 'minute': 796, 'sydney': 797, 'usa': 798, 'excuse': 799, 'temperature': 800, 'expect': 801, 'thunderstorm': 802, 'sky': 803, 'clear': 804, 'meteorological': 805, 'aware': 806, 'look': 807, 'snowman': 808, 'humid': 809, 'scorcher': 810, 'require': 811, 'raincoat': 812, 'rain': 813, 'forecast': 814, 'hot': 815, 'cold': 816, 'breezy': 817, 'outside': 818, 'cloudy': 819, 'zou': 820, 'sunny': 821, 'recognize': 822, 'exactly': 823, 'tombstone': 824, 'colleague': 825, 'parent': 826, 'inform': 827, 'everyone': 828, 'um': 829, 'el': 830, 'next': 831, 'come': 832, 'back': 833, 'sentence': 834, 'possible': 835, 'menu': 836, 'heart': 837, 'r': 838, 'ur': 839, \"'custom\": 840, 'flow': 841, 'keras': 842, 'kerasio': 843, 'wat': 844, 'turing': 845, 'huh': 846, 'begin': 847, 'development': 848, 'guide': 849, 'knowledge': 850, 'building': 851, 'configuration': 852, 'information': 853, 'benefit': 854, 'organisation': 855, 'crave': 856, 'prefer': 857, 'subsribe': 858, 'subscriber': 859, 'lindayolode': 860, 'pelase': 861, 'register': 862, 'alexexamplecz': 863, 'alexmailmail': 864, 'newspaper': 865, 'too': 866, 'alanmailcom': 867, 'alexmailcom': 868, 'pls': 869, 'yeaaah': 870, 'visit': 871, \"'website\": 872, 'beat': 873, 'become': 874, 'enough': 875, 'fool': 876, 'predictdirect': 877, 'rather': 878, 'rule': 879, 'logic': 880}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize Text to assign each word a number\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "\n",
    "tokenizer.fit_on_texts(x_train_processed)\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tokens = tokenizer.texts_to_sequences(x_train_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "max_tokens = 0\n",
    "\n",
    "for i in range(len(x_train_tokens)):\n",
    "    if len(x_train_tokens[i]) > max_tokens:\n",
    "        max_tokens = len(x_train_tokens[i])\n",
    "\n",
    "max_tokens = min(max_tokens, TEXT_LEN_LIMIT)\n",
    "print(max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "\n",
    "y_train_pad = encoder.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens, padding=PADDING, truncating=PADDING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1433 1433\n",
      "16 52\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train_pad), len(y_train_pad))\n",
    "print(len((x_train_pad[0])), len((y_train_pad[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "\n",
    "inverse_map = dict(zip(word_index.values(), word_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_string(tokens):\n",
    "    # Map from tokens back to words.\n",
    "    words = [inverse_map[token] for token in tokens if token != 0]\n",
    "\n",
    "    # Concatenate all words.\n",
    "    text = \" \".join(words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 55],\n",
       "      dtype=int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(x_train_processed[1])\n",
    "display(x_train_pad[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_to_string(x_train_pad[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove.6B.zip\n",
      "./data/glove.6B ./data/glove.6B.zip\n",
      "Zip file already present\n",
      "Files already present on disk\n"
     ]
    }
   ],
   "source": [
    "glove_word_vec_download_url = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "print(glove_word_vec_download_url.rsplit(\"/\", 1)[1])\n",
    "GLOVE_ZIP_PATH = os.path.join(DATA_FOLDER, glove_word_vec_download_url.rsplit(\"/\", 1)[1])\n",
    "GLOVE_FOLDER_PATH = os.path.join(DATA_FOLDER, glove_word_vec_download_url.rsplit(\"/\", 1)[1].rsplit(\".\", 1)[0])\n",
    "GLOVE_FILE = \"glove.6B.100d.txt\"\n",
    "GLOVE_VEC_SIZE = 100\n",
    "print(GLOVE_FOLDER_PATH, GLOVE_ZIP_PATH)\n",
    "\n",
    "if not os.path.exists(GLOVE_ZIP_PATH):\n",
    "    req = requests.get(glove_word_vec_download_url, allow_redirects=True)\n",
    "    # Writing the file to the local file system\n",
    "    with open(GLOVE_ZIP_PATH, 'wb') as output_file:\n",
    "        output_file.write(req.content)\n",
    "    print(\"Downloaded zip file\")\n",
    "else:\n",
    "    print(\"Zip file already present\")\n",
    "\n",
    "if not os.path.exists(GLOVE_FOLDER_PATH):\n",
    "    with zipfile.ZipFile(GLOVE_ZIP_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(GLOVE_FOLDER_PATH)\n",
    "    print(\"Extracted zip file\")\n",
    "else:\n",
    "    print(\"Files already present on disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sigarms', 'katuna', 'aqm', '1.3775', 'corythosaurus', 'chanty', 'kronik', 'rolonda', 'zsombor', 'sandberger']\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = dict()\n",
    "\n",
    "with open(os.path.join(GLOVE_FOLDER_PATH, GLOVE_FILE)) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(list(embeddings_index.keys())[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 20000\n",
    "EMBEDDING_DIM = GLOVE_VEC_SIZE\n",
    "# BATCH_SIZE = 128\n",
    "BATCH_SIZE = 64\n",
    "# BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "    if index > VOCAB_SIZE - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "text_input = Input(shape=(max_tokens,), name='text_input')\n",
    "\n",
    "emb_layer = Embedding(\n",
    "    input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=max_tokens, weights=[embedding_matrix],\n",
    "    trainable=False, name='embedding_layer'\n",
    ")(text_input)\n",
    "\n",
    "main_lstm = LSTM(units=num_intents, dropout=0.4, return_sequences=True, name='lstm_1')(emb_layer)\n",
    "\n",
    "sec_lstm = LSTM(units=num_intents, dropout=0.2, return_sequences=True, name='lstm_2')(main_lstm)\n",
    "\n",
    "final_lstm = LSTM(units=num_intents, name='last_lstm')(sec_lstm)\n",
    "\n",
    "out = Dense(num_intents, activation='softmax', name='out')(final_lstm)\n",
    "\n",
    "model = Model(text_input, out, name=\"intent_classifier\")\n",
    "\n",
    "model.build(input_shape=(BATCH_SIZE, max_tokens,))\n",
    "\n",
    "optimizer = adam_v2.Adam(learning_rate=1e-3)\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"intent_classifier\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_input (InputLayer)      [(None, 16)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_layer (Embedding)  (None, 16, 100)           2000000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 16, 52)            31824     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 16, 52)            21840     \n",
      "_________________________________________________________________\n",
      "last_lstm (LSTM)             (None, 52)                21840     \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 52)                2756      \n",
      "=================================================================\n",
      "Total params: 2,078,260\n",
      "Trainable params: 78,260\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(x_train_pad, y_train_pad, test_size=0.0)\n",
    "# # X_train, X_test, y_train, y_test=train_test_split(x_train_pad,y_train_pad,test_size=0.1, stratify=y_train_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = x_train_pad, y_train_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 17:36:38.028617: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 26s 2s/step - loss: 3.8848 - accuracy: 0.2519\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 23s 2s/step - loss: 3.4615 - accuracy: 0.2903\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 25s 2s/step - loss: 3.1480 - accuracy: 0.2903\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 25s 2s/step - loss: 3.0610 - accuracy: 0.2903\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 26s 2s/step - loss: 3.0233 - accuracy: 0.2903\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 27s 2s/step - loss: 2.9754 - accuracy: 0.2903\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 26s 2s/step - loss: 2.9117 - accuracy: 0.2903\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 25s 2s/step - loss: 2.8566 - accuracy: 0.2875\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 25s 2s/step - loss: 2.8026 - accuracy: 0.2980\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 26s 2s/step - loss: 2.7526 - accuracy: 0.3050\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 26s 2s/step - loss: 2.7084 - accuracy: 0.3077\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 26s 2s/step - loss: 2.6735 - accuracy: 0.3119\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 27s 2s/step - loss: 2.6187 - accuracy: 0.3210\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 28s 2s/step - loss: 2.5946 - accuracy: 0.3217\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 28s 2s/step - loss: 2.5585 - accuracy: 0.3259\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 26s 2s/step - loss: 2.5420 - accuracy: 0.3273\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 27s 2s/step - loss: 2.4976 - accuracy: 0.3273\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 27s 2s/step - loss: 2.4870 - accuracy: 0.3343\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 27s 2s/step - loss: 2.4697 - accuracy: 0.3385\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 26s 2s/step - loss: 2.4363 - accuracy: 0.3433\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 30s 2s/step - loss: 2.4027 - accuracy: 0.3517\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 37s 3s/step - loss: 2.3778 - accuracy: 0.3587\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 33s 3s/step - loss: 2.3358 - accuracy: 0.3726\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 32s 3s/step - loss: 2.2993 - accuracy: 0.3817\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 37s 3s/step - loss: 2.2770 - accuracy: 0.4145\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 36s 3s/step - loss: 2.2289 - accuracy: 0.4166\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 36s 3s/step - loss: 2.2150 - accuracy: 0.4201\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 36s 3s/step - loss: 2.1731 - accuracy: 0.4424\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 33s 3s/step - loss: 2.1465 - accuracy: 0.4578\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 39s 3s/step - loss: 2.1005 - accuracy: 0.4606\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 34s 3s/step - loss: 2.0880 - accuracy: 0.4689\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 32s 3s/step - loss: 2.0459 - accuracy: 0.4773\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 32s 3s/step - loss: 2.0243 - accuracy: 0.4906\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 31s 3s/step - loss: 2.0009 - accuracy: 0.5017\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 37s 3s/step - loss: 1.9559 - accuracy: 0.5052\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 33s 3s/step - loss: 1.9281 - accuracy: 0.5108\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 35s 3s/step - loss: 1.8923 - accuracy: 0.5073\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 36s 3s/step - loss: 1.8678 - accuracy: 0.5164\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 36s 3s/step - loss: 1.8359 - accuracy: 0.5283\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 34s 3s/step - loss: 1.8256 - accuracy: 0.5248\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 38s 3s/step - loss: 1.8083 - accuracy: 0.5262\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 43s 4s/step - loss: 1.7682 - accuracy: 0.5478\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 43s 4s/step - loss: 1.7513 - accuracy: 0.5443\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 40s 3s/step - loss: 1.7284 - accuracy: 0.5408\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 40s 3s/step - loss: 1.7085 - accuracy: 0.5597\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 38s 3s/step - loss: 1.6825 - accuracy: 0.5513\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 37s 3s/step - loss: 1.6520 - accuracy: 0.5583\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 35s 3s/step - loss: 1.6394 - accuracy: 0.5618\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 33s 3s/step - loss: 1.6060 - accuracy: 0.5736\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 34s 3s/step - loss: 1.6041 - accuracy: 0.5722\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 39s 3s/step - loss: 1.5913 - accuracy: 0.5652\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 40s 3s/step - loss: 1.5820 - accuracy: 0.5764\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 40s 3s/step - loss: 1.5491 - accuracy: 0.5764\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 33s 3s/step - loss: 1.5489 - accuracy: 0.5834\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 35s 3s/step - loss: 1.5203 - accuracy: 0.5946\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 37s 3s/step - loss: 1.4979 - accuracy: 0.5918\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 36s 3s/step - loss: 1.4874 - accuracy: 0.6015\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 36s 3s/step - loss: 1.4706 - accuracy: 0.6050\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 37s 3s/step - loss: 1.4617 - accuracy: 0.6001\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 37s 3s/step - loss: 1.4411 - accuracy: 0.6057\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 45s 4s/step - loss: 1.4159 - accuracy: 0.6141\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 47s 4s/step - loss: 1.4065 - accuracy: 0.6190\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 51s 4s/step - loss: 1.3880 - accuracy: 0.6162\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 48s 4s/step - loss: 1.3789 - accuracy: 0.6211\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 48s 4s/step - loss: 1.3730 - accuracy: 0.6211\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 47s 4s/step - loss: 1.3530 - accuracy: 0.6239\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 45s 4s/step - loss: 1.3388 - accuracy: 0.6336\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 38s 3s/step - loss: 1.3299 - accuracy: 0.6329\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 34s 3s/step - loss: 1.2998 - accuracy: 0.6343\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 36s 3s/step - loss: 1.3029 - accuracy: 0.6343\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 35s 3s/step - loss: 1.2802 - accuracy: 0.6427\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 41s 3s/step - loss: 1.2748 - accuracy: 0.6364\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 41s 3s/step - loss: 1.2673 - accuracy: 0.6497\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 44s 4s/step - loss: 1.2537 - accuracy: 0.6511\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 105s 9s/step - loss: 1.2402 - accuracy: 0.6629\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 63s 5s/step - loss: 1.2145 - accuracy: 0.6643\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 34s 3s/step - loss: 1.2289 - accuracy: 0.6469\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 32s 3s/step - loss: 1.1969 - accuracy: 0.6678\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 33s 3s/step - loss: 1.1868 - accuracy: 0.6720\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 30s 2s/step - loss: 1.1767 - accuracy: 0.6783\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 30s 3s/step - loss: 1.1650 - accuracy: 0.6811\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 29s 2s/step - loss: 1.1499 - accuracy: 0.6818\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 29s 2s/step - loss: 1.1462 - accuracy: 0.6783\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 29s 2s/step - loss: 1.1329 - accuracy: 0.6832\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 27s 2s/step - loss: 1.1365 - accuracy: 0.6818\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 27s 2s/step - loss: 1.1037 - accuracy: 0.7006\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 27s 2s/step - loss: 1.0997 - accuracy: 0.6950\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 28s 2s/step - loss: 1.0958 - accuracy: 0.6916\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 27s 2s/step - loss: 1.0797 - accuracy: 0.6950\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 27s 2s/step - loss: 1.0542 - accuracy: 0.7055\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 26s 2s/step - loss: 1.0552 - accuracy: 0.7083\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 26s 2s/step - loss: 1.0494 - accuracy: 0.7111\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 26s 2s/step - loss: 1.0334 - accuracy: 0.7069\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 27s 2s/step - loss: 1.0090 - accuracy: 0.7237\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 26s 2s/step - loss: 1.0140 - accuracy: 0.7292\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 26s 2s/step - loss: 0.9953 - accuracy: 0.7306\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 26s 2s/step - loss: 0.9753 - accuracy: 0.7285\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 26s 2s/step - loss: 0.9757 - accuracy: 0.7369\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 26s 2s/step - loss: 0.9674 - accuracy: 0.7488\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 26s 2s/step - loss: 0.9589 - accuracy: 0.7425\n",
      "CPU times: user 1h 27min 46s, sys: 2h 7min 52s, total: 3h 35min 38s\n",
      "Wall time: 56min 41s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2e5b82b80>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# epochs ~ 100\n",
    "EPOCHS = 100\n",
    "# EPOCHS = 20\n",
    "model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FOLDER = \"./models\"\n",
    "INTENT_MODEL_FOLDER = \"intent\"\n",
    "MODEL_SAVE_FOLDER = os.path.join(MODEL_FOLDER, INTENT_MODEL_FOLDER)\n",
    "TF_MODEL_SAVE_FOLDER = os.path.join(MODEL_FOLDER, INTENT_MODEL_FOLDER, \"tf_serving\")\n",
    "MODEL_SAVE_PATH = os.path.join(MODEL_FOLDER, INTENT_MODEL_FOLDER, \"intent_model.h5\")\n",
    "# pickle files\n",
    "CUSTOM_OBJS_PKL_FILE = \"custom_objects.pickle\"\n",
    "CUSTOM_OBJS_PKL_PATH = os.path.join(MODEL_SAVE_FOLDER, CUSTOM_OBJS_PKL_FILE)\n",
    "CUSTOM_STOPWORDS_PKL_FILE = \"custom_stopwords.pickle\"\n",
    "CUSTOM_STOPWORDS_PKL_PATH = os.path.join(MODEL_SAVE_FOLDER, CUSTOM_STOPWORDS_PKL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = open(CUSTOM_OBJS_PKL_PATH, \"wb\")\n",
    "pickle.dump(tokenizer, pickle_file)\n",
    "pickle.dump(encoder, pickle_file)\n",
    "pickle.dump(max_tokens, pickle_file)\n",
    "pickle.dump(PADDING, pickle_file)\n",
    "pickle_file.close()\n",
    "\n",
    "pickle_file = open(CUSTOM_STOPWORDS_PKL_PATH, \"wb\")\n",
    "pickle.dump(custom_stopwords, pickle_file)\n",
    "pickle_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Metal device set to: Apple M1\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer last_lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-15 23:04:59.631014: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-03-15 23:04:59.631187: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(CUSTOM_OBJS_PKL_PATH, \"rb\")\n",
    "tokenizer = pickle.load(f)\n",
    "encoder = pickle.load(f)\n",
    "max_tokens = pickle.load(f)\n",
    "PADDING = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "pickle_file = open(CUSTOM_STOPWORDS_PKL_PATH, \"rb\")\n",
    "custom_stopwords = pickle.load(pickle_file)\n",
    "pickle_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Transfer Learning:\n",
    "# x_train_tokens = tokenizer.texts_to_sequences(x_train_final_input)\n",
    "# max_tokens = 0\n",
    "# for i in range(len(x_train_tokens)):\n",
    "#     if len(x_train_tokens[i]) > max_tokens:\n",
    "#         max_tokens = len(x_train_tokens[i])\n",
    "# pad = 'pre'\n",
    "\n",
    "# x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens,\n",
    "#                             padding=pad, truncating=pad)\n",
    "\n",
    "# y_train_pad = encoder.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_question=\"hello, how are you?\"\n",
    "# question=word_tokenize(user_question)\n",
    "# print(question)\n",
    "# doc=nlp(user_question)\n",
    "# print(doc)\n",
    "# for token in doc:\n",
    "#     print(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(user_text):\n",
    "    # lower case the text\n",
    "    text = lower_case_text(user_text)\n",
    "    # remove punctuation\n",
    "    text = text.translate(translator)\n",
    "    # remove stopwords\n",
    "    text = remove_custom_stopwords(text, custom_stopwords)\n",
    "\n",
    "    if not text:\n",
    "        text = user_text\n",
    "    # process the text using Spacy's language model \"en_core_web_sm\".\n",
    "    doc = nlp(text)\n",
    "    # lemmatization\n",
    "    text = [token.lemma_ for token in doc]\n",
    "    # detokenize text\n",
    "    text = detokenize_sent(text)\n",
    "    return text\n",
    "\n",
    "def preprocess_text_keeping_stopwords(user_text):\n",
    "    # lower case the text\n",
    "    text = lower_case_text(user_text)\n",
    "    # remove punctuation\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    if not text:\n",
    "        text = user_text\n",
    "    # process the text using Spacy's language model \"en_core_web_sm\".\n",
    "    doc = nlp(text)\n",
    "    # lemmatization\n",
    "    text = [token.lemma_ for token in doc]\n",
    "    # detokenize text\n",
    "    text = detokenize_sent(text)\n",
    "    return text\n",
    "\n",
    "def prepare_text_for_prediction(text):\n",
    "    tokens = tokenizer.texts_to_sequences([text])\n",
    "    padded_tokens = pad_sequences(tokens, maxlen=max_tokens, padding=PADDING, truncating=PADDING)\n",
    "    return padded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-15 23:05:04.142094: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-03-15 23:05:04.297179: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bot_challenge'] 0.9915664\n"
     ]
    }
   ],
   "source": [
    "user_text = preprocess_text(\"Are you a chat bot?\")\n",
    "tokens = prepare_text_for_prediction(user_text)\n",
    "\n",
    "predict_class = model.predict(tokens)\n",
    "\n",
    "intent = encoder.inverse_transform(predict_class)\n",
    "index_number = np.argmax(predict_class)\n",
    "intent_value = predict_class[0, index_number]\n",
    "\n",
    "print(intent, intent_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_intent(text):\n",
    "    text = preprocess_text(text)\n",
    "    tokens = prepare_text_for_prediction(text)\n",
    "\n",
    "    predict_class = model.predict(tokens)\n",
    "\n",
    "    intent = encoder.inverse_transform(predict_class)\n",
    "    index_number = np.argmax(predict_class)\n",
    "    intent_value = predict_class[0, index_number]\n",
    "\n",
    "    print(intent[0], \"\\t\\t\", \"{0:2f}\".format(intent_value * 100))\n",
    "    return intent, intent_value\n",
    "\n",
    "def predict_intent_without_stopwords(text):\n",
    "    text = preprocess_text_keeping_stopwords(text)\n",
    "    tokens = prepare_text_for_prediction(text)\n",
    "\n",
    "    predict_class = model.predict(tokens)\n",
    "\n",
    "    intent = encoder.inverse_transform(predict_class)\n",
    "    index_number = np.argmax(predict_class)\n",
    "    intent_value = predict_class[0, index_number]\n",
    "\n",
    "    print(intent[0], \"\\t\\t\", \"{0:2f}\".format(intent_value * 100))\n",
    "    return intent, intent_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affirm \t\t 96.504629\n",
      "deny \t\t 84.965497\n",
      "stop \t\t 91.731769\n",
      "greet \t\t 98.034072\n",
      "bot_challenge \t\t 99.586499\n",
      "explain_deep_learning \t\t 79.549509\n",
      "explain_intents \t\t 46.799067\n",
      "explain_keras \t\t 53.524405\n",
      "explain_nlp \t\t 57.634521\n",
      "explain_nlu \t\t 80.276471\n",
      "why_machine_learning \t\t 96.803355\n",
      "why_make_a_bot \t\t 55.054760\n",
      "when_will_you_beat_turing_test \t\t 89.668566\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(['when_will_you_beat_turing_test'], dtype='<U30'), 0.89668566)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_intent(\"yes\")\n",
    "predict_intent(\"no\")\n",
    "predict_intent(\"stop\")\n",
    "predict_intent(\"Hello\")\n",
    "predict_intent(\"Hi there, are you the bot?\")\n",
    "predict_intent(\"What is deep learning?\")\n",
    "predict_intent(\"What are intents?\")\n",
    "predict_intent(\"What is keras?\")\n",
    "predict_intent(\"what is nlp?\")\n",
    "predict_intent(\"what is natural language understanding?\")\n",
    "predict_intent(\"why use machine learning in bots?\")\n",
    "predict_intent(\"why make a chatbot?\")\n",
    "predict_intent(\"when will you beat the turing test?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-15 22:44:13.076533: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greet \t\t 98.008507\n",
      "greet \t\t 98.034072\n",
      "greet \t\t 97.517437\n",
      "greet \t\t 98.034072\n",
      "greet \t\t 77.985638\n",
      "greet \t\t 69.710457\n",
      "greet \t\t 95.797789\n",
      "greet \t\t 98.008507\n",
      "bye \t\t 31.080961\n",
      "greet \t\t 98.882711\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 81.903851\n",
      "greet \t\t 77.870661\n",
      "inform \t\t 81.903851\n",
      "greet \t\t 97.744775\n",
      "greet \t\t 98.850226\n",
      "greet \t\t 93.706691\n",
      "greet \t\t 97.236192\n",
      "greet \t\t 98.034072\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 81.903851\n",
      "greet \t\t 98.790610\n",
      "inform \t\t 81.903851\n",
      "greet \t\t 98.008507\n",
      "greet \t\t 93.317109\n",
      "greet \t\t 98.866177\n",
      "mood_ask \t\t 25.027093\n",
      "greet \t\t 92.142057\n",
      "greet \t\t 98.034072\n",
      "greet \t\t 73.429006\n",
      "greet \t\t 69.710457\n",
      "greet \t\t 97.517437\n",
      "greet \t\t 98.033637\n",
      "greet \t\t 96.237797\n",
      "greet \t\t 97.517437\n",
      "greet \t\t 97.517437\n",
      "mood_ask \t\t 25.027093\n",
      "greet \t\t 87.486720\n",
      "greet \t\t 98.034072\n",
      "inform \t\t 81.903851\n",
      "greet \t\t 98.208064\n",
      "greet \t\t 98.208064\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 81.903851\n",
      "explain_support_bot \t\t 29.974899\n",
      "greet \t\t 93.706691\n",
      "greet \t\t 98.882711\n",
      "greet \t\t 91.946518\n",
      "greet \t\t 98.679256\n",
      "greet \t\t 98.008507\n",
      "inform \t\t 81.903851\n",
      "greet \t\t 98.704416\n",
      "greet \t\t 98.564917\n",
      "greet \t\t 77.870661\n",
      "greet \t\t 98.742408\n",
      "greet \t\t 97.236192\n",
      "greet \t\t 97.517437\n",
      "greet \t\t 97.124344\n",
      "greet \t\t 97.875679\n",
      "inform \t\t 81.903851\n",
      "greet \t\t 98.219621\n",
      "greet \t\t 98.008507\n",
      "greet \t\t 98.878366\n",
      "greet \t\t 98.034072\n",
      "greet \t\t 97.517437\n",
      "greet \t\t 97.517437\n",
      "greet \t\t 98.850226\n",
      "greet \t\t 98.216033\n",
      "greet \t\t 98.686427\n",
      "greet \t\t 98.905689\n",
      "greet \t\t 97.517437\n",
      "greet \t\t 97.875679\n",
      "greet \t\t 97.744775\n",
      "greet \t\t 97.875679\n",
      "greet \t\t 97.744775\n",
      "greet \t\t 96.959019\n",
      "greet \t\t 98.034072\n",
      "greet \t\t 97.124344\n",
      "greet \t\t 97.517437\n",
      "greet \t\t 98.008507\n",
      "greet \t\t 98.270869\n",
      "greet \t\t 98.850226\n",
      "greet \t\t 98.034072\n",
      "greet \t\t 97.517437\n",
      "greet \t\t 98.344558\n",
      "greet \t\t 77.985638\n",
      "bye \t\t 34.288302\n",
      "mood_great \t\t 45.713747\n",
      "bye \t\t 41.570932\n",
      "greet \t\t 46.249717\n",
      "bye \t\t 70.945549\n",
      "bye \t\t 65.692198\n",
      "good_day \t\t 73.442268\n",
      "bye \t\t 42.968425\n",
      "bye \t\t 78.610206\n",
      "bye \t\t 54.179662\n",
      "good_day \t\t 73.442268\n",
      "good_day \t\t 89.241463\n",
      "good_day \t\t 85.876441\n",
      "good_day \t\t 34.055498\n",
      "greet \t\t 69.710457\n",
      "good_day \t\t 82.924259\n",
      "mood_great \t\t 61.458707\n",
      "affirm \t\t 45.584404\n",
      "mood_great \t\t 50.356245\n",
      "goodbye \t\t 25.672731\n",
      "mood_great \t\t 54.129648\n",
      "mood_great \t\t 26.261082\n",
      "mood_great \t\t 74.041384\n",
      "mood_great \t\t 73.190385\n",
      "mood_great \t\t 54.182816\n",
      "mood_great \t\t 59.912622\n",
      "mood_great \t\t 64.011377\n",
      "mood_great \t\t 61.458707\n",
      "mood_great \t\t 45.713747\n",
      "mood_great \t\t 61.458707\n",
      "mood_unhappy \t\t 77.955043\n",
      "mood_unhappy \t\t 70.068055\n",
      "deny \t\t 31.557396\n",
      "deny \t\t 39.137965\n",
      "mood_unhappy \t\t 86.463344\n",
      "mood_unhappy \t\t 70.068055\n",
      "mood_unhappy \t\t 83.000028\n",
      "mood_unhappy \t\t 83.000028\n",
      "mood_unhappy \t\t 61.684453\n",
      "mood_unhappy \t\t 41.714445\n",
      "mood_unhappy \t\t 41.714445\n",
      "mood_unhappy \t\t 81.829363\n",
      "deny \t\t 44.759727\n",
      "mood_unhappy \t\t 83.000028\n",
      "bot_challenge \t\t 99.063361\n",
      "bot_challenge \t\t 98.809427\n",
      "bot_challenge \t\t 95.637119\n",
      "bot_challenge \t\t 92.673433\n",
      "bot_challenge \t\t 99.586499\n",
      "bot_challenge \t\t 99.156642\n",
      "bot_challenge \t\t 96.488857\n",
      "bot_challenge \t\t 98.902965\n",
      "bot_challenge \t\t 98.569804\n",
      "bot_challenge \t\t 96.204168\n",
      "bot_challenge \t\t 76.249039\n",
      "bot_challenge \t\t 97.586936\n",
      "bot_challenge \t\t 99.063361\n",
      "bot_challenge \t\t 89.465892\n",
      "bot_challenge \t\t 98.340636\n",
      "bot_challenge \t\t 99.063361\n",
      "bot_challenge \t\t 99.091482\n",
      "mood_ask \t\t 30.001685\n",
      "bot_challenge \t\t 84.830260\n",
      "bot_challenge \t\t 96.094048\n",
      "bot_challenge \t\t 99.063361\n",
      "bot_challenge \t\t 99.063361\n",
      "bot_challenge \t\t 99.063361\n",
      "bot_challenge \t\t 99.450088\n",
      "bot_challenge \t\t 99.063361\n",
      "bot_challenge \t\t 99.202186\n",
      "bot_challenge \t\t 98.902965\n",
      "bot_challenge \t\t 92.697811\n",
      "bot_challenge \t\t 98.902965\n",
      "bot_challenge \t\t 98.578483\n",
      "bot_challenge \t\t 98.027015\n",
      "bot_challenge \t\t 84.387130\n",
      "bot_challenge \t\t 93.163610\n",
      "bot_challenge \t\t 87.337220\n",
      "bot_challenge \t\t 58.248967\n",
      "bot_challenge \t\t 98.578370\n",
      "bot_challenge \t\t 98.540074\n",
      "bot_challenge \t\t 98.387188\n",
      "affirm \t\t 96.504629\n",
      "affirm \t\t 97.351533\n",
      "affirm \t\t 96.504629\n",
      "affirm \t\t 96.504629\n",
      "affirm \t\t 91.699201\n",
      "affirm \t\t 52.251583\n",
      "affirm \t\t 91.448754\n",
      "affirm \t\t 43.921787\n",
      "affirm \t\t 89.585197\n",
      "affirm \t\t 88.255209\n",
      "affirm \t\t 61.179239\n",
      "inform \t\t 81.903851\n",
      "affirm \t\t 98.276514\n",
      "affirm \t\t 85.762250\n",
      "affirm \t\t 97.485173\n",
      "affirm \t\t 97.485173\n",
      "affirm \t\t 77.712369\n",
      "affirm \t\t 98.345572\n",
      "affirm \t\t 95.547229\n",
      "affirm \t\t 86.883438\n",
      "affirm \t\t 96.642894\n",
      "affirm \t\t 97.755551\n",
      "affirm \t\t 98.385596\n",
      "affirm \t\t 93.563795\n",
      "affirm \t\t 45.584404\n",
      "affirm \t\t 93.563795\n",
      "affirm \t\t 97.967875\n",
      "affirm \t\t 72.679222\n",
      "affirm \t\t 98.263055\n",
      "affirm \t\t 78.418493\n",
      "mood_great \t\t 50.356245\n",
      "mood_ask \t\t 87.091553\n",
      "affirm \t\t 90.351248\n",
      "affirm \t\t 91.630107\n",
      "affirm \t\t 88.432467\n",
      "affirm \t\t 46.144152\n",
      "affirm \t\t 91.630107\n",
      "affirm \t\t 97.469568\n",
      "affirm \t\t 89.852178\n",
      "affirm \t\t 82.171404\n",
      "mood_great \t\t 61.458707\n",
      "affirm \t\t 90.431648\n",
      "affirm \t\t 91.858739\n",
      "affirm \t\t 96.272796\n",
      "affirm \t\t 78.023207\n",
      "affirm \t\t 90.134704\n",
      "affirm \t\t 85.692757\n",
      "affirm \t\t 87.336135\n",
      "affirm \t\t 95.460665\n",
      "affirm \t\t 96.504629\n",
      "affirm \t\t 95.923752\n",
      "affirm \t\t 98.342758\n",
      "affirm \t\t 95.206839\n",
      "affirm \t\t 95.073235\n",
      "bye \t\t 32.857904\n",
      "affirm \t\t 90.351248\n",
      "affirm \t\t 96.614498\n",
      "affirm \t\t 96.223027\n",
      "affirm \t\t 91.630107\n",
      "affirm \t\t 96.504629\n",
      "affirm \t\t 98.416489\n",
      "affirm \t\t 97.485173\n",
      "affirm \t\t 91.448754\n",
      "affirm \t\t 96.504629\n",
      "affirm \t\t 91.448754\n",
      "affirm \t\t 95.222551\n",
      "affirm \t\t 89.852178\n",
      "affirm \t\t 96.504629\n",
      "affirm \t\t 89.852178\n",
      "affirm \t\t 97.485173\n",
      "affirm \t\t 95.460665\n",
      "affirm \t\t 96.504629\n",
      "affirm \t\t 91.448754\n",
      "affirm \t\t 95.222551\n",
      "affirm \t\t 98.416489\n",
      "bye \t\t 70.945549\n",
      "inform \t\t 81.903851\n",
      "bye \t\t 88.849318\n",
      "bye \t\t 65.692198\n",
      "bye \t\t 65.692198\n",
      "bye \t\t 70.945549\n",
      "bye \t\t 78.610206\n",
      "bye \t\t 91.712368\n",
      "bye \t\t 88.849318\n",
      "mood_ask \t\t 43.395969\n",
      "bye \t\t 70.945549\n",
      "bye \t\t 54.179662\n",
      "inform \t\t 81.903851\n",
      "bye \t\t 65.692198\n",
      "bye \t\t 68.718809\n",
      "bye \t\t 62.140220\n",
      "greet \t\t 46.249717\n",
      "bye \t\t 49.261737\n",
      "mood_ask \t\t 43.395969\n",
      "inform \t\t 81.903851\n",
      "bye \t\t 78.610206\n",
      "stop \t\t 34.424871\n",
      "bye \t\t 65.632355\n",
      "bye \t\t 77.369499\n",
      "bye \t\t 80.420464\n",
      "bye \t\t 70.945549\n",
      "bye \t\t 89.530933\n",
      "bye \t\t 90.637839\n",
      "bye \t\t 61.447245\n",
      "bye \t\t 75.002837\n",
      "bye \t\t 85.684872\n",
      "bye \t\t 87.754548\n",
      "deny \t\t 89.263386\n",
      "deny \t\t 46.047401\n",
      "deny \t\t 54.439145\n",
      "deny \t\t 91.720867\n",
      "deny \t\t 91.198575\n",
      "deny \t\t 84.965497\n",
      "deny \t\t 95.154870\n",
      "deny \t\t 93.476439\n",
      "deny \t\t 88.786870\n",
      "deny \t\t 65.174699\n",
      "deny \t\t 60.881305\n",
      "deny \t\t 46.047401\n",
      "deny \t\t 89.598715\n",
      "deny \t\t 74.486774\n",
      "deny \t\t 94.411510\n",
      "deny \t\t 87.761652\n",
      "deny \t\t 95.154870\n",
      "deny \t\t 74.383640\n",
      "deny \t\t 84.965497\n",
      "deny \t\t 40.307277\n",
      "deny \t\t 93.314099\n",
      "deny \t\t 77.657121\n",
      "deny \t\t 96.292394\n",
      "deny \t\t 96.706390\n",
      "deny \t\t 86.107922\n",
      "deny \t\t 84.965497\n",
      "deny \t\t 77.657121\n",
      "deny \t\t 96.292394\n",
      "deny \t\t 96.706390\n",
      "deny \t\t 94.536453\n",
      "deny \t\t 87.499887\n",
      "deny \t\t 56.131572\n",
      "deny \t\t 93.314099\n",
      "deny \t\t 51.201391\n",
      "deny \t\t 84.965497\n",
      "deny \t\t 95.696306\n",
      "deny \t\t 96.292394\n",
      "deny \t\t 77.657121\n",
      "deny \t\t 93.314099\n",
      "deny \t\t 78.369284\n",
      "deny \t\t 88.836455\n",
      "deny \t\t 90.707523\n",
      "deny \t\t 96.064746\n",
      "deny \t\t 96.053779\n",
      "deny \t\t 97.354162\n",
      "deny \t\t 91.826463\n",
      "deny \t\t 66.640347\n",
      "deny \t\t 97.065556\n",
      "deny \t\t 94.478077\n",
      "deny \t\t 70.840073\n",
      "stop \t\t 92.450124\n",
      "stop \t\t 91.731769\n",
      "stop \t\t 92.076373\n",
      "stop \t\t 91.731769\n",
      "stop \t\t 92.450124\n",
      "stop \t\t 98.736948\n",
      "stop \t\t 98.549068\n",
      "stop \t\t 97.357452\n",
      "stop \t\t 95.968890\n",
      "stop \t\t 85.934544\n",
      "stop \t\t 99.086130\n",
      "stop \t\t 91.731769\n",
      "stop \t\t 99.030566\n",
      "stop \t\t 85.934544\n",
      "stop \t\t 95.968890\n",
      "stop \t\t 99.030566\n",
      "stop \t\t 92.450124\n",
      "stop \t\t 99.086130\n",
      "stop \t\t 92.410016\n",
      "stop \t\t 85.934544\n",
      "stop \t\t 85.934544\n",
      "stop \t\t 91.731769\n",
      "stop \t\t 98.549068\n",
      "stop \t\t 91.731769\n",
      "stop \t\t 97.357452\n",
      "stop \t\t 91.731769\n",
      "stop \t\t 92.410016\n",
      "stop \t\t 92.450124\n",
      "stop \t\t 98.736948\n",
      "good_day \t\t 73.442268\n",
      "good_day \t\t 89.241463\n",
      "good_day \t\t 85.876441\n",
      "good_day \t\t 78.365147\n",
      "greet \t\t 69.710457\n",
      "good_day \t\t 82.924259\n",
      "handle_insult \t\t 76.181102\n",
      "handle_insult \t\t 83.715743\n",
      "affirm \t\t 34.677362\n",
      "deny \t\t 40.511486\n",
      "handle_insult \t\t 50.327259\n",
      "deny \t\t 42.567852\n",
      "handle_insult \t\t 96.268952\n",
      "deny \t\t 42.567852\n",
      "handle_insult \t\t 80.991381\n",
      "deny \t\t 42.802426\n",
      "handle_insult \t\t 83.715743\n",
      "handle_insult \t\t 96.268952\n",
      "bot_challenge \t\t 58.248967\n",
      "handle_insult \t\t 85.812134\n",
      "handle_insult \t\t 80.991381\n",
      "handle_insult \t\t 56.938821\n",
      "human_handoff \t\t 99.751854\n",
      "human_handoff \t\t 99.751854\n",
      "human_handoff \t\t 99.874467\n",
      "human_handoff \t\t 99.606258\n",
      "human_handoff \t\t 99.776107\n",
      "human_handoff \t\t 97.481734\n",
      "human_handoff \t\t 99.396181\n",
      "human_handoff \t\t 98.111665\n",
      "human_handoff \t\t 99.229103\n",
      "human_handoff \t\t 99.788046\n",
      "human_handoff \t\t 99.949324\n",
      "human_handoff \t\t 95.890772\n",
      "human_handoff \t\t 97.541082\n",
      "human_handoff \t\t 99.396181\n",
      "human_handoff \t\t 86.756867\n",
      "human_handoff \t\t 99.679655\n",
      "human_handoff \t\t 96.896726\n",
      "human_handoff \t\t 99.731052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_handoff \t\t 99.451315\n",
      "human_handoff \t\t 99.852091\n",
      "human_handoff \t\t 99.876225\n",
      "human_handoff \t\t 99.754560\n",
      "human_handoff \t\t 99.543577\n",
      "human_handoff \t\t 95.890772\n",
      "human_handoff \t\t 96.092772\n",
      "human_handoff \t\t 83.979863\n",
      "human_handoff \t\t 99.919921\n",
      "human_handoff \t\t 95.936608\n",
      "human_handoff \t\t 98.350877\n",
      "human_handoff \t\t 97.616816\n",
      "human_handoff \t\t 99.707735\n",
      "human_handoff \t\t 99.278617\n",
      "human_handoff \t\t 81.508464\n",
      "human_handoff \t\t 99.855572\n",
      "human_handoff \t\t 99.857748\n",
      "human_handoff \t\t 99.951649\n",
      "thank \t\t 95.527053\n",
      "thank \t\t 97.538924\n",
      "thank \t\t 98.091459\n",
      "thank \t\t 94.660139\n",
      "thank \t\t 97.085249\n",
      "thank \t\t 98.177850\n",
      "thank \t\t 95.527053\n",
      "thank \t\t 98.091459\n",
      "thank \t\t 95.527053\n",
      "thank \t\t 95.527053\n",
      "thank \t\t 95.527053\n",
      "thank \t\t 98.428822\n",
      "thank \t\t 96.058774\n",
      "thank \t\t 98.091459\n",
      "thank \t\t 95.527053\n",
      "thank \t\t 97.085249\n",
      "thank \t\t 98.620307\n",
      "thank \t\t 97.386748\n",
      "thank \t\t 98.423171\n",
      "thank \t\t 98.078674\n",
      "thank \t\t 97.874773\n",
      "thank \t\t 72.698218\n",
      "thank \t\t 97.470957\n",
      "thank \t\t 98.846275\n",
      "thank \t\t 98.402196\n",
      "thank \t\t 91.796118\n",
      "inform \t\t 99.916339\n",
      "inform \t\t 99.912220\n",
      "inform \t\t 99.950862\n",
      "inform \t\t 99.514991\n",
      "inform \t\t 99.730140\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 98.731381\n",
      "inform \t\t 99.935597\n",
      "inform \t\t 99.968910\n",
      "inform \t\t 96.074504\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.904382\n",
      "inform \t\t 99.899846\n",
      "inform \t\t 98.360389\n",
      "inform \t\t 99.870825\n",
      "inform \t\t 99.952936\n",
      "inform \t\t 99.949419\n",
      "inform \t\t 97.545642\n",
      "inform \t\t 99.658954\n",
      "inform \t\t 99.934608\n",
      "inform \t\t 99.853539\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 97.952342\n",
      "inform \t\t 99.921137\n",
      "inform \t\t 99.915946\n",
      "inform \t\t 99.805981\n",
      "inform \t\t 99.929476\n",
      "inform \t\t 99.955684\n",
      "inform \t\t 99.737018\n",
      "inform \t\t 99.098992\n",
      "inform \t\t 99.966896\n",
      "inform \t\t 99.866903\n",
      "inform \t\t 99.825072\n",
      "inform \t\t 99.925584\n",
      "deny \t\t 48.240152\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.963474\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 53.928071\n",
      "inform \t\t 99.011970\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.945086\n",
      "inform \t\t 99.385440\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.946302\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.872470\n",
      "inform \t\t 99.917793\n",
      "inform \t\t 99.902201\n",
      "inform \t\t 98.560429\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.950302\n",
      "inform \t\t 99.875057\n",
      "inform \t\t 99.036229\n",
      "inform \t\t 98.795342\n",
      "inform \t\t 98.686171\n",
      "inform \t\t 99.721485\n",
      "human_handoff \t\t 75.517994\n",
      "inform \t\t 99.925280\n",
      "inform \t\t 99.694520\n",
      "inform \t\t 99.526310\n",
      "inform \t\t 99.859023\n",
      "inform \t\t 99.502528\n",
      "inform \t\t 98.996162\n",
      "inform \t\t 99.916792\n",
      "inform \t\t 98.850375\n",
      "inform \t\t 98.996162\n",
      "inform \t\t 99.739432\n",
      "inform \t\t 99.806684\n",
      "inform \t\t 99.935263\n",
      "ask_builder \t\t 40.790656\n",
      "inform \t\t 99.837393\n",
      "mood_ask \t\t 45.525703\n",
      "inform \t\t 99.855584\n",
      "inform \t\t 99.784791\n",
      "inform \t\t 99.703705\n",
      "inform \t\t 99.915397\n",
      "inform \t\t 73.938531\n",
      "inform \t\t 99.924338\n",
      "inform \t\t 99.928659\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.916792\n",
      "inform \t\t 97.545642\n",
      "inform \t\t 99.895507\n",
      "inform \t\t 99.613380\n",
      "inform \t\t 99.865586\n",
      "inform \t\t 99.956459\n",
      "inform \t\t 99.875426\n",
      "inform \t\t 99.914718\n",
      "inform \t\t 99.920279\n",
      "inform \t\t 98.981935\n",
      "inform \t\t 99.861294\n",
      "inform \t\t 99.879932\n",
      "inform \t\t 99.907368\n",
      "inform \t\t 94.870263\n",
      "inform \t\t 89.175564\n",
      "inform \t\t 99.929410\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.226379\n",
      "inform \t\t 95.878702\n",
      "inform \t\t 99.895364\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.902451\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.882919\n",
      "inform \t\t 99.858201\n",
      "inform \t\t 99.921137\n",
      "inform \t\t 99.936169\n",
      "inform \t\t 99.958915\n",
      "inform \t\t 95.648450\n",
      "inform \t\t 99.839449\n",
      "inform \t\t 99.699223\n",
      "inform \t\t 99.860471\n",
      "inform \t\t 99.910641\n",
      "inform \t\t 99.943799\n",
      "inform \t\t 99.934500\n",
      "inform \t\t 99.934500\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.742401\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.960977\n",
      "inform \t\t 97.017050\n",
      "inform \t\t 99.789709\n",
      "inform \t\t 99.953914\n",
      "inform \t\t 99.864525\n",
      "inform \t\t 98.573464\n",
      "inform \t\t 98.573464\n",
      "inform \t\t 99.950516\n",
      "inform \t\t 99.921572\n",
      "inform \t\t 99.466586\n",
      "inform \t\t 99.865550\n",
      "inform \t\t 99.898952\n",
      "inform \t\t 99.935836\n",
      "inform \t\t 99.939382\n",
      "inform \t\t 99.946433\n",
      "inform \t\t 99.893373\n",
      "inform \t\t 99.899477\n",
      "inform \t\t 99.759632\n",
      "inform \t\t 99.763405\n",
      "inform \t\t 98.907030\n",
      "inform \t\t 99.922657\n",
      "inform \t\t 99.535936\n",
      "inform \t\t 99.948663\n",
      "inform \t\t 99.931812\n",
      "inform \t\t 99.634331\n",
      "inform \t\t 99.751014\n",
      "inform \t\t 99.842250\n",
      "inform \t\t 99.110633\n",
      "inform \t\t 99.805129\n",
      "inform \t\t 99.802661\n",
      "inform \t\t 99.792415\n",
      "inform \t\t 99.850547\n",
      "inform \t\t 76.096308\n",
      "inform \t\t 99.890769\n",
      "inform \t\t 99.887961\n",
      "inform \t\t 99.857557\n",
      "inform \t\t 99.696028\n",
      "inform \t\t 99.921572\n",
      "inform \t\t 99.675369\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.397439\n",
      "inform \t\t 99.176520\n",
      "inform \t\t 99.908233\n",
      "inform \t\t 99.922872\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.930048\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.837011\n",
      "inform \t\t 99.927610\n",
      "inform \t\t 99.954832\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.863774\n",
      "inform \t\t 99.929386\n",
      "inform \t\t 99.393272\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.673814\n",
      "inform \t\t 99.763906\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.849677\n",
      "inform \t\t 99.822897\n",
      "inform \t\t 99.888891\n",
      "inform \t\t 99.339068\n",
      "inform \t\t 99.927062\n",
      "inform \t\t 90.093929\n",
      "inform \t\t 99.921679\n",
      "inform \t\t 99.935192\n",
      "inform \t\t 99.933529\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.823976\n",
      "inform \t\t 99.623352\n",
      "inform \t\t 99.931610\n",
      "inform \t\t 99.028742\n",
      "inform \t\t 99.466670\n",
      "inform \t\t 99.886370\n",
      "inform \t\t 99.927109\n",
      "inform \t\t 99.891210\n",
      "inform \t\t 99.961370\n",
      "inform \t\t 99.944764\n",
      "inform \t\t 99.821448\n",
      "inform \t\t 99.838603\n",
      "inform \t\t 99.828660\n",
      "inform \t\t 99.763185\n",
      "inform \t\t 99.895269\n",
      "inform \t\t 99.880195\n",
      "inform \t\t 99.958748\n",
      "inform \t\t 99.706250\n",
      "inform \t\t 99.212271\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 88.384545\n",
      "inform \t\t 99.294645\n",
      "inform \t\t 97.569650\n",
      "inform \t\t 99.915946\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.520844\n",
      "inform \t\t 99.520844\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.739909\n",
      "inform \t\t 98.989505\n",
      "inform \t\t 99.953699\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.769413\n",
      "inform \t\t 99.845243\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.895340\n",
      "inform \t\t 99.898762\n",
      "inform \t\t 99.949646\n",
      "inform \t\t 99.789673\n",
      "inform \t\t 99.673426\n",
      "inform \t\t 99.651557\n",
      "inform \t\t 98.354822\n",
      "inform \t\t 99.914503\n",
      "inform \t\t 99.911362\n",
      "inform \t\t 99.876559\n",
      "inform \t\t 99.891627\n",
      "inform \t\t 99.513453\n",
      "inform \t\t 99.874502\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.924672\n",
      "inform \t\t 99.574989\n",
      "inform \t\t 99.879175\n",
      "inform \t\t 99.929458\n",
      "inform \t\t 99.898666\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 98.493052\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.789709\n",
      "inform \t\t 99.789709\n",
      "inform \t\t 98.540187\n",
      "inform \t\t 99.789709\n",
      "inform \t\t 98.622185\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.943292\n",
      "inform \t\t 99.842238\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.880004\n",
      "inform \t\t 99.291068\n",
      "inform \t\t 98.497260\n",
      "inform \t\t 90.974128\n",
      "inform \t\t 99.862647\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 97.900325\n",
      "inform \t\t 99.806672\n",
      "inform \t\t 99.727243\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 84.245032\n",
      "inform \t\t 99.501312\n",
      "inform \t\t 99.822301\n",
      "inform \t\t 99.907213\n",
      "inform \t\t 99.912649\n",
      "inform \t\t 99.965775\n",
      "inform \t\t 99.789709\n",
      "inform \t\t 99.935192\n",
      "inform \t\t 99.701560\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.880004\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.098992\n",
      "deny \t\t 48.512366\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 98.685580\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.828720\n",
      "inform \t\t 99.931717\n",
      "inform \t\t 99.113995\n",
      "inform \t\t 98.686171\n",
      "inform \t\t 99.713528\n",
      "inform \t\t 97.175330\n",
      "inform \t\t 98.573464\n",
      "inform \t\t 99.803060\n",
      "inform \t\t 99.694425\n",
      "inform \t\t 99.937087\n",
      "inform \t\t 98.319256\n",
      "inform \t\t 93.476081\n",
      "inform \t\t 99.342352\n",
      "inform \t\t 93.766147\n",
      "inform \t\t 99.865586\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.717325\n",
      "inform \t\t 99.595547\n",
      "inform \t\t 98.219275\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 94.615185\n",
      "inform \t\t 99.834740\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.551481\n",
      "inform \t\t 99.930501\n",
      "inform \t\t 99.929643\n",
      "inform \t\t 99.802029\n",
      "inform \t\t 99.812788\n",
      "inform \t\t 92.090172\n",
      "inform \t\t 99.936169\n",
      "inform \t\t 99.892449\n",
      "inform \t\t 99.955243\n",
      "inform \t\t 97.071767\n",
      "inform \t\t 99.892449\n",
      "inform \t\t 99.954212\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.936169\n",
      "inform \t\t 99.802434\n",
      "inform \t\t 99.898762\n",
      "inform \t\t 99.050564\n",
      "inform \t\t 99.846494\n",
      "inform \t\t 99.805129\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 98.732579\n",
      "inform \t\t 99.789709\n",
      "inform \t\t 99.948865\n",
      "inform \t\t 99.929160\n",
      "inform \t\t 99.754673\n",
      "inform \t\t 97.545642\n",
      "inform \t\t 99.876642\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 98.573464\n",
      "inform \t\t 99.896550\n",
      "inform \t\t 97.973871\n",
      "inform \t\t 99.781257\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 81.903851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inform \t\t 99.679285\n",
      "inform \t\t 99.939406\n",
      "inform \t\t 99.891019\n",
      "inform \t\t 99.346834\n",
      "inform \t\t 99.840814\n",
      "inform \t\t 99.821603\n",
      "inform \t\t 98.452526\n",
      "inform \t\t 99.922538\n",
      "inform \t\t 99.688458\n",
      "inform \t\t 99.893862\n",
      "inform \t\t 99.891782\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.873227\n",
      "inform \t\t 97.142845\n",
      "inform \t\t 99.899465\n",
      "inform \t\t 98.552471\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.517822\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.909008\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 90.093929\n",
      "inform \t\t 99.893957\n",
      "inform \t\t 99.864668\n",
      "inform \t\t 99.855191\n",
      "inform \t\t 88.144368\n",
      "inform \t\t 60.842258\n",
      "inform \t\t 99.158013\n",
      "inform \t\t 99.859339\n",
      "inform \t\t 94.870263\n",
      "inform \t\t 61.220664\n",
      "inform \t\t 99.895340\n",
      "inform \t\t 52.154785\n",
      "inform \t\t 87.658477\n",
      "inform \t\t 99.810296\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.344856\n",
      "inform \t\t 99.514991\n",
      "inform \t\t 97.650701\n",
      "inform \t\t 99.922872\n",
      "inform \t\t 99.805129\n",
      "inform \t\t 99.761665\n",
      "inform \t\t 88.981962\n",
      "inform \t\t 99.555403\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.901438\n",
      "inform \t\t 99.810332\n",
      "inform \t\t 99.922168\n",
      "inform \t\t 81.903851\n",
      "inform \t\t 99.893051\n",
      "inform \t\t 99.862647\n",
      "inform \t\t 81.903851\n",
      "mood_ask \t\t 98.100680\n",
      "mood_ask \t\t 87.522739\n",
      "mood_ask \t\t 95.233136\n",
      "mood_ask \t\t 93.781489\n",
      "mood_ask \t\t 98.773569\n",
      "mood_ask \t\t 98.761755\n",
      "mood_ask \t\t 98.726326\n",
      "mood_ask \t\t 79.048371\n",
      "mood_ask \t\t 93.781489\n",
      "mood_ask \t\t 30.649695\n",
      "mood_ask \t\t 79.467452\n",
      "mood_ask \t\t 35.768875\n",
      "mood_ask \t\t 79.728800\n",
      "mood_ask \t\t 86.800981\n",
      "mood_ask \t\t 85.276324\n",
      "mood_ask \t\t 46.873569\n",
      "handle_insult \t\t 38.389003\n",
      "mood_ask \t\t 91.689748\n",
      "mood_ask \t\t 98.773569\n",
      "mood_ask \t\t 97.809464\n",
      "mood_ask \t\t 99.165815\n",
      "mood_ask \t\t 98.740828\n",
      "mood_ask \t\t 94.835901\n",
      "mood_ask \t\t 95.233136\n",
      "mood_ask \t\t 92.651802\n",
      "mood_ask \t\t 98.586130\n",
      "mood_ask \t\t 83.897877\n",
      "mood_ask \t\t 98.100680\n",
      "mood_ask \t\t 95.233136\n",
      "mood_ask \t\t 98.100680\n",
      "mood_ask \t\t 25.027093\n",
      "mood_ask \t\t 30.649695\n",
      "mood_ask \t\t 35.768875\n",
      "mood_ask \t\t 95.962411\n",
      "mood_ask \t\t 98.475629\n",
      "mood_ask \t\t 79.048371\n",
      "mood_ask \t\t 93.781489\n",
      "mood_ask \t\t 98.773569\n",
      "mood_ask \t\t 98.726326\n",
      "mood_ask \t\t 87.522739\n",
      "mood_ask \t\t 89.108175\n",
      "inform \t\t 81.903851\n",
      "affirm \t\t 54.641193\n",
      "mood_ask \t\t 69.667357\n",
      "affirm \t\t 25.658143\n",
      "mood_ask \t\t 98.905295\n",
      "mood_ask \t\t 89.512432\n",
      "mood_ask \t\t 89.108175\n",
      "mood_ask \t\t 88.662058\n",
      "mood_ask \t\t 85.911745\n",
      "mood_ask \t\t 98.106134\n",
      "mood_ask \t\t 98.583305\n",
      "mood_ask \t\t 98.475629\n",
      "mood_ask \t\t 98.100680\n",
      "mood_ask \t\t 25.027093\n",
      "mood_ask \t\t 52.549440\n",
      "mood_ask \t\t 77.287549\n",
      "mood_ask \t\t 96.682024\n",
      "mood_ask \t\t 98.100680\n",
      "mood_ask \t\t 98.843414\n",
      "mood_ask \t\t 98.100680\n",
      "mood_ask \t\t 98.100680\n",
      "nice_talking_to_you \t\t 88.790148\n",
      "nice_talking_to_you \t\t 91.013950\n",
      "nice_talking_to_you \t\t 85.319132\n",
      "nice_talking_to_you \t\t 92.093998\n",
      "feedback \t\t 58.837694\n",
      "feedback \t\t 94.719392\n",
      "feedback \t\t 91.911620\n",
      "feedback \t\t 87.751883\n",
      "feedback \t\t 97.229540\n",
      "feedback \t\t 99.399781\n",
      "feedback \t\t 88.966399\n",
      "feedback \t\t 99.178737\n",
      "feedback \t\t 97.726840\n",
      "feedback \t\t 91.257745\n",
      "feedback \t\t 70.899618\n",
      "feedback \t\t 98.149168\n",
      "feedback \t\t 99.367589\n",
      "feedback \t\t 98.284942\n",
      "feedback \t\t 98.939115\n",
      "feedback \t\t 91.706198\n",
      "feedback \t\t 97.506839\n",
      "feedback \t\t 98.664993\n",
      "feedback \t\t 93.350965\n",
      "feedback \t\t 98.845369\n",
      "feedback \t\t 99.169874\n",
      "feedback \t\t 99.337244\n",
      "feedback \t\t 99.084949\n",
      "feedback \t\t 99.419242\n",
      "feedback \t\t 98.980415\n",
      "feedback \t\t 98.976141\n",
      "ask_builder \t\t 97.561818\n",
      "ask_builder \t\t 99.131119\n",
      "ask_builder \t\t 99.660212\n",
      "ask_builder \t\t 96.185005\n",
      "ask_builder \t\t 51.486874\n",
      "ask_builder \t\t 99.486369\n",
      "ask_builder \t\t 99.607599\n",
      "ask_builder \t\t 99.606144\n",
      "ask_builder \t\t 95.606816\n",
      "ask_builder \t\t 99.621552\n",
      "ask_builder \t\t 98.762614\n",
      "ask_builder \t\t 99.660212\n",
      "ask_builder \t\t 97.667557\n",
      "ask_builder \t\t 99.417698\n",
      "ask_builder \t\t 97.987181\n",
      "ask_builder \t\t 99.581522\n",
      "ask_builder \t\t 99.675494\n",
      "ask_builder \t\t 99.498230\n",
      "ask_builder \t\t 99.682856\n",
      "ask_builder \t\t 97.309202\n",
      "ask_builder \t\t 99.657887\n",
      "ask_builder \t\t 99.164420\n",
      "ask_builder \t\t 96.787226\n",
      "ask_builder \t\t 99.403811\n",
      "ask_builder \t\t 98.837703\n",
      "ask_builder \t\t 97.613484\n",
      "ask_builder \t\t 97.667557\n",
      "ask_builder \t\t 99.425417\n",
      "ask_builder \t\t 99.612898\n",
      "ask_builder \t\t 96.075153\n",
      "ask_builder \t\t 99.656290\n",
      "ask_builder \t\t 99.687403\n",
      "ask_builder \t\t 83.181947\n",
      "ask_builder \t\t 96.787226\n",
      "ask_builder \t\t 53.171593\n",
      "ask_builder \t\t 39.491072\n",
      "ask_builder \t\t 92.612320\n",
      "ask_builder \t\t 98.837703\n",
      "ask_builder \t\t 99.357080\n",
      "ask_whoisit \t\t 96.855789\n",
      "ask_builder \t\t 84.722298\n",
      "ask_builder \t\t 99.612898\n",
      "ask_builder \t\t 99.370027\n",
      "ask_builder \t\t 98.938692\n",
      "ask_builder \t\t 98.453611\n",
      "ask_builder \t\t 99.452788\n",
      "ask_howold \t\t 41.838026\n",
      "ask_howold \t\t 86.046696\n",
      "ask_howold \t\t 90.830803\n",
      "ask_weather \t\t 25.262362\n",
      "ask_time \t\t 96.466744\n",
      "ask_time \t\t 68.404555\n",
      "ask_time \t\t 98.750383\n",
      "ask_time \t\t 98.580104\n",
      "ask_time \t\t 99.390948\n",
      "ask_time \t\t 97.637117\n",
      "ask_time \t\t 96.320122\n",
      "ask_time \t\t 99.041021\n",
      "ask_time \t\t 95.807713\n",
      "ask_time \t\t 98.404551\n",
      "ask_time \t\t 98.750383\n",
      "ask_time \t\t 99.448943\n",
      "ask_time \t\t 99.390948\n",
      "ask_time \t\t 99.272233\n",
      "ask_time \t\t 99.315017\n",
      "ask_time \t\t 98.457599\n",
      "ask_time \t\t 99.143046\n",
      "ask_time \t\t 97.637117\n",
      "ask_time \t\t 98.580408\n",
      "ask_time \t\t 56.771260\n",
      "ask_time \t\t 98.632753\n",
      "ask_time \t\t 99.033093\n",
      "ask_time \t\t 98.889369\n",
      "ask_time \t\t 98.750383\n",
      "ask_time \t\t 98.750383\n",
      "ask_time \t\t 98.885638\n",
      "ask_time \t\t 89.095682\n",
      "ask_time \t\t 98.719680\n",
      "ask_time \t\t 99.392754\n",
      "ask_time \t\t 98.885638\n",
      "ask_time \t\t 98.572010\n",
      "ask_time \t\t 89.095682\n",
      "ask_time \t\t 98.750383\n",
      "ask_time \t\t 99.390948\n",
      "ask_time \t\t 68.404555\n",
      "ask_time \t\t 99.033093\n",
      "ask_time \t\t 99.272233\n",
      "ask_time \t\t 97.205102\n",
      "ask_time \t\t 98.580408\n",
      "ask_time \t\t 98.676199\n",
      "ask_time \t\t 96.014726\n",
      "ask_time \t\t 98.580104\n",
      "ask_time \t\t 99.053711\n",
      "ask_time \t\t 98.924947\n",
      "ask_time \t\t 98.750383\n",
      "ask_time \t\t 98.104328\n",
      "good_day \t\t 56.064177\n",
      "ask_weather \t\t 98.355442\n",
      "ask_weather \t\t 86.164182\n",
      "ask_weather \t\t 98.995250\n",
      "ask_weather \t\t 96.087545\n",
      "ask_weather \t\t 92.064333\n",
      "ask_weather \t\t 95.579821\n",
      "ask_weather \t\t 96.105289\n",
      "ask_weather \t\t 98.929113\n",
      "ask_weather \t\t 88.298249\n",
      "ask_weather \t\t 92.712015\n",
      "ask_weather \t\t 76.805657\n",
      "ask_weather \t\t 98.828679\n",
      "ask_weather \t\t 94.563317\n",
      "ask_weather \t\t 73.126715\n",
      "mood_ask \t\t 42.488241\n",
      "ask_weather \t\t 78.790289\n",
      "ask_weather \t\t 99.077004\n",
      "ask_weather \t\t 95.255578\n",
      "ask_weather \t\t 98.741698\n",
      "ask_weather \t\t 99.092019\n",
      "ask_weather \t\t 99.116921\n",
      "ask_weather \t\t 95.899701\n",
      "ask_weather \t\t 98.956400\n",
      "ask_weather \t\t 99.326986\n",
      "ask_weather \t\t 98.681706\n",
      "ask_weather \t\t 95.255578\n",
      "ask_weather \t\t 98.681706\n",
      "ask_weather \t\t 95.255578\n",
      "ask_weather \t\t 99.326986\n",
      "ask_weather \t\t 95.255578\n",
      "ask_weather \t\t 76.146060\n",
      "ask_weather \t\t 99.230498\n",
      "ask_weather \t\t 89.758939\n",
      "ask_weather \t\t 97.670174\n",
      "ask_weather \t\t 72.426707\n",
      "ask_weather \t\t 94.048446\n",
      "ask_weather \t\t 98.798263\n",
      "ask_weather \t\t 95.255578\n",
      "ask_weather \t\t 99.189430\n",
      "ask_whatismyname \t\t 92.246073\n",
      "ask_whatismyname \t\t 95.061165\n",
      "ask_whatismyname \t\t 93.725055\n",
      "ask_whatismyname \t\t 98.050940\n",
      "ask_whatismyname \t\t 90.880507\n",
      "ask_whatismyname \t\t 96.083069\n",
      "ask_whatismyname \t\t 90.880507\n",
      "ask_whatismyname \t\t 93.133056\n",
      "ask_whatismyname \t\t 87.545085\n",
      "ask_whatismyname \t\t 94.092268\n",
      "ask_whatismyname \t\t 95.702887\n",
      "ask_whatismyname \t\t 96.661234\n",
      "ask_whatismyname \t\t 93.236518\n",
      "ask_whatismyname \t\t 97.730815\n",
      "ask_whatismyname \t\t 97.835481\n",
      "ask_whatismyname \t\t 90.880507\n",
      "ask_whatismyname \t\t 96.451336\n",
      "ask_whatismyname \t\t 90.880507\n",
      "ask_whatismyname \t\t 97.535241\n",
      "ask_whatismyname \t\t 98.050940\n",
      "ask_whatismyname \t\t 89.670414\n",
      "ask_whatismyname \t\t 96.451336\n",
      "ask_whatismyname \t\t 88.885480\n",
      "ask_whatismyname \t\t 97.349203\n",
      "ask_whatismyname \t\t 96.932441\n",
      "ask_whatismyname \t\t 95.481288\n",
      "ask_whatismyname \t\t 83.959943\n",
      "ask_whatismyname \t\t 89.121783\n",
      "ask_whatismyname \t\t 91.800517\n",
      "ask_whatismyname \t\t 98.396760\n",
      "ask_whatismyname \t\t 76.235962\n",
      "ask_whatismyname \t\t 96.509814\n",
      "ask_whatismyname \t\t 97.092944\n",
      "ask_whatismyname \t\t 96.147907\n",
      "ask_whatismyname \t\t 90.402997\n",
      "ask_whatismyname \t\t 94.092268\n",
      "ask_whatismyname \t\t 93.038565\n",
      "ask_whatismyname \t\t 92.696595\n",
      "ask_whatspossible \t\t 97.856927\n",
      "ask_whatspossible \t\t 96.570390\n",
      "ask_whatspossible \t\t 50.981718\n",
      "ask_whatspossible \t\t 78.823417\n",
      "ask_whatspossible \t\t 93.071729\n",
      "ask_whatspossible \t\t 78.823417\n",
      "ask_whatspossible \t\t 96.898371\n",
      "ask_whatspossible \t\t 71.894991\n",
      "how_is_it_going_to_help \t\t 59.428954\n",
      "ask_whatspossible \t\t 92.400223\n",
      "ask_whatismyname \t\t 60.954928\n",
      "ask_whatspossible \t\t 40.484932\n",
      "ask_whatspossible \t\t 99.560404\n",
      "ask_whatspossible \t\t 97.523820\n",
      "ask_whatspossible \t\t 95.303953\n",
      "ask_whatspossible \t\t 58.033568\n",
      "ask_whatspossible \t\t 95.248687\n",
      "ask_whatspossible \t\t 88.096899\n",
      "ask_whatspossible \t\t 67.902011\n",
      "ask_whatspossible \t\t 88.262564\n",
      "ask_whatspossible \t\t 93.071729\n",
      "ask_whatspossible \t\t 96.818662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ask_whatspossible \t\t 51.948088\n",
      "ask_whatspossible \t\t 93.149644\n",
      "ask_whatspossible \t\t 97.856927\n",
      "ask_whatspossible \t\t 87.531888\n",
      "ask_whatspossible \t\t 99.319863\n",
      "ask_whatspossible \t\t 95.303953\n",
      "ask_whatspossible \t\t 41.920212\n",
      "ask_whatspossible \t\t 79.824364\n",
      "ask_whatspossible \t\t 62.214470\n",
      "ask_whatspossible \t\t 35.493448\n",
      "ask_whatspossible \t\t 90.652430\n",
      "ask_whatspossible \t\t 93.071729\n",
      "ask_whatspossible \t\t 83.118248\n",
      "ask_whatspossible \t\t 96.994567\n",
      "ask_whatspossible \t\t 76.644528\n",
      "ask_whoisit \t\t 97.959661\n",
      "ask_whoisit \t\t 81.791180\n",
      "ask_whoisit \t\t 88.945353\n",
      "ask_whoisit \t\t 97.970688\n",
      "ask_whoisit \t\t 97.891515\n",
      "ask_whoisit \t\t 98.337460\n",
      "ask_whoisit \t\t 98.142642\n",
      "ask_whoisit \t\t 94.715768\n",
      "ask_whoisit \t\t 97.959661\n",
      "ask_whoisit \t\t 97.959661\n",
      "ask_whatismyname \t\t 90.880507\n",
      "ask_whoisit \t\t 71.639812\n",
      "ask_whoisit \t\t 78.008282\n",
      "ask_whoisit \t\t 97.339851\n",
      "ask_whoisit \t\t 88.945353\n",
      "ask_whoisit \t\t 88.945353\n",
      "ask_whoisit \t\t 96.728712\n",
      "ask_whatismyname \t\t 90.880507\n",
      "ask_whoisit \t\t 97.959661\n",
      "ask_whoisit \t\t 97.816318\n",
      "ask_whatismyname \t\t 57.576543\n",
      "ask_whatismyname \t\t 65.353030\n",
      "ask_whatismyname \t\t 53.030032\n",
      "ask_whoisit \t\t 91.217369\n",
      "ask_whoisit \t\t 91.217369\n",
      "ask_whoisit \t\t 96.482909\n",
      "ask_whoisit \t\t 97.063422\n",
      "dialogue_tools \t\t 96.873784\n",
      "dialogue_tools \t\t 87.858981\n",
      "dialogue_tools \t\t 82.702047\n",
      "dialogue_tools \t\t 84.031260\n",
      "explain_custom_bot \t\t 46.236849\n",
      "explain_custom_bot \t\t 88.354915\n",
      "explain_custom_bot \t\t 86.072177\n",
      "explain_deep_learning \t\t 79.549509\n",
      "explain_deep_learning \t\t 79.549509\n",
      "explain_deep_learning \t\t 63.208419\n",
      "explain_deep_learning \t\t 79.803169\n",
      "explain_deep_learning \t\t 81.294775\n",
      "explain_dialogue_machines \t\t 65.156299\n",
      "explain_dialogue_machines \t\t 38.713616\n",
      "explain_dialogue_machines \t\t 75.036591\n",
      "explain_dialogue_machines \t\t 63.948667\n",
      "explain_dialogue_machines \t\t 64.502001\n",
      "explain_entities \t\t 84.959400\n",
      "explain_entities \t\t 82.085013\n",
      "explain_entities \t\t 82.085013\n",
      "explain_entities \t\t 87.349331\n",
      "explain_entities \t\t 85.569638\n",
      "explain_feedback_bot \t\t 65.383810\n",
      "explain_feedback_bot \t\t 74.694377\n",
      "explain_feedback_bot \t\t 60.648304\n",
      "explain_feedback_bot \t\t 64.860940\n",
      "explain_feedback_bot \t\t 64.997935\n",
      "explain_generative_chatbot \t\t 89.030194\n",
      "explain_generative_chatbot \t\t 87.870854\n",
      "explain_generative_chatbot \t\t 67.947376\n",
      "explain_generative_chatbot \t\t 78.937525\n",
      "explain_generative_chatbot \t\t 80.004883\n",
      "explain_intents \t\t 46.799067\n",
      "explain_intents \t\t 51.948339\n",
      "explain_intents \t\t 51.948339\n",
      "explain_intents \t\t 66.236788\n",
      "explain_intents \t\t 62.443089\n",
      "explain_keras \t\t 53.524405\n",
      "explain_keras \t\t 53.524405\n",
      "explain_keras \t\t 20.429428\n",
      "explain_keras \t\t 80.564326\n",
      "explain_keras \t\t 51.158178\n",
      "explain_keras \t\t 50.283879\n",
      "explain_lead_bot \t\t 57.092541\n",
      "explain_lead_bot \t\t 67.671603\n",
      "explain_lead_bot \t\t 76.168478\n",
      "explain_lead_bot \t\t 65.472949\n",
      "explain_lead_bot \t\t 64.855254\n",
      "explain_nlp \t\t 57.634521\n",
      "explain_nlp \t\t 57.634521\n",
      "explain_nlp \t\t 70.153838\n",
      "explain_nlp \t\t 66.954660\n",
      "explain_nlp \t\t 83.833677\n",
      "explain_nlp \t\t 67.121422\n",
      "explain_nlp \t\t 83.901513\n",
      "explain_keras \t\t 20.429428\n",
      "explain_keras \t\t 20.429428\n",
      "explain_nltk \t\t 43.288049\n",
      "explain_nltk \t\t 29.625300\n",
      "explain_nltk \t\t 34.140795\n",
      "explain_spacy \t\t 34.939030\n",
      "explain_spacy \t\t 34.939030\n",
      "explain_nlu \t\t 80.276471\n",
      "explain_nlu \t\t 39.367634\n",
      "explain_nlu \t\t 75.925410\n",
      "explain_nlu \t\t 44.406858\n",
      "explain_nlu \t\t 78.018028\n",
      "explain_retention_bot \t\t 80.596656\n",
      "explain_retention_bot \t\t 71.284962\n",
      "explain_retention_bot \t\t 59.307200\n",
      "explain_retention_bot \t\t 77.708316\n",
      "explain_retention_bot \t\t 76.552778\n",
      "explain_retrieval_chatbot \t\t 84.039700\n",
      "explain_retrieval_chatbot \t\t 74.878490\n",
      "explain_retrieval_chatbot \t\t 74.282914\n",
      "explain_retrieval_chatbot \t\t 73.753542\n",
      "explain_retrieval_chatbot \t\t 76.225382\n",
      "explain_spacy \t\t 53.119779\n",
      "explain_spacy \t\t 53.119779\n",
      "explain_spacy \t\t 61.975861\n",
      "explain_spacy \t\t 56.217831\n",
      "explain_spacy \t\t 54.312545\n",
      "explain_support_bot \t\t 72.211367\n",
      "explain_support_bot \t\t 40.104529\n",
      "explain_support_bot \t\t 67.525935\n",
      "explain_support_bot \t\t 77.005172\n",
      "explain_support_bot \t\t 74.838907\n",
      "explain_keras \t\t 20.429428\n",
      "explain_keras \t\t 20.429428\n",
      "explain_tensorflow \t\t 33.484116\n",
      "explain_nltk \t\t 29.625300\n",
      "explain_nltk \t\t 34.140795\n",
      "explain_turing_test \t\t 91.289741\n",
      "explain_turing_test \t\t 82.488436\n",
      "explain_turing_test \t\t 85.333550\n",
      "explain_turing_test \t\t 81.856185\n",
      "explain_turing_test \t\t 83.836925\n",
      "get_started \t\t 82.989919\n",
      "get_started \t\t 98.684001\n",
      "get_started \t\t 91.281247\n",
      "get_started \t\t 88.182378\n",
      "get_started \t\t 92.903429\n",
      "get_started \t\t 97.224694\n",
      "get_started \t\t 99.303883\n",
      "get_started \t\t 99.148327\n",
      "get_started \t\t 80.155724\n",
      "get_started \t\t 97.099584\n",
      "get_started \t\t 99.658376\n",
      "get_started \t\t 99.665678\n",
      "get_started \t\t 90.783250\n",
      "get_started \t\t 96.621364\n",
      "get_started \t\t 99.207473\n",
      "get_started \t\t 97.972715\n",
      "get_started \t\t 91.107595\n",
      "get_started \t\t 99.208307\n",
      "get_started \t\t 99.807459\n",
      "get_started \t\t 95.090371\n",
      "get_started \t\t 72.644216\n",
      "get_started \t\t 70.538408\n",
      "get_started \t\t 99.421775\n",
      "get_started \t\t 97.251600\n",
      "get_started \t\t 99.497885\n",
      "get_started \t\t 76.918757\n",
      "get_started \t\t 30.303848\n",
      "how_is_it_going_to_help \t\t 59.428954\n",
      "how_is_it_going_to_help \t\t 79.163325\n",
      "how_is_it_going_to_help \t\t 75.191873\n",
      "how_is_it_going_to_help \t\t 79.163325\n",
      "how_is_it_going_to_help \t\t 89.118534\n",
      "ask_whatspossible \t\t 45.456648\n",
      "how_is_it_going_to_help \t\t 89.118534\n",
      "subscribe_newsletter \t\t 98.821497\n",
      "subscribe_newsletter \t\t 99.639022\n",
      "subscribe_newsletter \t\t 99.620169\n",
      "subscribe_newsletter \t\t 98.678315\n",
      "subscribe_newsletter \t\t 99.711597\n",
      "subscribe_newsletter \t\t 98.734236\n",
      "subscribe_newsletter \t\t 99.226159\n",
      "subscribe_newsletter \t\t 99.167258\n",
      "subscribe_newsletter \t\t 99.873036\n",
      "subscribe_newsletter \t\t 99.877506\n",
      "subscribe_newsletter \t\t 99.875915\n",
      "subscribe_newsletter \t\t 99.792081\n",
      "subscribe_newsletter \t\t 99.823964\n",
      "subscribe_newsletter \t\t 99.374843\n",
      "subscribe_newsletter \t\t 99.696320\n",
      "subscribe_newsletter \t\t 99.394786\n",
      "subscribe_newsletter \t\t 99.225861\n",
      "subscribe_newsletter \t\t 99.620169\n",
      "subscribe_newsletter \t\t 93.424475\n",
      "subscribe_newsletter \t\t 97.534615\n",
      "subscribe_newsletter \t\t 97.534615\n",
      "subscribe_newsletter \t\t 99.374843\n",
      "subscribe_newsletter \t\t 95.972735\n",
      "subscribe_newsletter \t\t 93.774992\n",
      "subscribe_newsletter \t\t 99.711549\n",
      "subscribe_newsletter \t\t 97.817492\n",
      "subscribe_newsletter \t\t 99.626631\n",
      "subscribe_newsletter \t\t 99.374843\n",
      "subscribe_newsletter \t\t 99.455893\n",
      "subscribe_newsletter \t\t 99.890924\n",
      "subscribe_newsletter \t\t 99.746549\n",
      "subscribe_newsletter \t\t 96.354860\n",
      "subscribe_newsletter \t\t 99.811542\n",
      "subscribe_newsletter \t\t 99.778146\n",
      "subscribe_newsletter \t\t 99.636203\n",
      "subscribe_newsletter \t\t 99.897313\n",
      "subscribe_newsletter \t\t 99.671292\n",
      "subscribe_newsletter \t\t 99.814558\n",
      "subscribe_newsletter \t\t 99.776554\n",
      "subscribe_newsletter \t\t 99.225861\n",
      "subscribe_newsletter \t\t 99.726915\n",
      "subscribe_newsletter \t\t 99.866760\n",
      "subscribe_newsletter \t\t 99.387771\n",
      "subscribe_newsletter \t\t 95.947671\n",
      "subscribe_newsletter \t\t 96.679896\n",
      "subscribe_newsletter \t\t 99.875915\n",
      "subscribe_newsletter \t\t 98.073399\n",
      "subscribe_newsletter \t\t 98.942417\n",
      "subscribe_newsletter \t\t 89.939660\n",
      "subscribe_newsletter \t\t 93.048912\n",
      "subscribe_newsletter \t\t 99.614763\n",
      "subscribe_newsletter \t\t 99.675673\n",
      "subscribe_newsletter \t\t 97.871631\n",
      "subscribe_newsletter \t\t 99.805892\n",
      "subscribe_newsletter \t\t 99.806589\n",
      "subscribe_newsletter \t\t 98.359746\n",
      "subscribe_newsletter \t\t 97.546434\n",
      "subscribe_newsletter \t\t 98.764312\n",
      "subscribe_newsletter \t\t 99.873036\n",
      "subscribe_newsletter \t\t 99.778348\n",
      "subscribe_newsletter \t\t 99.888498\n",
      "subscribe_newsletter \t\t 99.744356\n",
      "subscribe_newsletter \t\t 99.880290\n",
      "subscribe_newsletter \t\t 99.799538\n",
      "subscribe_newsletter \t\t 99.806052\n",
      "subscribe_newsletter \t\t 99.833935\n",
      "subscribe_newsletter \t\t 99.581158\n",
      "subscribe_newsletter \t\t 99.453592\n",
      "subscribe_newsletter \t\t 99.824584\n",
      "subscribe_newsletter \t\t 98.502707\n",
      "subscribe_newsletter \t\t 99.706125\n",
      "subscribe_newsletter \t\t 99.866760\n",
      "subscribe_newsletter \t\t 99.457473\n",
      "subscribe_newsletter \t\t 99.836284\n",
      "subscribe_newsletter \t\t 99.861670\n",
      "subscribe_newsletter \t\t 86.581635\n",
      "subscribe_newsletter \t\t 99.793756\n",
      "subscribe_newsletter \t\t 99.528521\n",
      "subscribe_newsletter \t\t 99.846017\n",
      "subscribe_newsletter \t\t 99.726892\n",
      "subscribe_newsletter \t\t 99.739575\n",
      "subscribe_newsletter \t\t 99.888498\n",
      "subscribe_newsletter \t\t 99.394786\n",
      "subscribe_newsletter \t\t 99.756837\n",
      "subscribe_newsletter \t\t 99.744356\n",
      "subscribe_newsletter \t\t 99.689215\n",
      "subscribe_newsletter \t\t 99.805892\n",
      "subscribe_newsletter \t\t 99.402148\n",
      "subscribe_newsletter \t\t 99.865806\n",
      "subscribe_newsletter \t\t 99.811542\n",
      "subscribe_newsletter \t\t 99.726093\n",
      "subscribe_newsletter \t\t 99.225861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subscribe_newsletter \t\t 95.972735\n",
      "subscribe_newsletter \t\t 99.783641\n",
      "subscribe_newsletter \t\t 99.394786\n",
      "subscribe_newsletter \t\t 99.856073\n",
      "subscribe_newsletter \t\t 99.711549\n",
      "types_of_chatbots \t\t 79.755354\n",
      "types_of_chatbots \t\t 60.516077\n",
      "types_of_chatbots \t\t 93.475407\n",
      "types_of_chatbots \t\t 93.217587\n",
      "visit_website \t\t 64.864922\n",
      "visit_website \t\t 75.773400\n",
      "visit_website \t\t 72.028339\n",
      "visit_website \t\t 59.651631\n",
      "ask_whatismyname \t\t 22.851534\n",
      "when_will_you_beat_turing_test \t\t 89.668566\n",
      "when_will_you_beat_turing_test \t\t 95.130062\n",
      "why_did_you_make_a_bot \t\t 49.606308\n",
      "why_did_you_make_a_bot \t\t 74.618018\n",
      "why_machine_learning \t\t 95.888644\n",
      "why_machine_learning \t\t 96.803355\n",
      "why_machine_learning \t\t 89.185959\n",
      "why_machine_learning \t\t 96.881753\n",
      "why_make_a_bot \t\t 55.054760\n",
      "why_make_a_bot \t\t 69.042778\n",
      "\n",
      " 1433 \n",
      "\n",
      "CPU times: user 3min 12s, sys: 2min 21s, total: 5min 34s\n",
      "Wall time: 2min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# max_index = int(len(x_train_final_input)/10 + 1)\n",
    "max_index = len(x_train_processed)\n",
    "\n",
    "for i in range(max_index):\n",
    "    predict_intent(x_train_processed[i])\n",
    "\n",
    "print(\"\\n\", max_index, \"\\n\")\n",
    "# TODO: CREATE A CONFUSION MATRIX TO QUICKLY CHECK THE FALSE POSITIVES AND FALSE POSITIVES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_intents(texts):\n",
    "    final_texts = []\n",
    "    for i in range(len(texts)):\n",
    "        text = preprocess_text(texts[i])\n",
    "        tokens = prepare_text_for_prediction(text)\n",
    "        final_texts.append(tokens)\n",
    "\n",
    "    pred_intents = []\n",
    "    for i in range(len(final_texts)):\n",
    "        predict_class = model.predict(final_texts[i])\n",
    "        \n",
    "        intent = encoder.inverse_transform(predict_class)\n",
    "        index_number = np.argmax(predict_class)\n",
    "        intent_value = predict_class[0, index_number]\n",
    "        pred_intents.append((intent[0], intent_value))\n",
    "        \n",
    "    return pred_intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('greet', 0.9800851), ('greet', 0.9803407), ('greet', 0.97517437), ('greet', 0.9803407), ('greet', 0.7798564), ('greet', 0.6971046), ('greet', 0.9579779), ('greet', 0.9800851), ('bye', 0.3108096), ('greet', 0.9888271), ('inform', 0.8190385), ('inform', 0.8190385), ('greet', 0.7787066), ('inform', 0.8190385), ('greet', 0.97744775), ('greet', 0.98850226), ('greet', 0.9370669), ('greet', 0.9723619), ('greet', 0.9803407), ('inform', 0.8190385)]\n",
      "\n",
      " 1433 \n",
      "\n",
      "CPU times: user 2.59 s, sys: 1.95 s, total: 4.54 s\n",
      "Wall time: 2.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# max_index = int(len(x_train_final_input)/10 + 1)\n",
    "max_index = len(x_train_processed)\n",
    "\n",
    "pred = predict_intents(x_train_processed[:20])\n",
    "\n",
    "print(pred)\n",
    "print(\"\\n\", max_index, \"\\n\")\n",
    "# TODO: CREATE A CONFUSION MATRIX TO QUICKLY CHECK THE FALSE POSITIVES AND FALSE POSITIVES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_pred = []\n",
    "for i in range(len(X_test)):\n",
    "    pred_intent, pred_intent_value = predict_intent(X_test[i])\n",
    "    label_tokens = np.array([y_test[i]])\n",
    "    true_intent = encoder.inverse_transform(label_tokens)\n",
    "    if pred_intent != true_intent:\n",
    "        incorrect_pred.append([tokens_to_string(X_test[i]), pred_intent[0], true_intent[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence                                          || Prediction      || True Intent\n"
     ]
    }
   ],
   "source": [
    "print(\"{:50}|| {:15} || {}\".format(\"Sentence\", \"Prediction\", \"True Intent\"))\n",
    "for i in range(len(incorrect_pred)):\n",
    "    print(\"{:50}: {:15}\\t{}\".format(incorrect_pred[i][0], incorrect_pred[i][1], incorrect_pred[i][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((len(X_test) - len(incorrect_pred)) / len(X_test) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_embedding = model.get_layer('embedding_layer')\n",
    "weights_embedding = layer_embedding.get_weights()[0]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "inverse_map = dict(zip(word_index.values(), word_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def print_sorted_words(word, metric='cosine'):\n",
    "    \"\"\"\n",
    "    Print the words in the vocabulary sorted according to their\n",
    "    embedding-distance to the given word.\n",
    "    Different metrics can be used, e.g. 'cosine' or 'euclidean'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the token (i.e. integer ID) for the given word.\n",
    "    token = tokenizer.word_index[word]\n",
    "\n",
    "    # Get the embedding for the given word. Note that the\n",
    "    # embedding-weight-matrix is indexed by the word-tokens\n",
    "    # which are integer IDs.\n",
    "    embedding = weights_embedding[token]\n",
    "\n",
    "    # Calculate the distance between the embeddings for\n",
    "    # this word and all other words in the vocabulary.\n",
    "    distances = cdist(weights_embedding, [embedding], metric=metric).T[0]\n",
    "\n",
    "    # Get an index sorted according to the embedding-distances.\n",
    "    # These are the tokens (integer IDs) for words in the vocabulary.\n",
    "    sorted_index = np.argsort(distances)\n",
    "\n",
    "    # Sort the embedding-distances.\n",
    "    sorted_distances = distances[sorted_index]\n",
    "\n",
    "    # Sort all the words in the vocabulary according to their\n",
    "    # embedding-distance. This is a bit excessive because we\n",
    "    # will only print the top and bottom words.\n",
    "    sorted_words = [inverse_map[token] for token in sorted_index\n",
    "                    if token != 0 and token in inverse_map]\n",
    "\n",
    "    # Helper-function for printing words and embedding-distances.\n",
    "    def _print_words(words, distances):\n",
    "        for word, distance in zip(words, distances):\n",
    "            print(\"{0:.3f} - {1}\".format(distance, word))\n",
    "\n",
    "    # Number of words to print from the top and bottom of the list.\n",
    "    k = 10\n",
    "\n",
    "    print(\"Distance from '{0}':\".format(word))\n",
    "\n",
    "    # Print the words with smallest embedding-distance.\n",
    "    _print_words(sorted_words[0:k], sorted_distances[0:k])\n",
    "\n",
    "#     print(\"...\")\n",
    "\n",
    "    # Print the words with highest embedding-distance. \n",
    "#     _print_words(sorted_words[-k:], sorted_distances[-k:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance from 'machine':\n",
      "0.000 - machine\n",
      "0.387 - tool\n",
      "0.428 - system\n",
      "0.439 - type\n",
      "0.457 - use\n",
      "0.471 - factory\n",
      "0.481 - model\n",
      "0.490 - design\n",
      "0.492 - software\n",
      "0.496 - robot\n"
     ]
    }
   ],
   "source": [
    "print_sorted_words('machine', metric='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_FILE = \"glove.6B.100d.txt\"\n",
    "GLOVE_VEC_SIZE = 100\n",
    "VOCAB_SIZE = 20000\n",
    "EMBEDDING_DIM = GLOVE_VEC_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_words = {}\n",
    "\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_words[word] = embedding_vector\n",
    "    if index > VOCAB_SIZE - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_words = [\n",
    "    \"knowledge\", \"information\" \"organisation\", \"chatbot\", \"bot\", \"signup, newsletter\", \"subscriber\", \n",
    "    \"nlu\", \"nlp\", \"machine\", \"learning\", \"deep\", \"logic\", \"subscription\", \"development\", \"built\",\n",
    "    \"understanding\", \"nltk\", \"spacy\", \"natural\", \"tensorflow\", \"keras\", \"turing\", \"computer\",\n",
    "    \"dialogue\", \"show\", \"possible\", \"explain\", \"intent\", \"entity\", \"entities\", \"recognize\", \"look\",\n",
    "    \"weather\", \"sky\", \"sunny\", \"programmer\", \"classification\", \"listen\", \"artificial\", \"intelligence\",\n",
    "    \"ai\", \"ml\", \"test\", \"website\", \"setup\", \"start\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_and_labels_emb(embedding_words, word_list):\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    for w, t in embedding_words.items():\n",
    "        if w in word_list:\n",
    "            tokens.append(t)\n",
    "            labels.append(w)\n",
    "    return labels, tokens\n",
    "\n",
    "def tsne_fit_and_plot(tokens, labels):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=42)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anurags/miniforge3/envs/tensorflow_exp/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/anurags/miniforge3/envs/tensorflow_exp/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:982: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x1152 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels, tokens = get_tokens_and_labels_emb(embedding_words, popular_words)\n",
    "tsne_fit_and_plot(tokens=tokens, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE Dimensionality reduction\n",
    "As we can see, reducing a 100 dimension vector so much doesn't seem to make much sense, and thus it's better to look at the words closest to target word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance from 'weather':\n",
      "0.000 - weather\n",
      "0.292 - rain\n",
      "0.363 - cold\n",
      "0.367 - sunny\n",
      "0.394 - humid\n",
      "0.402 - forecast\n",
      "0.406 - temperature\n",
      "0.410 - meteorological\n",
      "0.423 - cool\n",
      "0.436 - hot\n"
     ]
    }
   ],
   "source": [
    "print_sorted_words('weather', metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance from 'classification':\n",
      "0.000 - classification\n",
      "0.497 - type\n",
      "0.525 - system\n",
      "0.530 - configuration\n",
      "0.556 - proper\n",
      "0.557 - course\n",
      "0.559 - humid\n",
      "0.572 - race\n",
      "0.576 - list\n",
      "0.578 - different\n"
     ]
    }
   ],
   "source": [
    "print_sorted_words('classification', metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance from 'programmer':\n",
      "0.000 - programmer\n",
      "0.383 - software\n",
      "0.430 - engineer\n",
      "0.475 - developer\n",
      "0.499 - creator\n",
      "0.505 - scientist\n",
      "0.518 - setup\n",
      "0.519 - designer\n",
      "0.522 - lets\n",
      "0.532 - master\n"
     ]
    }
   ],
   "source": [
    "print_sorted_words('programmer', metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

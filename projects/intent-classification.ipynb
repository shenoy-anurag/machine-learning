{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.8.0, Keras version: 2.7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "print(\"Tensorflow version: {}, Keras version: {}\".format(tf.__version__, keras.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import pickle\n",
    "import string\n",
    "import requests\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.layers import Input, Dense, GRU, LSTM, Embedding\n",
    "from tensorflow.python.keras.optimizers import adam_v2\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from mosestokenizer import MosesDetokenizer\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/anurags/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /Users/anurags/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "detokenizer = MosesDetokenizer('en')\n",
    "translator = str.maketrans('', '', string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"wouldn't\", 'against', 'there', 'because', 'during', \"they'd\", 'didn', 'hadn', \"needn't\", \"isn't\", \"i'd\", 'after', 'as', 'few', 'ain', 'weren', \"mustn't\", \"shouldn't\", 'their', 'very', 'these', 'while', 'has', 'through', 'mightn', 'doesn', 'an', \"you'd\", 'don', 'out', \"we'd\", 'now', 'from', 'such', 'themselves', \"won't\", \"weren't\", 'over', 'ours', \"she'd\", \"he's\", 'or', 'same', 'won', 't', 'my', 'any', 'and', 'further', 'than', 'then', 'below', 'haven', 'll', \"they're\", 's', \"wasn't\", 'itself', 'all', 'your', 'nor', 'wouldn', \"they'll\", \"you've\", 'if', 'so', 'shan', \"it'd\", \"i'll\", \"doesn't\", \"he'd\", 'into', \"couldn't\", 'theirs', 'had', 'is', 'under', 'each', 'her', 'yours', \"hasn't\", \"i'm\", 'hasn', 'hers', 'been', 'we', 'me', 'his', 'd', 'but', 'does', \"she'll\", 'was', \"should've\", 'myself', 'more', 'couldn', 're', 'were', 'those', \"he'll\", 'whom', 'both', 'a', 'this', 'having', 'only', 'being', 'ourselves', 'some', 'above', 'wasn', 'when', 'he', 'down', 'once', \"i've\", 'yourself', 'yourselves', \"you're\", 'o', \"don't\", 'him', \"they've\", 'until', 'aren', 'them', \"she's\", \"we're\", 've', 'other', 'shouldn', 'that', \"didn't\", \"haven't\", \"it's\", 'its', \"we'll\", 'they', 'ma', \"aren't\", 'by', \"we've\", \"shan't\", \"it'll\", 'again', \"mightn't\", \"you'll\", 'it', 'mustn', 'doing', 'm', \"that'll\", 'herself', 'should', \"hadn't\", 'between', 'most', 'am', 'isn', 'did', 'just', 'she', 'himself', 'own', 'needn', 'our'}\n"
     ]
    }
   ],
   "source": [
    "# Reduce stopwords list by excluding these words from the list.\n",
    "not_stopwords = [\n",
    "    'no', 'not', 'to', 'i', 'be', 'how', 'are', 'r', 'y', 'you', 'the', 'do', 'what', 'up', 'can', 'for',\n",
    "    'of', 'where', 'have', 'who', 'on', 'with', 'which', 'in', 'about', 'at', 'will', 'here', 'too', 'off',\n",
    "    'before', 'why'\n",
    "]\n",
    "\n",
    "NLTK_STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def get_stopwords(stopwords, not_stopwords):\n",
    "    for word in not_stopwords:\n",
    "        stopwords.discard(word)\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "custom_stopwords = get_stopwords(NLTK_STOPWORDS, not_stopwords=not_stopwords)\n",
    "\n",
    "print(custom_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/Intent/intents.yml\n"
     ]
    }
   ],
   "source": [
    "DATA_FOLDER = \"../data\"\n",
    "\n",
    "dataset_url = \"https://www.kaggle.com/elvinagammed/chatbots-intent-recognition-dataset/\"\n",
    "\n",
    "# INTENT_FILE_NAME = \"Intent.json\"\n",
    "INTENT_FILE_NAME = \"intents.yml\"\n",
    "\n",
    "INTENT_DATA_FOLDER = os.path.join(os.path.join(DATA_FOLDER, \"Intent\"))\n",
    "INTENT_FILE_PATH = os.path.join(INTENT_DATA_FOLDER, INTENT_FILE_NAME)\n",
    "print(INTENT_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'intent': 'greet',\n",
       "  'examples': \"- hey\\n- hello\\n- hi\\n- hello there\\n- good morning\\n- good evening\\n- moin\\n- hey there\\n- let's go\\n- hey dude\\n- goodmorning\\n- goodevening\\n- good afternoon\\n- Hellllooooooo\\n- howdy\\n- hey bot\\n- heya\\n- Hallo\\n- hello?\\n- ayyyy whaddup\\n- heyo\\n- helloooo\\n- hellooo\\n- hello sweatheart\\n- hiihihi\\n- hey there\\n- yoo\\n- hello sweet boy\\n- whats up\\n- Hei\\n- hello\\n- I said, helllllloooooO!!!!\\n- good evening\\n- hi there it's me\\n- hi friend\\n- jop\\n- hi\\n- hi there\\n- what up\\n- hii\\n- hello it is me again\\n- jojojo\\n- hey let's talk\\n- hey, let's talk\\n- heyho\\n- hiii\\n- Whats up my bot\\n- Heya\\n- hey dude\\n- Well hello there\\n- hello friend\\n- Hey\\n- greetings\\n- hello everybody\\n- hello is anybody there\\n- good afternoon\\n- hello robot\\n- hallo\\n- hi?\\n- hola\\n- yo\\n- heeey\\n- hi hi\\n- hey\\n- hey hey\\n- hello there\\n- hi\\n- hi there\\n- hey bot!\\n- hi pal!\\n- hi folks\\n- Hey man\\n- Hi\\n- Yo!\\n- Howdy\\n- Yo!\\n- Howdy\\n- Hiya\\n- Hello!\\n- Hola!\\n- Hi\\n- Hey\\n- Hi bot\\n- Hey bot\\n- Hello\\n- hi again\\n- hi Mister\\n- good morning\\n\"},\n",
       " {'intent': 'goodbye',\n",
       "  'examples': '- cu\\n- good by\\n- cee you later\\n- good night\\n- bye\\n- goodbye\\n- have a nice day\\n- see you around\\n- bye bye\\n- see you later\\n'},\n",
       " {'intent': 'good_day',\n",
       "  'examples': \"- have a nice day\\n- good day to you\\n- have a good one!\\n- G'day mate!\\n- it is a good evening\\n- it is a good day today\\n\"},\n",
       " {'intent': 'mood_great',\n",
       "  'examples': '- perfect\\n- great\\n- amazing\\n- feeling like a king\\n- wonderful\\n- I am feeling very good\\n- I am great\\n- I am amazing\\n- I am going to save the world\\n- super stoked\\n- extremely good\\n- so so perfect\\n- so good\\n- so perfect\\n'},\n",
       " {'intent': 'mood_unhappy',\n",
       "  'examples': \"- my day was horrible\\n- I am sad\\n- I don't feel very well\\n- I am disappointed\\n- super sad\\n- I'm so sad\\n- sad\\n- very sad\\n- unhappy\\n- not good\\n- not very good\\n- extremly sad\\n- so saad\\n- so sad\\n\"}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "intent_data = None\n",
    "with open(INTENT_FILE_PATH, 'r') as f:\n",
    "    # The FullLoader parameter handles the conversion from YAML\n",
    "    # scalar values to Python the dictionary format\n",
    "    intent_data = yaml.full_load(f)\n",
    "    intent_data = intent_data[\"intents\"]\n",
    "\n",
    "display(intent_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1433\n",
      "           text intent\n",
      "0           hey  greet\n",
      "1         hello  greet\n",
      "2            hi  greet\n",
      "3   hello there  greet\n",
      "4  good morning  greet\n"
     ]
    }
   ],
   "source": [
    "train_data = {'text': [], 'intent': []}\n",
    "\n",
    "for i in range(len(intent_data)):\n",
    "    lines = intent_data[i][\"examples\"]\n",
    "    lines = lines.split(\"\\n\")\n",
    "    lines = [l.strip(\"- \").strip(\"''\") for l in lines if l.strip()]\n",
    "    intent_lines = [intent_data[i][\"intent\"] for _ in range(len(lines))]\n",
    "    train_data[\"text\"].extend(lines)\n",
    "    train_data[\"intent\"].extend(intent_lines)\n",
    "\n",
    "train_data = pd.DataFrame(train_data)\n",
    "print(len(train_data))\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_data['text'].tolist()\n",
    "y_train = train_data['intent'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    return text.translate(translator)\n",
    "\n",
    "\n",
    "def tokenize_sent(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "\n",
    "def detokenize_sent(tokenized_text):\n",
    "    return detokenizer(tokenized_text)\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = text.lower()\n",
    "    tokenized_text = tokenize_sent(text)\n",
    "    cleaned_text = [word for word in tokenized_text if not word in NLTK_STOPWORDS]\n",
    "    return ' '.join(cleaned_text)\n",
    "\n",
    "\n",
    "def remove_custom_stopwords(text, stopwords):\n",
    "    text = text.lower()\n",
    "    tokenized_text = tokenize_sent(text)\n",
    "    cleaned_text = [word for word in tokenized_text if not word in stopwords]\n",
    "    return ' '.join(cleaned_text)\n",
    "\n",
    "\n",
    "def lower_case_text(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING = 'pre'\n",
    "TEXT_LEN_LIMIT = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey', 'hello', 'hi', 'hello', 'good morning']\n"
     ]
    }
   ],
   "source": [
    "x_train_processed = []\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "    # lower case the text\n",
    "    text = lower_case_text(x_train[i])\n",
    "    # remove punctuation\n",
    "    text = text.translate(translator)\n",
    "    # remove stopwords\n",
    "    text = remove_custom_stopwords(text, custom_stopwords)\n",
    "\n",
    "    if not text:\n",
    "        text = x_train[i]\n",
    "    # process the text using Spacy's language model \"en_core_web_sm\".\n",
    "    doc = nlp(text)\n",
    "    # lemmatization\n",
    "    text = [token.lemma_ for token in doc]\n",
    "    # detokenize text\n",
    "    text = detokenize_sent(text)\n",
    "\n",
    "    x_train_processed.append(text)\n",
    "\n",
    "print(x_train_processed[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for i in range(len(x_train_processed)):\n",
    "    words = word_tokenize(x_train_processed[i])\n",
    "    all_words.extend(words)\n",
    "\n",
    "all_words = list(set(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_intents = len(set(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 1, 'to': 2, 'you': 3, 'the': 4, 'what': 5, 'want': 6, 'be': 7, 'bot': 8, 'how': 9, 'newsletter': 10, 'do': 11, 'about': 12, 's': 13, 'who': 14, 'm': 15, 'can': 16, 'for': 17, 'name': 18, 'time': 19, 'know': 20, 'chatbot': 21, 'not': 22, 'like': 23, 'good': 24, 'in': 25, 'of': 26, 'thank': 27, 'tell': 28, 'build': 29, 'up': 30, 'talk': 31, 'have': 32, 'no': 33, 'learn': 34, 'please': 35, 'give': 36, 'get': 37, 'company': 38, 'weather': 39, 'hi': 40, 'go': 41, 'help': 42, 'one': 43, 'sign': 44, 'subscribe': 45, 'bye': 46, 'day': 47, 'yes': 48, 'stop': 49, 'with': 50, 'hey': 51, 'would': 52, 'work': 53, 'feedback': 54, 'hello': 55, 'today': 56, 'human': 57, 'manager': 58, 'developer': 59, 'make': 60, 'let': 61, 'will': 62, 'email': 63, 'on': 64, 'why': 65, 'use': 66, 'cool': 67, 'speak': 68, 'ok': 69, 'nice': 70, 'sure': 71, 'd': 72, 'start': 73, 'great': 74, 'real': 75, 'else': 76, 'create': 77, 'call': 78, 'see': 79, 'accept': 80, 'machine': 81, 'which': 82, 'akela': 83, 'improve': 84, 'dialogue': 85, 'learning': 86, 'later': 87, 'sad': 88, 'support': 89, 'million': 90, 'engineer': 91, 'language': 92, 'management': 93, 'lead': 94, 'intent': 95, 'suggest': 96, 'robot': 97, 'course': 98, 'absolutely': 99, 'thing': 100, 'exit': 101, 'person': 102, 'wan': 103, 'na': 104, 'customer': 105, 'sale': 106, 'euro': 107, 'place': 108, 'boss': 109, 'project': 110, 'tool': 111, 'ture': 112, 'oov': 113, 'need': 114, 'deep': 115, 'natural': 116, 'test': 117, 'signup': 118, 'add': 119, 'evening': 120, 'friend': 121, 'perfect': 122, 'feel': 123, 'here': 124, 'oh': 125, 'ai': 126, 'think': 127, 'u': 128, 'someone': 129, 'everything': 130, '1': 131, 'ice': 132, 'dev': 133, 'at': 134, 'year': 135, 'idea': 136, 'right': 137, 'where': 138, 'anything': 139, 'entity': 140, 'generative': 141, 'kera': 142, 'nltk': 143, 'retention': 144, 'retrieval': 145, 'spacy': 146, 'tensorflow': 147, 'list': 148, 'morning': 149, 'goodbye': 150, 'mate': 151, 'amazing': 152, 'ask': 153, 'really': 154, 'alright': 155, 'yet': 156, 'never': 157, 'quit': 158, 'action': 159, 'touch': 160, 'service': 161, 'team': 162, 'send': 163, 'business': 164, '5': 165, 'none': 166, 'insurance': 167, 'health': 168, 'english': 169, 'people': 170, 'life': 171, 'inr': 172, 'product': 173, 'new': 174, 'suggestion': 175, 'invent': 176, 'could': 177, 'first': 178, 'nlp': 179, 'nlu': 180, 'receive': 181, 'nl': 182, 'howdy': 183, 'sweet': 184, 'well': 185, 'yo': 186, 'around': 187, 'super': 188, 'chat': 189, 'yep': 190, 'mind': 191, 'nah': 192, \"'day\": 193, 'idiot': 194, 'fuck': 195, 'also': 196, 'cute': 197, 'picture': 198, 'big': 199, 'researcher': 200, 'old': 201, '10': 202, 'head': 203, 'buck': 204, 'acme': 205, 'full': 206, 'stack': 207, 'money': 208, 'distance': 209, 'marketing': 210, 'mop': 211, 'base': 212, 'linda': 213, 'meier': 214, 'alex': 215, 'dollar': 216, 'ceo': 217, 'we': 218, 'mail': 219, 'york': 220, 'times': 221, 'philipp': 222, \"'go\": 223, 'generate': 224, 'creator': 225, 'current': 226, 'berlin': 227, 'option': 228, 'processing': 229, 'understanding': 230, 'configure': 231, 'subscription': 232, 'dope': 233, 'join': 234, 'registration': 235, 'type': 236, 'dude': 237, 'afternoon': 238, 'heya': 239, 'hallo': 240, 'boy': 241, 'say': 242, 'hola': 243, 'man': 244, 'night': 245, 'world': 246, 'simple': 247, 're': 248, 'artificial': 249, 'intelligence': 250, 'guess': 251, 'y': 252, 'fine': 253, 'change': 254, 'hm': 255, 'definitely': 256, 'agree': 257, 'affirmative': 258, 'udo': 259, 'k': 260, 'nein': 261, 'wrong': 262, 'decide': 263, 'decline': 264, 'mean': 265, 'either': 266, 'execution': 267, 'moron': 268, 'piece': 269, 'shit': 270, 'hate': 271, 'stupid': 272, 'anyone': 273, 'founder': 274, 'care': 275, 'check': 276, 'news': 277, 'much': 278, 'cheer': 279, 'lot': 280, 'advocate': 281, 'juste': 282, 'jimmy': 283, 'cream': 284, 'transformer': 285, '200': 286, 'quid': 287, 'sentient': 288, 'glibber': 289, 'glitter': 290, '60': 291, 'choose': 292, 'car': 293, 'actually': 294, 'brand': 295, '50': 296, 'data': 297, 'scientist': 298, 'own': 299, 'freelancer': 300, 'something': 301, 'extract': 302, 'amount': 303, 'long': 304, 'cube': 305, 'promote': 306, 'turtle': 307, 'function': 308, 'department': 309, 'animal': 310, 'conversational': 311, 'assistant': 312, 'python': 313, 'max': 314, 'tom': 315, 'miller': 316, '2': 317, 'jim': 318, 'different': 319, 'plan': 320, 'microsoft': 321, 'per': 322, 'abcd': 323, '100k': 324, '150000': 325, 'bcg': 326, 'maxmeierfirmade': 327, 'botfanbotscom': 328, 'elise': 329, 'biz': 330, 'co': 331, 'kg': 332, 'budget': 333, 'ali': 334, 'park': 335, '200k': 336, 'eisenkleber': 337, 'okay': 338, 'treat': 339, 'opinion': 340, 'model': 341, 'focus': 342, 'owner': 343, 'design': 344, 'may': 345, 'set': 346, 'india': 347, 'beautiful': 348, 'tomorrow': 349, 'quite': 350, \"'the\": 351, 'sun': 352, 'explain': 353, 'show': 354, 'kind': 355, 'custom': 356, 'conversation': 357, 'setup': 358, 'love': 359, 'subscribtion': 360, 'try': 361, 'website': 362, 'site': 363, 'moin': 364, 'goodmorning': 365, 'goodevene': 366, 'hellllooooooo': 367, 'ayyyy': 368, 'whaddup': 369, 'heyo': 370, 'helloooo': 371, 'hellooo': 372, 'sweatheart': 373, 'hiihihi': 374, 'yoo': 375, 'hei': 376, 'helllllloooooo': 377, 'jop': 378, 'hii': 379, 'jojojo': 380, 'heyho': 381, 'hiii': 382, 'greeting': 383, 'everybody': 384, 'anybody': 385, 'heeey': 386, 'pal': 387, 'folk': 388, 'hiya': 389, 'mister': 390, 'cu': 391, 'cee': 392, 'gday': 393, 'king': 394, 'wonderful': 395, 'save': 396, 'stoke': 397, 'extremely': 398, 'horrible': 399, 'disappoint': 400, 'unhappy': 401, 'extremly': 402, 'saad': 403, 'ar': 404, 'cuz': 405, 'question': 406, 'lol': 407, 'skynet': 408, 'indeed': 409, 'sound': 410, 'correct': 411, 'confirm': 412, 'amayze': 413, 'jo': 414, 'okey': 415, 'dokey': 416, 'behave': 417, 'ja': 418, 'yup': 419, 'yas': 420, 'without': 421, 'doubt': 422, 'coolio': 423, 'yay': 424, 'yop': 425, 'awesome': 426, 'yeah': 427, 'ttyl': 428, 'byebye': 429, 'goodnight': 430, 'ya': 431, 'toodleoo': 432, 'farewell': 433, 'ciao': 434, 'catch': 435, 'byyye': 436, 'slay': 437, 'tlak': 438, 'address': 439, 'nevermind': 440, 'deny': 441, 'neither': 442, 'afraid': 443, 'way': 444, 'sorry': 445, 'sir': 446, 'maam': 447, 'restart': 448, 'g': 449, 'neckbeard': 450, 'f': 451, 'dumbass': 452, 'smart': 453, 'put': 454, 'that': 455, 'annoy': 456, 'gim': 457, 'proper': 458, 'handoff': 459, 'agent': 460, 'forward': 461, 'forum': 462, 'link': 463, 'ill': 464, 'bro': 465, 'bunch': 466, 'herbert': 467, 'shiba': 468, 'tylerthematemanclubmatecom': 469, 'tyler': 470, 'construction': 471, 'worker': 472, 'club': 473, 'memecom': 474, 'herbertgmailcom': 475, 'shibashibacom': 476, 'chief': 477, 'lemonade': 478, 'officer': 479, '500': 480, 'half': 481, 'ol': 482, 'hielisede': 483, 'justejustecom': 484, 'ullegmxde': 485, 'csi': 486, 'ml': 487, 'saving': 488, 'account': 489, 'sam': 490, 'nothing': 491, 'tamedmousemicerevolutionfr': 492, 'volodimir': 493, 'voldemarich': 494, 'idk': 495, 'junkjunkcom': 496, 'jpcom': 497, 'alacmeorg': 498, 'al': 499, 'capone': 500, '£50k': 501, 'hr': 502, 'stuff': 503, 'nonenonecouk': 504, 'p': 505, 'my': 506, 'meyoude': 507, 'akelaistcoolschwabenländlede': 508, '120000': 509, 'spam': 510, 'pick': 511, 'nose': 512, 'busy': 513, \"'in\": 514, 'date': 515, 'message': 516, 'interested': 517, 'ordinal': 518, 'duration': 519, 'emailemailcom': 520, 'neutron': 521, 'industry': 522, '123akelacom': 523, 'akelaakelacom': 524, 'akelas': 525, 'billion': 526, 'bank': 527, 'alan': 528, 'impress': 529, 'badass': 530, 'tester': 531, 'promotion': 532, 'projject': 533, 'james': 534, 'validemailonede': 535, 'validoneemail': 536, 'unemploye': 537, 'spend': 538, '500000000': 539, 'wolf': 540, 'akilla': 541, 'sell': 542, 'salesman': 543, 'factory': 544, 'propella': 545, 'innvoation': 546, 'job': 547, 'responsible': 548, 'innovation': 549, 'frontend': 550, 'foobarcom': 551, '10000000': 552, 'mrmopmopsapp': 553, 'janitor': 554, 'lindalindalinda': 555, 'favorite': 556, 'color': 557, 'employee': 558, 'book': 559, 'meeting': 560, 'room': 561, 'ralph': 562, 'white': 563, 'ann': 564, 'snyder': 565, 'victoria': 566, 'mcmillan': 567, 'denise': 568, 'perry': 569, 'bob': 570, 'geldorf': 571, 'susan': 572, 'catterfeld': 573, 'taylor': 574, 'shwe': 575, 'meredith': 576, 'grey': 577, 'karev': 578, 'consult': 579, 'potential': 580, 'emayl': 581, 'yolo': 582, 'yolode': 583, '50k': 584, 'mopbot': 585, 'mr': 586, 'lorettastrawberryicecome': 587, 'strawberry': 588, 'philip': 589, 'bosch': 590, 'faq': 591, 'hellohellocom': 592, '10000k': 593, 'adelegmxcom': 594, 'marc': 595, 'trillion': 596, 'developerjobfunctionfull': 597, 'moabityogacom': 598, 'moabit': 599, 'yoga': 600, 'studio': 601, 'oli': 602, '100000k': 603, '10000': 604, '400': 605, 'trilion': 606, 'philippsuperphilippphilipp': 607, 'mln': 608, 'loretta': 609, 'mgmt': 610, 'schlabberjimmyglibberglitteredu': 611, '100': 612, '200000': 613, '50000': 614, 'thousand': 615, 'moment': 616, '75000150000': 617, 'estimation': 618, '10k': 619, 'bayer': 620, 'daimler': 621, 'uber': 622, 'small': 623, 'us': 624, 'tech': 625, 'apple': 626, 'clue': 627, 'sap': 628, '50000000': 629, 'ibm': 630, 'bout': 631, '4000000': 632, '500000': 633, '123': 634, '1234': 635, 'a2b3c4d5': 636, 'abcde': 637, 'efgh': 638, '45113': 639, '4612': 640, '43499': 641, '240kyear': 642, 'usd': 643, '250000': 644, 'tmobile': 645, 'vodafone': 646, 'accenture': 647, 'chengmingaliyuncom': 648, 'solomq122qqmailcom': 649, 'analyst': 650, 'datum': 651, 'growth': 652, 'cto': 653, 'sislawawaindiacom': 654, 'alexanderdenkertuberlinde': 655, 'nerdstanfordedu': 656, 'saswatkarharrediffmailcom': 657, 'mckinsey': 658, 'germany': 659, 'brazil': 660, 'digital': 661, 'venture': 662, 'stanford': 663, 'university': 664, 'research': 665, 'group': 666, 'centre': 667, 'ubc': 668, 'vancouver': 669, 'canada': 670, 'coo': 671, 'markjobsibmcom': 672, 'khardikkmosubolnetin': 673, 'mikemillersalesapplecom': 674, 'stephanywhitemicrosoftcom': 675, 'julianfrankhotmailcom': 676, 'santaklausgooglemailcom': 677, '4': 678, 'halpert': 679, 'udoai': 680, 'lindayoloyolode': 681, 'problem': 682, 'solve': 683, 'helvetia': 684, 'elisegmailcom': 685, 'system': 686, 'flatter': 687, 'every': 688, '90k': 689, 'donezo': 690, 'serve': 691, 'butter': 692, 'deve': 693, '300k': 694, 'number': 695, 'kevinyolooozde': 696, '200000000': 697, 'lindamailcom': 698, 'zendesk': 699, 'jacqueline': 700, '123gmailcom': 701, 'udoudoai': 702, 'heykldpeffesfokenoinwf': 703, 'master': 704, 'desaster': 705, 'wurst': 706, '0': 707, 'woman': 708, 'dispenser': 709, '500k': 710, 'develope': 711, 'generation': 712, '100000': 713, 'yespleaseyescom': 714, 'scalable': 715, 'propelladaskapitalde': 716, 'nyt': 717, 'foudner': 718, 'lindalindacom': 719, 'success': 720, 'lindas': 721, 'freya': 722, 'alinytimescom': 723, 'chocolate': 724, 'bbc': 725, 'helphelpcom': 726, 'akelaphilippcom': 727, 'testtestcom': 728, 'software': 729, 'john': 730, 'smith': 731, 'jenny': 732, 'douglas': 733, 'klaus': 734, 'klausson': 735, 'race': 736, 'driver': 737, 'bolschewistische': 738, 'kurkapelle': 739, 'schwarzrot': 740, 'ifuckrobots666applecom': 741, 'saswat': 742, 'self': 743, 'emplaye': 744, '2000k': 745, 'klausimausiapplecom': 746, 'ulrikovitcheisenklebereisenkleberlimitedcokgcom': 747, 'limit': 748, 'ulrikovitch': 749, 'designer': 750, 'pizza': 751, 'operation': 752, 'vladimir': 753, 'scrap': 754, '123skdvfvsdj': 755, 'reddit': 756, '20k': 757, 'philippthephilippcompanycom': 758, 'joeykoolmanconsultingcom': 759, 'load': 760, 'myemailgmailcom': 761, 've': 762, 'hang': 763, \"'up\": 764, \"'new\": 765, \"'everything\": 766, \"'life\": 767, 'whazzup': 768, 'sup': 769, 'ahoy': 770, 'matey': 771, 'chatting': 772, 'take': 773, 'shall': 774, 'listen': 775, 'classification': 776, 'ability': 777, 'chitchat': 778, 'improvement': 779, 'share': 780, 'produce': 781, 'fabricate': 782, 'behind': 783, 'legal': 784, 'programmer': 785, 'builder': 786, 'yoi': 787, 'trouble': 788, 'bring': 789, 'existence': 790, 'since': 791, 'age': 792, 'pardon': 793, 'might': 794, 'hour': 795, 'minute': 796, 'sydney': 797, 'usa': 798, 'excuse': 799, 'temperature': 800, 'expect': 801, 'thunderstorm': 802, 'sky': 803, 'clear': 804, 'meteorological': 805, 'aware': 806, 'look': 807, 'snowman': 808, 'humid': 809, 'scorcher': 810, 'require': 811, 'raincoat': 812, 'rain': 813, 'forecast': 814, 'hot': 815, 'cold': 816, 'breezy': 817, 'outside': 818, 'cloudy': 819, 'zou': 820, 'sunny': 821, 'recognize': 822, 'exactly': 823, 'tombstone': 824, 'colleague': 825, 'parent': 826, 'inform': 827, 'everyone': 828, 'um': 829, 'el': 830, 'next': 831, 'come': 832, 'back': 833, 'sentence': 834, 'possible': 835, 'menu': 836, 'heart': 837, 'r': 838, 'ur': 839, \"'custom\": 840, 'flow': 841, 'kerasio': 842, 'wat': 843, 'turing': 844, 'huh': 845, 'begin': 846, 'development': 847, 'guide': 848, 'knowledge': 849, 'building': 850, 'configuration': 851, 'information': 852, 'benefit': 853, 'organisation': 854, 'crave': 855, 'prefer': 856, 'subsribe': 857, 'subscriber': 858, 'lindayolode': 859, 'pelase': 860, 'register': 861, 'alexexamplecz': 862, 'alexmailmail': 863, 'newspaper': 864, 'too': 865, 'alanmailcom': 866, 'alexmailcom': 867, 'pls': 868, 'yeaaah': 869, 'visit': 870, \"'website\": 871, 'beat': 872, 'become': 873, 'enough': 874, 'fool': 875, 'predictdirect': 876, 'rather': 877, 'rule': 878, 'logic': 879}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize Text to assign each word a number\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "\n",
    "tokenizer.fit_on_texts(x_train_processed)\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tokens = tokenizer.texts_to_sequences(x_train_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "max_tokens = 0\n",
    "\n",
    "for i in range(len(x_train_tokens)):\n",
    "    if len(x_train_tokens[i]) > max_tokens:\n",
    "        max_tokens = len(x_train_tokens[i])\n",
    "\n",
    "max_tokens = min(max_tokens, TEXT_LEN_LIMIT)\n",
    "print(max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "\n",
    "y_train_pad = encoder.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens, padding=PADDING, truncating=PADDING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1433 1433\n",
      "16 52\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train_pad), len(y_train_pad))\n",
    "print(len((x_train_pad[0])), len((y_train_pad[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "\n",
    "inverse_map = dict(zip(word_index.values(), word_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_string(tokens):\n",
    "    # Map from tokens back to words.\n",
    "    words = [inverse_map[token] for token in tokens if token != 0]\n",
    "\n",
    "    # Concatenate all words.\n",
    "    text = \" \".join(words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 55],\n",
       "      dtype=int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(x_train_processed[1])\n",
    "display(x_train_pad[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_to_string(x_train_pad[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove.6B.zip\n",
      "../data/glove.6B ../data/glove.6B.zip\n",
      "Downloaded zip file\n",
      "Extracted zip file\n"
     ]
    }
   ],
   "source": [
    "glove_word_vec_download_url = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "print(glove_word_vec_download_url.rsplit(\"/\", 1)[1])\n",
    "GLOVE_ZIP_PATH = os.path.join(DATA_FOLDER, glove_word_vec_download_url.rsplit(\"/\", 1)[1])\n",
    "GLOVE_FOLDER_PATH = os.path.join(DATA_FOLDER, glove_word_vec_download_url.rsplit(\"/\", 1)[1].rsplit(\".\", 1)[0])\n",
    "GLOVE_FILE = \"glove.6B.100d.txt\"\n",
    "GLOVE_VEC_SIZE = 100\n",
    "print(GLOVE_FOLDER_PATH, GLOVE_ZIP_PATH)\n",
    "\n",
    "if not os.path.exists(GLOVE_ZIP_PATH):\n",
    "    req = requests.get(glove_word_vec_download_url, allow_redirects=True)\n",
    "    # Writing the file to the local file system\n",
    "    with open(GLOVE_ZIP_PATH, 'wb') as output_file:\n",
    "        output_file.write(req.content)\n",
    "    print(\"Downloaded zip file\")\n",
    "else:\n",
    "    print(\"Zip file already present\")\n",
    "\n",
    "if not os.path.exists(GLOVE_FOLDER_PATH):\n",
    "    with zipfile.ZipFile(GLOVE_ZIP_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(GLOVE_FOLDER_PATH)\n",
    "    print(\"Extracted zip file\")\n",
    "else:\n",
    "    print(\"Files already present on disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sigarms', 'katuna', 'aqm', '1.3775', 'corythosaurus', 'chanty', 'kronik', 'rolonda', 'zsombor', 'sandberger']\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = dict()\n",
    "\n",
    "with open(os.path.join(GLOVE_FOLDER_PATH, GLOVE_FILE)) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(list(embeddings_index.keys())[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 20000\n",
    "EMBEDDING_DIM = GLOVE_VEC_SIZE\n",
    "# BATCH_SIZE = 128\n",
    "BATCH_SIZE = 64\n",
    "# BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "    if index > VOCAB_SIZE - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-01 04:15:38.778952: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-03-01 04:15:38.779643: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "text_input = Input(shape=(max_tokens,), name='text_input')\n",
    "\n",
    "emb_layer = Embedding(\n",
    "    input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=max_tokens, weights=[embedding_matrix],\n",
    "    trainable=False, name='embedding_layer'\n",
    ")(text_input)\n",
    "\n",
    "main_lstm = LSTM(units=num_intents, dropout=0.4, return_sequences=True, name='lstm_1')(emb_layer)\n",
    "\n",
    "sec_lstm = LSTM(units=num_intents, dropout=0.2, return_sequences=True, name='lstm_2')(main_lstm)\n",
    "\n",
    "final_lstm = LSTM(units=num_intents, name='last_lstm')(sec_lstm)\n",
    "\n",
    "out = Dense(num_intents, activation='softmax', name='out')(final_lstm)\n",
    "\n",
    "model = Model(text_input, out, name=\"intent_classifier\")\n",
    "\n",
    "model.build(input_shape=(BATCH_SIZE, max_tokens,))\n",
    "\n",
    "optimizer = adam_v2.Adam(learning_rate=1e-3)\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"intent_classifier\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_input (InputLayer)      [(None, 16)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_layer (Embedding)  (None, 16, 100)           2000000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 16, 52)            31824     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 16, 52)            21840     \n",
      "_________________________________________________________________\n",
      "last_lstm (LSTM)             (None, 52)                21840     \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 52)                2756      \n",
      "=================================================================\n",
      "Total params: 2,078,260\n",
      "Trainable params: 78,260\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(x_train_pad, y_train_pad, test_size=0.0)\n",
    "# # X_train, X_test, y_train, y_test=train_test_split(x_train_pad,y_train_pad,test_size=0.1, stratify=y_train_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = x_train_pad, y_train_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-01 04:15:50.467126: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-01 04:15:52.246300: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 50s 2s/step - loss: 3.8122 - accuracy: 0.2666\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - 48s 2s/step - loss: 3.1489 - accuracy: 0.2903\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - 53s 2s/step - loss: 2.9967 - accuracy: 0.2903\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - 57s 2s/step - loss: 2.8911 - accuracy: 0.2924\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - 65s 3s/step - loss: 2.7942 - accuracy: 0.2980\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - 68s 3s/step - loss: 2.7038 - accuracy: 0.3008\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - 74s 3s/step - loss: 2.6274 - accuracy: 0.3105\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - 60s 3s/step - loss: 2.5666 - accuracy: 0.3210\n",
      "Epoch 9/100\n",
      "23/23 [==============================] - 59s 3s/step - loss: 2.5116 - accuracy: 0.3308\n",
      "Epoch 10/100\n",
      "23/23 [==============================] - 60s 3s/step - loss: 2.4426 - accuracy: 0.3419\n",
      "Epoch 11/100\n",
      "23/23 [==============================] - 62s 3s/step - loss: 2.3754 - accuracy: 0.3678\n",
      "Epoch 12/100\n",
      "23/23 [==============================] - 60s 3s/step - loss: 2.3186 - accuracy: 0.3950\n",
      "Epoch 13/100\n",
      "23/23 [==============================] - 60s 3s/step - loss: 2.2703 - accuracy: 0.3957\n",
      "Epoch 14/100\n",
      "23/23 [==============================] - 62s 3s/step - loss: 2.2272 - accuracy: 0.4201\n",
      "Epoch 15/100\n",
      "23/23 [==============================] - 66s 3s/step - loss: 2.1776 - accuracy: 0.4382\n",
      "Epoch 16/100\n",
      "23/23 [==============================] - 65s 3s/step - loss: 2.1272 - accuracy: 0.4431\n",
      "Epoch 17/100\n",
      "23/23 [==============================] - 64s 3s/step - loss: 2.0973 - accuracy: 0.4564\n",
      "Epoch 18/100\n",
      "23/23 [==============================] - 63s 3s/step - loss: 2.0709 - accuracy: 0.4696\n",
      "Epoch 19/100\n",
      "23/23 [==============================] - 71s 3s/step - loss: 2.0114 - accuracy: 0.4948\n",
      "Epoch 20/100\n",
      "23/23 [==============================] - 64s 3s/step - loss: 1.9508 - accuracy: 0.5024\n",
      "Epoch 21/100\n",
      "23/23 [==============================] - 61s 3s/step - loss: 1.9221 - accuracy: 0.5003\n",
      "Epoch 22/100\n",
      "23/23 [==============================] - 58s 3s/step - loss: 1.8812 - accuracy: 0.5178\n",
      "Epoch 23/100\n",
      "23/23 [==============================] - 58s 3s/step - loss: 1.8420 - accuracy: 0.5185\n",
      "Epoch 24/100\n",
      "23/23 [==============================] - 65s 3s/step - loss: 1.8041 - accuracy: 0.5324\n",
      "Epoch 25/100\n",
      "23/23 [==============================] - 61s 3s/step - loss: 1.7803 - accuracy: 0.5338\n",
      "Epoch 26/100\n",
      "23/23 [==============================] - 69s 3s/step - loss: 1.7515 - accuracy: 0.5485\n",
      "Epoch 27/100\n",
      "23/23 [==============================] - 68s 3s/step - loss: 1.7049 - accuracy: 0.5492\n",
      "Epoch 28/100\n",
      "23/23 [==============================] - 60s 3s/step - loss: 1.6683 - accuracy: 0.5687\n",
      "Epoch 29/100\n",
      "23/23 [==============================] - 63s 3s/step - loss: 1.6472 - accuracy: 0.5687\n",
      "Epoch 30/100\n",
      "23/23 [==============================] - 61s 3s/step - loss: 1.6205 - accuracy: 0.5673\n",
      "Epoch 31/100\n",
      "23/23 [==============================] - 58s 3s/step - loss: 1.5829 - accuracy: 0.5771\n",
      "Epoch 32/100\n",
      "23/23 [==============================] - 54s 2s/step - loss: 1.5509 - accuracy: 0.5946\n",
      "Epoch 33/100\n",
      "23/23 [==============================] - 54s 2s/step - loss: 1.5228 - accuracy: 0.6022\n",
      "Epoch 34/100\n",
      "23/23 [==============================] - 62s 3s/step - loss: 1.4714 - accuracy: 0.6113\n",
      "Epoch 35/100\n",
      "23/23 [==============================] - 68s 3s/step - loss: 1.4498 - accuracy: 0.6092\n",
      "Epoch 36/100\n",
      "23/23 [==============================] - 58s 3s/step - loss: 1.4081 - accuracy: 0.6336\n",
      "Epoch 37/100\n",
      "23/23 [==============================] - 58s 3s/step - loss: 1.3814 - accuracy: 0.6239\n",
      "Epoch 38/100\n",
      "23/23 [==============================] - 64s 3s/step - loss: 1.3606 - accuracy: 0.6441\n",
      "Epoch 39/100\n",
      "23/23 [==============================] - 85s 4s/step - loss: 1.3410 - accuracy: 0.6385\n",
      "Epoch 40/100\n",
      "23/23 [==============================] - 66s 3s/step - loss: 1.3041 - accuracy: 0.6518\n",
      "Epoch 41/100\n",
      "23/23 [==============================] - 63s 3s/step - loss: 1.2681 - accuracy: 0.6615\n",
      "Epoch 42/100\n",
      "23/23 [==============================] - 73s 3s/step - loss: 1.2576 - accuracy: 0.6643\n",
      "Epoch 43/100\n",
      "23/23 [==============================] - 61s 3s/step - loss: 1.2111 - accuracy: 0.6727\n",
      "Epoch 44/100\n",
      "23/23 [==============================] - 66s 3s/step - loss: 1.2113 - accuracy: 0.6629\n",
      "Epoch 45/100\n",
      "23/23 [==============================] - 67s 3s/step - loss: 1.1750 - accuracy: 0.6916\n",
      "Epoch 46/100\n",
      "23/23 [==============================] - 67s 3s/step - loss: 1.1533 - accuracy: 0.6888\n",
      "Epoch 47/100\n",
      "23/23 [==============================] - 60s 3s/step - loss: 1.1249 - accuracy: 0.6964\n",
      "Epoch 48/100\n",
      "23/23 [==============================] - 57s 2s/step - loss: 1.0996 - accuracy: 0.7048\n",
      "Epoch 49/100\n",
      "23/23 [==============================] - 60s 3s/step - loss: 1.0773 - accuracy: 0.7006\n",
      "Epoch 50/100\n",
      "23/23 [==============================] - 62s 3s/step - loss: 1.0777 - accuracy: 0.7111\n",
      "Epoch 51/100\n",
      "23/23 [==============================] - 61s 3s/step - loss: 1.0381 - accuracy: 0.7195\n",
      "Epoch 52/100\n",
      "23/23 [==============================] - 62s 3s/step - loss: 1.0141 - accuracy: 0.7181\n",
      "Epoch 53/100\n",
      "23/23 [==============================] - 59s 3s/step - loss: 0.9952 - accuracy: 0.7383\n",
      "Epoch 54/100\n",
      "23/23 [==============================] - 59s 3s/step - loss: 0.9626 - accuracy: 0.7446\n",
      "Epoch 55/100\n",
      "23/23 [==============================] - 58s 3s/step - loss: 0.9487 - accuracy: 0.7467\n",
      "Epoch 56/100\n",
      "23/23 [==============================] - 58s 3s/step - loss: 0.9393 - accuracy: 0.7572\n",
      "Epoch 57/100\n",
      "23/23 [==============================] - 62s 3s/step - loss: 0.9150 - accuracy: 0.7537\n",
      "Epoch 58/100\n",
      "23/23 [==============================] - 57s 2s/step - loss: 0.9130 - accuracy: 0.7544\n",
      "Epoch 59/100\n",
      "23/23 [==============================] - 58s 3s/step - loss: 0.8793 - accuracy: 0.7641\n",
      "Epoch 60/100\n",
      "23/23 [==============================] - 59s 3s/step - loss: 0.8774 - accuracy: 0.7655\n",
      "Epoch 61/100\n",
      "23/23 [==============================] - 59s 3s/step - loss: 0.8660 - accuracy: 0.7648\n",
      "Epoch 62/100\n",
      "23/23 [==============================] - 58s 3s/step - loss: 0.8335 - accuracy: 0.7732\n",
      "Epoch 63/100\n",
      "23/23 [==============================] - 59s 3s/step - loss: 0.8177 - accuracy: 0.7886\n",
      "Epoch 64/100\n",
      "23/23 [==============================] - 65s 3s/step - loss: 0.8019 - accuracy: 0.7927\n",
      "Epoch 65/100\n",
      "23/23 [==============================] - 61s 3s/step - loss: 0.7736 - accuracy: 0.7927\n",
      "Epoch 66/100\n",
      "23/23 [==============================] - 58s 3s/step - loss: 0.7736 - accuracy: 0.7962\n",
      "Epoch 67/100\n",
      "23/23 [==============================] - 57s 2s/step - loss: 0.7696 - accuracy: 0.8032\n",
      "Epoch 68/100\n",
      "23/23 [==============================] - 57s 2s/step - loss: 0.7526 - accuracy: 0.8081\n",
      "Epoch 69/100\n",
      "23/23 [==============================] - 56s 2s/step - loss: 0.7471 - accuracy: 0.8018\n",
      "Epoch 70/100\n",
      "23/23 [==============================] - 59s 3s/step - loss: 0.7393 - accuracy: 0.8053\n",
      "Epoch 71/100\n",
      "23/23 [==============================] - 61s 3s/step - loss: 0.7066 - accuracy: 0.8207\n",
      "Epoch 72/100\n",
      "23/23 [==============================] - 59s 3s/step - loss: 0.7056 - accuracy: 0.8088\n",
      "Epoch 73/100\n",
      "23/23 [==============================] - 58s 3s/step - loss: 0.6874 - accuracy: 0.8172\n",
      "Epoch 74/100\n",
      "23/23 [==============================] - 64s 3s/step - loss: 0.6765 - accuracy: 0.8276\n",
      "Epoch 75/100\n",
      "23/23 [==============================] - 68s 3s/step - loss: 0.6661 - accuracy: 0.8221\n",
      "Epoch 76/100\n",
      "23/23 [==============================] - 67s 3s/step - loss: 0.6710 - accuracy: 0.8311\n",
      "Epoch 77/100\n",
      "23/23 [==============================] - 67s 3s/step - loss: 0.6324 - accuracy: 0.8409\n",
      "Epoch 78/100\n",
      "23/23 [==============================] - 60s 3s/step - loss: 0.6429 - accuracy: 0.8353\n",
      "Epoch 79/100\n",
      "23/23 [==============================] - 55s 2s/step - loss: 0.6151 - accuracy: 0.8486\n",
      "Epoch 80/100\n",
      "23/23 [==============================] - 56s 2s/step - loss: 0.6122 - accuracy: 0.8437\n",
      "Epoch 81/100\n",
      "23/23 [==============================] - 54s 2s/step - loss: 0.5897 - accuracy: 0.8521\n",
      "Epoch 82/100\n",
      "23/23 [==============================] - 55s 2s/step - loss: 0.5869 - accuracy: 0.8479\n",
      "Epoch 83/100\n",
      "23/23 [==============================] - 64s 3s/step - loss: 0.5909 - accuracy: 0.8555\n",
      "Epoch 84/100\n",
      "23/23 [==============================] - 65s 3s/step - loss: 0.5707 - accuracy: 0.8542\n",
      "Epoch 85/100\n",
      "23/23 [==============================] - 65s 3s/step - loss: 0.5523 - accuracy: 0.8604\n",
      "Epoch 86/100\n",
      "23/23 [==============================] - 65s 3s/step - loss: 0.5458 - accuracy: 0.8702\n",
      "Epoch 87/100\n",
      "23/23 [==============================] - 66s 3s/step - loss: 0.5408 - accuracy: 0.8667\n",
      "Epoch 88/100\n",
      "23/23 [==============================] - 64s 3s/step - loss: 0.5220 - accuracy: 0.8737\n",
      "Epoch 89/100\n",
      "23/23 [==============================] - 65s 3s/step - loss: 0.5025 - accuracy: 0.8828\n",
      "Epoch 90/100\n",
      "23/23 [==============================] - 64s 3s/step - loss: 0.5063 - accuracy: 0.8751\n",
      "Epoch 91/100\n",
      "23/23 [==============================] - 65s 3s/step - loss: 0.4903 - accuracy: 0.8786\n",
      "Epoch 92/100\n",
      "23/23 [==============================] - 64s 3s/step - loss: 0.4817 - accuracy: 0.8807\n",
      "Epoch 93/100\n",
      "23/23 [==============================] - 65s 3s/step - loss: 0.4863 - accuracy: 0.8786\n",
      "Epoch 94/100\n",
      "23/23 [==============================] - 66s 3s/step - loss: 0.4780 - accuracy: 0.8842\n",
      "Epoch 95/100\n",
      "23/23 [==============================] - 65s 3s/step - loss: 0.4734 - accuracy: 0.8856\n",
      "Epoch 96/100\n",
      "23/23 [==============================] - 66s 3s/step - loss: 0.4757 - accuracy: 0.8800\n",
      "Epoch 97/100\n",
      "23/23 [==============================] - 65s 3s/step - loss: 0.4801 - accuracy: 0.8744\n",
      "Epoch 98/100\n",
      "23/23 [==============================] - 66s 3s/step - loss: 0.4698 - accuracy: 0.8751\n",
      "Epoch 99/100\n",
      "23/23 [==============================] - 65s 3s/step - loss: 0.4459 - accuracy: 0.8890\n",
      "Epoch 100/100\n",
      "23/23 [==============================] - 68s 3s/step - loss: 0.4355 - accuracy: 0.8883\n",
      "CPU times: user 3h, sys: 4h 35min 17s, total: 7h 35min 18s\n",
      "Wall time: 1h 43min 15s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x336cc2fd0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# epochs ~ 100\n",
    "EPOCHS = 100\n",
    "# EPOCHS = 20\n",
    "model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FOLDER = \"../models\"\n",
    "INTENT_MODEL_FOLDER = \"intent\"\n",
    "MODEL_SAVE_FOLDER = os.path.join(MODEL_FOLDER, INTENT_MODEL_FOLDER)\n",
    "TF_MODEL_SAVE_FOLDER = os.path.join(MODEL_FOLDER, INTENT_MODEL_FOLDER, \"tf_serving\")\n",
    "MODEL_SAVE_PATH = os.path.join(MODEL_FOLDER, INTENT_MODEL_FOLDER, \"intent_model.h5\")\n",
    "# pickle files\n",
    "CUSTOM_OBJS_PKL_FILE = \"custom_objects.pickle\"\n",
    "CUSTOM_OBJS_PKL_PATH = os.path.join(MODEL_SAVE_FOLDER, CUSTOM_OBJS_PKL_FILE)\n",
    "CUSTOM_STOPWORDS_PKL_FILE = \"custom_stopwords.pickle\"\n",
    "CUSTOM_STOPWORDS_PKL_PATH = os.path.join(MODEL_SAVE_FOLDER, CUSTOM_STOPWORDS_PKL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = open(CUSTOM_OBJS_PKL_PATH, \"wb\")\n",
    "pickle.dump(tokenizer, pickle_file)\n",
    "pickle.dump(encoder, pickle_file)\n",
    "pickle.dump(max_tokens, pickle_file)\n",
    "pickle.dump(PADDING, pickle_file)\n",
    "pickle_file.close()\n",
    "\n",
    "pickle_file = open(CUSTOM_STOPWORDS_PKL_PATH, \"wb\")\n",
    "pickle.dump(custom_stopwords, pickle_file)\n",
    "pickle_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer last_lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(CUSTOM_OBJS_PKL_PATH, \"rb\")\n",
    "tokenizer = pickle.load(f)\n",
    "encoder = pickle.load(f)\n",
    "max_tokens = pickle.load(f)\n",
    "PADDING = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "pickle_file = open(CUSTOM_STOPWORDS_PKL_PATH, \"rb\")\n",
    "custom_stopwords = pickle.load(pickle_file)\n",
    "pickle_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Transfer Learning:\n",
    "# x_train_tokens = tokenizer.texts_to_sequences(x_train_final_input)\n",
    "# max_tokens = 0\n",
    "# for i in range(len(x_train_tokens)):\n",
    "#     if len(x_train_tokens[i]) > max_tokens:\n",
    "#         max_tokens = len(x_train_tokens[i])\n",
    "# pad = 'pre'\n",
    "\n",
    "# x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens,\n",
    "#                             padding=pad, truncating=pad)\n",
    "\n",
    "# y_train_pad = encoder.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_question=\"hello, how are you?\"\n",
    "# question=word_tokenize(user_question)\n",
    "# print(question)\n",
    "# doc=nlp(user_question)\n",
    "# print(doc)\n",
    "# for token in doc:\n",
    "#     print(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(user_text):\n",
    "    # lower case the text\n",
    "    text = lower_case_text(user_text)\n",
    "    # remove punctuation\n",
    "    text = text.translate(translator)\n",
    "    # remove stopwords\n",
    "    text = remove_custom_stopwords(text, custom_stopwords)\n",
    "\n",
    "    if not text:\n",
    "        text = user_text\n",
    "    # process the text using Spacy's language model \"en_core_web_sm\".\n",
    "    doc = nlp(text)\n",
    "    # lemmatization\n",
    "    text = [token.lemma_ for token in doc]\n",
    "    # detokenize text\n",
    "    text = detokenize_sent(text)\n",
    "    return text\n",
    "\n",
    "def preprocess_text_keeping_stopwords(user_text):\n",
    "    # lower case the text\n",
    "    text = lower_case_text(user_text)\n",
    "    # remove punctuation\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    if not text:\n",
    "        text = user_text\n",
    "    # process the text using Spacy's language model \"en_core_web_sm\".\n",
    "    doc = nlp(text)\n",
    "    # lemmatization\n",
    "    text = [token.lemma_ for token in doc]\n",
    "    # detokenize text\n",
    "    text = detokenize_sent(text)\n",
    "    return text\n",
    "\n",
    "def prepare_text_for_prediction(text):\n",
    "    tokens = tokenizer.texts_to_sequences([text])\n",
    "    padded_tokens = pad_sequences(tokens, maxlen=max_tokens, padding=PADDING, truncating=PADDING)\n",
    "    return padded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-01 05:59:08.043523: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bot_challenge'] 0.99338055\n"
     ]
    }
   ],
   "source": [
    "user_text = preprocess_text(\"Are you a chat bot?\")\n",
    "tokens = prepare_text_for_prediction(user_text)\n",
    "\n",
    "predict_class = model.predict(tokens)\n",
    "\n",
    "intent = encoder.inverse_transform(predict_class)\n",
    "index_number = np.argmax(predict_class)\n",
    "intent_value = predict_class[0, index_number]\n",
    "\n",
    "print(intent, intent_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_intent(text):\n",
    "    text = preprocess_text(text)\n",
    "    tokens = prepare_text_for_prediction(text)\n",
    "\n",
    "    predict_class = model.predict(tokens)\n",
    "\n",
    "    intent = encoder.inverse_transform(predict_class)\n",
    "    index_number = np.argmax(predict_class)\n",
    "    intent_value = predict_class[0, index_number]\n",
    "\n",
    "    print(intent[0], \"\\t\\t\", \"{0:2f}\".format(intent_value * 100))\n",
    "    return intent, intent_value\n",
    "\n",
    "def predict_intent_without_stopwords(text):\n",
    "    text = preprocess_text_keeping_stopwords(text)\n",
    "    tokens = prepare_text_for_prediction(text)\n",
    "\n",
    "    predict_class = model.predict(tokens)\n",
    "\n",
    "    intent = encoder.inverse_transform(predict_class)\n",
    "    index_number = np.argmax(predict_class)\n",
    "    intent_value = predict_class[0, index_number]\n",
    "\n",
    "    print(intent[0], \"\\t\\t\", \"{0:2f}\".format(intent_value * 100))\n",
    "    return intent, intent_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affirm \t\t 88.048750\n",
      "deny \t\t 88.365167\n",
      "stop \t\t 89.013112\n",
      "greet \t\t 95.675308\n",
      "bot_challenge \t\t 98.590183\n",
      "explain_deep_learning \t\t 64.633191\n",
      "explain_intents \t\t 55.773479\n",
      "explain_keras \t\t 44.333079\n",
      "explain_nlp \t\t 22.855259\n",
      "explain_nlu \t\t 32.276469\n",
      "why_machine_learning \t\t 79.801780\n",
      "explain_support_bot \t\t 21.450703\n",
      "when_will_you_beat_turing_test \t\t 80.211788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(['when_will_you_beat_turing_test'], dtype='<U30'), 0.8021179)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_intent(\"yes\")\n",
    "predict_intent(\"no\")\n",
    "predict_intent(\"stop\")\n",
    "predict_intent(\"Hello\")\n",
    "predict_intent(\"Hi there, are you the bot?\")\n",
    "predict_intent(\"What is deep learning?\")\n",
    "predict_intent(\"What are intents?\")\n",
    "predict_intent(\"What is keras?\")\n",
    "predict_intent(\"what is nlp?\")\n",
    "predict_intent(\"what is natural language understanding?\")\n",
    "predict_intent(\"why use machine learning in bots?\")\n",
    "predict_intent(\"why make a chatbot?\")\n",
    "predict_intent(\"when will you beat the turing test?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greet \t\t 96.712577\n",
      "greet \t\t 95.675308\n",
      "greet \t\t 96.930164\n",
      "greet \t\t 95.675308\n",
      "greet \t\t 65.860093\n",
      "greet \t\t 62.380773\n",
      "greet \t\t 94.388998\n",
      "greet \t\t 96.712577\n",
      "bye \t\t 52.806836\n",
      "greet \t\t 97.665173\n",
      "greet \t\t 88.771784\n",
      "inform \t\t 81.191975\n",
      "greet \t\t 70.120573\n",
      "inform \t\t 81.191975\n",
      "greet \t\t 94.575262\n",
      "greet \t\t 97.111398\n",
      "greet \t\t 89.354163\n",
      "greet \t\t 92.160153\n",
      "greet \t\t 95.675308\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 81.191975\n",
      "greet \t\t 95.942032\n",
      "inform \t\t 81.191975\n",
      "greet \t\t 96.712577\n",
      "greet \t\t 81.731331\n",
      "greet \t\t 96.108145\n",
      "mood_ask \t\t 19.572946\n",
      "greet \t\t 77.439749\n",
      "greet \t\t 95.675308\n",
      "greet \t\t 42.059654\n",
      "greet \t\t 62.380773\n",
      "greet \t\t 96.930164\n",
      "greet \t\t 96.267837\n",
      "greet \t\t 94.336903\n",
      "greet \t\t 96.930164\n",
      "greet \t\t 96.930164\n",
      "mood_ask \t\t 19.572946\n",
      "greet \t\t 78.801823\n",
      "greet \t\t 95.675308\n",
      "inform \t\t 81.191975\n",
      "greet \t\t 94.959420\n",
      "greet \t\t 94.959420\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 81.191975\n",
      "get_started \t\t 32.751909\n",
      "greet \t\t 89.354163\n",
      "greet \t\t 97.665173\n",
      "greet \t\t 89.823204\n",
      "greet \t\t 97.232473\n",
      "greet \t\t 96.712577\n",
      "inform \t\t 81.191975\n",
      "greet \t\t 97.194666\n",
      "greet \t\t 96.916503\n",
      "greet \t\t 70.120573\n",
      "greet \t\t 96.571702\n",
      "greet \t\t 92.160153\n",
      "greet \t\t 96.930164\n",
      "greet \t\t 91.208094\n",
      "greet \t\t 95.203650\n",
      "inform \t\t 81.191975\n",
      "greet \t\t 97.604162\n",
      "greet \t\t 96.712577\n",
      "greet \t\t 97.734207\n",
      "greet \t\t 95.675308\n",
      "greet \t\t 96.930164\n",
      "greet \t\t 96.930164\n",
      "greet \t\t 97.111398\n",
      "greet \t\t 97.023666\n",
      "greet \t\t 97.220242\n",
      "greet \t\t 97.140932\n",
      "greet \t\t 96.930164\n",
      "greet \t\t 95.203650\n",
      "greet \t\t 94.575262\n",
      "greet \t\t 95.203650\n",
      "greet \t\t 94.575262\n",
      "greet \t\t 86.017364\n",
      "greet \t\t 95.675308\n",
      "greet \t\t 91.208094\n",
      "greet \t\t 96.930164\n",
      "greet \t\t 96.712577\n",
      "greet \t\t 95.853704\n",
      "greet \t\t 97.111398\n",
      "greet \t\t 95.675308\n",
      "greet \t\t 96.930164\n",
      "greet \t\t 97.551525\n",
      "greet \t\t 65.860093\n",
      "greet \t\t 48.878196\n",
      "affirm \t\t 52.811337\n",
      "bye \t\t 55.808109\n",
      "greet \t\t 33.358198\n",
      "bye \t\t 70.568407\n",
      "bye \t\t 59.946555\n",
      "good_day \t\t 65.118843\n",
      "bye \t\t 49.889320\n",
      "bye \t\t 82.349676\n",
      "bye \t\t 61.704260\n",
      "good_day \t\t 65.118843\n",
      "good_day \t\t 66.999459\n",
      "good_day \t\t 78.669798\n",
      "affirm \t\t 42.945936\n",
      "greet \t\t 62.380773\n",
      "good_day \t\t 73.413855\n",
      "affirm \t\t 54.899848\n",
      "affirm \t\t 40.179664\n",
      "affirm \t\t 42.657343\n",
      "nice_talking_to_you \t\t 25.302178\n",
      "mood_unhappy \t\t 33.014688\n",
      "mood_unhappy \t\t 29.480499\n",
      "mood_great \t\t 41.155234\n",
      "mood_great \t\t 47.736916\n",
      "human_handoff \t\t 64.421034\n",
      "affirm \t\t 27.813646\n",
      "mood_unhappy \t\t 36.600572\n",
      "affirm \t\t 54.899848\n",
      "affirm \t\t 52.811337\n",
      "affirm \t\t 54.899848\n",
      "mood_unhappy \t\t 48.602450\n",
      "mood_unhappy \t\t 62.237602\n",
      "affirm \t\t 28.504354\n",
      "affirm \t\t 55.750960\n",
      "mood_unhappy \t\t 59.786576\n",
      "mood_unhappy \t\t 62.237602\n",
      "mood_unhappy \t\t 53.512102\n",
      "mood_unhappy \t\t 53.512102\n",
      "affirm \t\t 37.547585\n",
      "mood_unhappy \t\t 38.848239\n",
      "mood_unhappy \t\t 38.848239\n",
      "mood_unhappy \t\t 57.736748\n",
      "affirm \t\t 42.100555\n",
      "mood_unhappy \t\t 53.512102\n",
      "bot_challenge \t\t 99.560672\n",
      "bot_challenge \t\t 99.126804\n",
      "bot_challenge \t\t 94.478852\n",
      "bot_challenge \t\t 84.095740\n",
      "bot_challenge \t\t 98.590183\n",
      "bot_challenge \t\t 99.338055\n",
      "bot_challenge \t\t 93.604910\n",
      "bot_challenge \t\t 99.537909\n",
      "bot_challenge \t\t 97.621268\n",
      "bot_challenge \t\t 97.753602\n",
      "ask_whatspossible \t\t 40.753400\n",
      "bot_challenge \t\t 94.338852\n",
      "bot_challenge \t\t 99.560672\n",
      "bot_challenge \t\t 89.242572\n",
      "bot_challenge \t\t 97.023058\n",
      "bot_challenge \t\t 99.560672\n",
      "bot_challenge \t\t 98.053104\n",
      "mood_ask \t\t 40.442711\n",
      "bot_challenge \t\t 80.344015\n",
      "bot_challenge \t\t 96.467686\n",
      "bot_challenge \t\t 99.560672\n",
      "bot_challenge \t\t 99.560672\n",
      "bot_challenge \t\t 99.560672\n",
      "bot_challenge \t\t 98.675597\n",
      "bot_challenge \t\t 99.560672\n",
      "bot_challenge \t\t 98.506099\n",
      "bot_challenge \t\t 99.537909\n",
      "bot_challenge \t\t 88.270932\n",
      "bot_challenge \t\t 99.537909\n",
      "bot_challenge \t\t 99.111676\n",
      "bot_challenge \t\t 94.204760\n",
      "bot_challenge \t\t 90.582395\n",
      "bot_challenge \t\t 97.455752\n",
      "bot_challenge \t\t 85.552680\n",
      "bot_challenge \t\t 51.485509\n",
      "bot_challenge \t\t 98.674726\n",
      "bot_challenge \t\t 96.511394\n",
      "bot_challenge \t\t 99.341500\n",
      "affirm \t\t 88.048750\n",
      "affirm \t\t 79.998797\n",
      "affirm \t\t 88.048750\n",
      "affirm \t\t 88.048750\n",
      "affirm \t\t 70.104641\n",
      "deny \t\t 48.110697\n",
      "affirm \t\t 84.914976\n",
      "affirm \t\t 34.391624\n",
      "affirm \t\t 65.160900\n",
      "affirm \t\t 56.817645\n",
      "affirm \t\t 48.934647\n",
      "inform \t\t 81.191975\n",
      "affirm \t\t 92.138159\n",
      "affirm \t\t 76.740730\n",
      "affirm \t\t 88.267505\n",
      "affirm \t\t 88.267505\n",
      "affirm \t\t 63.382703\n",
      "affirm \t\t 86.036801\n",
      "affirm \t\t 72.416544\n",
      "affirm \t\t 69.417435\n",
      "affirm \t\t 82.585394\n",
      "deny \t\t 41.631326\n",
      "affirm \t\t 93.839872\n",
      "affirm \t\t 80.182397\n",
      "affirm \t\t 40.179664\n",
      "affirm \t\t 80.182397\n",
      "affirm \t\t 86.792988\n",
      "affirm \t\t 67.818785\n",
      "affirm \t\t 94.118959\n",
      "affirm \t\t 69.730753\n",
      "affirm \t\t 42.657343\n",
      "mood_ask \t\t 93.661201\n",
      "affirm \t\t 76.835001\n",
      "affirm \t\t 65.546745\n",
      "affirm \t\t 73.300171\n",
      "affirm \t\t 78.534472\n",
      "affirm \t\t 65.546745\n",
      "affirm \t\t 82.172638\n",
      "affirm \t\t 71.427780\n",
      "affirm \t\t 28.192556\n",
      "affirm \t\t 54.899848\n",
      "affirm \t\t 74.591446\n",
      "affirm \t\t 76.731378\n",
      "affirm \t\t 64.596117\n",
      "affirm \t\t 77.282649\n",
      "affirm \t\t 71.907330\n",
      "affirm \t\t 59.006232\n",
      "affirm \t\t 77.784550\n",
      "affirm \t\t 75.902319\n",
      "affirm \t\t 88.048750\n",
      "affirm \t\t 76.374942\n",
      "affirm \t\t 85.949796\n",
      "affirm \t\t 85.590130\n",
      "affirm \t\t 68.324578\n",
      "affirm \t\t 29.798597\n",
      "affirm \t\t 76.835001\n",
      "affirm \t\t 86.809951\n",
      "affirm \t\t 83.557165\n",
      "affirm \t\t 65.546745\n",
      "affirm \t\t 88.048750\n",
      "affirm \t\t 91.534764\n",
      "affirm \t\t 88.267505\n",
      "affirm \t\t 84.914976\n",
      "affirm \t\t 88.048750\n",
      "affirm \t\t 84.914976\n",
      "affirm \t\t 81.361789\n",
      "affirm \t\t 71.427780\n",
      "affirm \t\t 88.048750\n",
      "affirm \t\t 71.427780\n",
      "affirm \t\t 88.267505\n",
      "affirm \t\t 75.902319\n",
      "affirm \t\t 88.048750\n",
      "affirm \t\t 84.914976\n",
      "affirm \t\t 81.361789\n",
      "affirm \t\t 91.534764\n",
      "bye \t\t 70.568407\n",
      "inform \t\t 81.191975\n",
      "bye \t\t 73.682177\n",
      "bye \t\t 59.946555\n",
      "bye \t\t 59.946555\n",
      "bye \t\t 70.568407\n",
      "bye \t\t 82.349676\n",
      "bye \t\t 78.311980\n",
      "bye \t\t 73.682177\n",
      "bye \t\t 34.610191\n",
      "bye \t\t 70.568407\n",
      "bye \t\t 61.704260\n",
      "inform \t\t 81.191975\n",
      "bye \t\t 59.946555\n",
      "bye \t\t 61.547977\n",
      "bye \t\t 44.328761\n",
      "greet \t\t 33.358198\n",
      "bye \t\t 48.103398\n",
      "bye \t\t 34.610191\n",
      "inform \t\t 81.191975\n",
      "bye \t\t 82.349676\n",
      "bye \t\t 46.814641\n",
      "bye \t\t 58.972478\n",
      "bye \t\t 63.445181\n",
      "bye \t\t 66.844785\n",
      "bye \t\t 70.568407\n",
      "bye \t\t 83.364749\n",
      "bye \t\t 76.021290\n",
      "affirm \t\t 48.770687\n",
      "bye \t\t 69.554174\n",
      "bye \t\t 74.279743\n",
      "bye \t\t 79.223686\n",
      "deny \t\t 87.831670\n",
      "affirm \t\t 50.823998\n",
      "deny \t\t 79.699594\n",
      "deny \t\t 89.048505\n",
      "deny \t\t 81.265229\n",
      "deny \t\t 88.365167\n",
      "deny \t\t 82.208949\n",
      "deny \t\t 79.048812\n",
      "deny \t\t 91.716784\n",
      "feedback \t\t 45.378372\n",
      "deny \t\t 54.404360\n",
      "affirm \t\t 50.823998\n",
      "deny \t\t 76.682097\n",
      "deny \t\t 73.763144\n",
      "deny \t\t 67.696315\n",
      "deny \t\t 81.147414\n",
      "deny \t\t 82.208949\n",
      "deny \t\t 80.997223\n",
      "deny \t\t 88.365167\n",
      "deny \t\t 53.246009\n",
      "deny \t\t 90.369385\n",
      "deny \t\t 83.621126\n",
      "deny \t\t 94.226748\n",
      "deny \t\t 92.380148\n",
      "deny \t\t 64.329261\n",
      "deny \t\t 88.365167\n",
      "deny \t\t 83.621126\n",
      "deny \t\t 94.226748\n",
      "deny \t\t 92.380148\n",
      "deny \t\t 83.069813\n",
      "deny \t\t 89.366168\n",
      "deny \t\t 85.301125\n",
      "deny \t\t 90.369385\n",
      "deny \t\t 57.375818\n",
      "deny \t\t 88.365167\n",
      "deny \t\t 93.499708\n",
      "deny \t\t 94.226748\n",
      "deny \t\t 83.621126\n",
      "deny \t\t 90.369385\n",
      "deny \t\t 75.698370\n",
      "deny \t\t 63.216454\n",
      "deny \t\t 66.906440\n",
      "deny \t\t 94.407737\n",
      "deny \t\t 89.424127\n",
      "deny \t\t 78.430218\n",
      "deny \t\t 92.408264\n",
      "deny \t\t 67.464083\n",
      "deny \t\t 93.039715\n",
      "deny \t\t 94.061017\n",
      "deny \t\t 77.150822\n",
      "stop \t\t 79.112142\n",
      "stop \t\t 89.013112\n",
      "stop \t\t 87.604225\n",
      "stop \t\t 89.013112\n",
      "stop \t\t 79.112142\n",
      "stop \t\t 89.656299\n",
      "stop \t\t 95.611370\n",
      "stop \t\t 93.924350\n",
      "stop \t\t 88.457417\n",
      "stop \t\t 85.062391\n",
      "stop \t\t 97.183561\n",
      "stop \t\t 89.013112\n",
      "stop \t\t 96.946710\n",
      "stop \t\t 85.062391\n",
      "stop \t\t 88.457417\n",
      "stop \t\t 96.946710\n",
      "stop \t\t 79.112142\n",
      "stop \t\t 97.183561\n",
      "stop \t\t 88.608748\n",
      "stop \t\t 85.062391\n",
      "stop \t\t 85.062391\n",
      "stop \t\t 89.013112\n",
      "stop \t\t 95.611370\n",
      "stop \t\t 89.013112\n",
      "stop \t\t 93.924350\n",
      "stop \t\t 89.013112\n",
      "stop \t\t 88.608748\n",
      "stop \t\t 79.112142\n",
      "stop \t\t 89.656299\n",
      "good_day \t\t 65.118843\n",
      "good_day \t\t 66.999459\n",
      "good_day \t\t 78.669798\n",
      "inform \t\t 51.886880\n",
      "greet \t\t 62.380773\n",
      "good_day \t\t 73.413855\n",
      "handle_insult \t\t 32.154807\n",
      "handle_insult \t\t 63.095105\n",
      "affirm \t\t 59.077668\n",
      "affirm \t\t 52.763820\n",
      "affirm \t\t 29.167229\n",
      "affirm \t\t 43.247572\n",
      "handle_insult \t\t 67.752123\n",
      "affirm \t\t 43.247572\n",
      "nice_talking_to_you \t\t 35.656330\n",
      "affirm \t\t 58.202744\n",
      "handle_insult \t\t 63.095105\n",
      "handle_insult \t\t 67.752123\n",
      "bot_challenge \t\t 51.485509\n",
      "handle_insult \t\t 45.190153\n",
      "nice_talking_to_you \t\t 35.656330\n",
      "handle_insult \t\t 52.014893\n",
      "human_handoff \t\t 97.482342\n",
      "human_handoff \t\t 97.482342\n",
      "human_handoff \t\t 90.284508\n",
      "human_handoff \t\t 92.950130\n",
      "human_handoff \t\t 68.026149\n",
      "human_handoff \t\t 90.203905\n",
      "human_handoff \t\t 98.184776\n",
      "human_handoff \t\t 89.450932\n",
      "human_handoff \t\t 97.148335\n",
      "human_handoff \t\t 97.718471\n",
      "human_handoff \t\t 97.404206\n",
      "human_handoff \t\t 85.155720\n",
      "human_handoff \t\t 85.707808\n",
      "human_handoff \t\t 98.184776\n",
      "human_handoff \t\t 79.222053\n",
      "human_handoff \t\t 84.046382\n",
      "human_handoff \t\t 76.413715\n",
      "human_handoff \t\t 97.213483\n",
      "human_handoff \t\t 98.053449\n",
      "human_handoff \t\t 96.081811\n",
      "human_handoff \t\t 85.978627\n",
      "human_handoff \t\t 97.947419\n",
      "human_handoff \t\t 93.946719\n",
      "human_handoff \t\t 85.155720\n",
      "human_handoff \t\t 85.540915\n",
      "human_handoff \t\t 67.293656\n",
      "human_handoff \t\t 98.628718\n",
      "human_handoff \t\t 66.026771\n",
      "human_handoff \t\t 92.444235\n",
      "human_handoff \t\t 64.931613\n",
      "human_handoff \t\t 96.103442\n",
      "human_handoff \t\t 96.796298\n",
      "human_handoff \t\t 64.073455\n",
      "human_handoff \t\t 98.554194\n",
      "human_handoff \t\t 99.121642\n",
      "human_handoff \t\t 98.159719\n",
      "thank \t\t 90.614241\n",
      "thank \t\t 97.610497\n",
      "thank \t\t 94.329602\n",
      "thank \t\t 92.794418\n",
      "thank \t\t 96.483737\n",
      "thank \t\t 97.758543\n",
      "thank \t\t 90.614241\n",
      "thank \t\t 94.329602\n",
      "thank \t\t 90.614241\n",
      "thank \t\t 90.614241\n",
      "thank \t\t 90.614241\n",
      "thank \t\t 96.799499\n",
      "thank \t\t 94.497496\n",
      "thank \t\t 94.329602\n",
      "thank \t\t 90.614241\n",
      "thank \t\t 96.483737\n",
      "thank \t\t 96.014631\n",
      "thank \t\t 96.285927\n",
      "thank \t\t 97.333330\n",
      "thank \t\t 95.867902\n",
      "thank \t\t 95.934403\n",
      "thank \t\t 73.234093\n",
      "thank \t\t 96.304673\n",
      "thank \t\t 97.929949\n",
      "thank \t\t 97.517866\n",
      "thank \t\t 84.091312\n",
      "inform \t\t 98.839033\n",
      "inform \t\t 99.374050\n",
      "inform \t\t 99.778533\n",
      "inform \t\t 99.736077\n",
      "inform \t\t 96.967679\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 96.870428\n",
      "inform \t\t 99.825352\n",
      "inform \t\t 99.841511\n",
      "inform \t\t 98.691678\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 99.805897\n",
      "inform \t\t 99.850357\n",
      "inform \t\t 90.166634\n",
      "inform \t\t 99.494344\n",
      "inform \t\t 99.557000\n",
      "inform \t\t 99.626547\n",
      "inform \t\t 95.348233\n",
      "inform \t\t 99.042481\n",
      "inform \t\t 99.420071\n",
      "inform \t\t 99.822038\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 95.795280\n",
      "inform \t\t 99.434137\n",
      "inform \t\t 99.359596\n",
      "inform \t\t 99.435920\n",
      "inform \t\t 99.807322\n",
      "inform \t\t 99.703985\n",
      "inform \t\t 98.949242\n",
      "inform \t\t 95.610553\n",
      "inform \t\t 99.504375\n",
      "inform \t\t 99.653506\n",
      "inform \t\t 99.095517\n",
      "inform \t\t 99.703324\n",
      "deny \t\t 64.785427\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 99.888664\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 81.191975\n",
      "deny \t\t 74.398988\n",
      "inform \t\t 98.643386\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 99.563867\n",
      "inform \t\t 99.613273\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 99.903286\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 98.755395\n",
      "inform \t\t 99.448591\n",
      "inform \t\t 99.501580\n",
      "inform \t\t 97.735274\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 99.799204\n",
      "inform \t\t 99.487203\n",
      "inform \t\t 98.599273\n",
      "inform \t\t 97.654015\n",
      "inform \t\t 98.959023\n",
      "inform \t\t 91.608936\n",
      "ask_whatspossible \t\t 36.069244\n",
      "inform \t\t 99.817896\n",
      "inform \t\t 98.828274\n",
      "inform \t\t 99.533832\n",
      "inform \t\t 99.066788\n",
      "inform \t\t 98.893636\n",
      "inform \t\t 94.516385\n",
      "inform \t\t 99.605221\n",
      "inform \t\t 97.478217\n",
      "inform \t\t 94.516385\n",
      "inform \t\t 99.123740\n",
      "inform \t\t 99.438703\n",
      "inform \t\t 99.737060\n",
      "get_started \t\t 40.719104\n",
      "inform \t\t 95.418394\n",
      "inform \t\t 41.138148\n",
      "inform \t\t 99.145365\n",
      "inform \t\t 98.621756\n",
      "inform \t\t 99.654168\n",
      "inform \t\t 99.897122\n",
      "inform \t\t 72.843039\n",
      "inform \t\t 99.624687\n",
      "inform \t\t 99.570054\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 99.605221\n",
      "inform \t\t 95.348233\n",
      "inform \t\t 99.849141\n",
      "inform \t\t 99.805105\n",
      "inform \t\t 99.719882\n",
      "inform \t\t 99.568462\n",
      "inform \t\t 99.515682\n",
      "inform \t\t 85.067368\n",
      "inform \t\t 99.537563\n",
      "inform \t\t 99.025017\n",
      "inform \t\t 99.369806\n",
      "inform \t\t 99.896514\n",
      "inform \t\t 99.735850\n",
      "inform \t\t 66.857398\n",
      "inform \t\t 90.812546\n",
      "inform \t\t 99.525827\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 94.604421\n",
      "inform \t\t 99.611807\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 99.597710\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 97.843552\n",
      "inform \t\t 99.432373\n",
      "inform \t\t 99.434137\n",
      "inform \t\t 99.197388\n",
      "inform \t\t 99.889660\n",
      "inform \t\t 86.817777\n",
      "inform \t\t 99.853456\n",
      "inform \t\t 98.608053\n",
      "inform \t\t 99.796522\n",
      "inform \t\t 99.726391\n",
      "inform \t\t 99.656409\n",
      "inform \t\t 99.845028\n",
      "inform \t\t 99.845028\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 98.037040\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 99.583399\n",
      "inform \t\t 96.462613\n",
      "inform \t\t 99.376410\n",
      "inform \t\t 99.895453\n",
      "inform \t\t 99.815410\n",
      "inform \t\t 99.439442\n",
      "inform \t\t 99.439442\n",
      "inform \t\t 99.914098\n",
      "inform \t\t 99.265313\n",
      "inform \t\t 96.186441\n",
      "inform \t\t 99.375421\n",
      "inform \t\t 99.848688\n",
      "inform \t\t 99.455678\n",
      "inform \t\t 99.769920\n",
      "inform \t\t 99.627411\n",
      "inform \t\t 98.945928\n",
      "inform \t\t 99.160284\n",
      "inform \t\t 99.205244\n",
      "inform \t\t 99.116009\n",
      "inform \t\t 98.405910\n",
      "inform \t\t 99.395639\n",
      "inform \t\t 99.142319\n",
      "inform \t\t 99.904472\n",
      "inform \t\t 99.913824\n",
      "inform \t\t 99.330729\n",
      "inform \t\t 99.361235\n",
      "inform \t\t 99.549675\n",
      "inform \t\t 96.420372\n",
      "inform \t\t 99.326527\n",
      "inform \t\t 99.522614\n",
      "inform \t\t 98.160660\n",
      "inform \t\t 99.863631\n",
      "inform \t\t 71.070468\n",
      "inform \t\t 99.324071\n",
      "inform \t\t 99.410582\n",
      "inform \t\t 99.079341\n",
      "inform \t\t 99.729449\n",
      "inform \t\t 99.265313\n",
      "inform \t\t 99.496233\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 99.427092\n",
      "inform \t\t 97.450167\n",
      "inform \t\t 99.807405\n",
      "inform \t\t 99.598557\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 99.889034\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 99.752444\n",
      "inform \t\t 99.881893\n",
      "inform \t\t 99.843937\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 99.575883\n",
      "inform \t\t 99.835443\n",
      "inform \t\t 99.619800\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 98.935473\n",
      "inform \t\t 98.806530\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 99.583215\n",
      "inform \t\t 99.107343\n",
      "inform \t\t 99.224550\n",
      "inform \t\t 99.836999\n",
      "inform \t\t 99.286920\n",
      "inform \t\t 90.723014\n",
      "inform \t\t 99.877721\n",
      "inform \t\t 99.687093\n",
      "inform \t\t 99.328917\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 98.554623\n",
      "inform \t\t 98.527145\n",
      "inform \t\t 99.749738\n",
      "inform \t\t 98.940045\n",
      "inform \t\t 99.463838\n",
      "inform \t\t 99.865085\n",
      "inform \t\t 99.633223\n",
      "inform \t\t 99.312699\n",
      "inform \t\t 99.911934\n",
      "inform \t\t 99.659038\n",
      "inform \t\t 99.391055\n",
      "inform \t\t 86.917394\n",
      "inform \t\t 99.341947\n",
      "inform \t\t 98.441792\n",
      "inform \t\t 99.615175\n",
      "inform \t\t 98.576087\n",
      "inform \t\t 99.829930\n",
      "inform \t\t 97.938997\n",
      "inform \t\t 92.512131\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 72.392190\n",
      "inform \t\t 93.955570\n",
      "inform \t\t 87.637919\n",
      "inform \t\t 99.359596\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 97.441000\n",
      "inform \t\t 97.441000\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 98.922616\n",
      "inform \t\t 98.675925\n",
      "inform \t\t 99.889404\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 99.228996\n",
      "inform \t\t 99.428803\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 99.585634\n",
      "inform \t\t 99.849427\n",
      "inform \t\t 99.902380\n",
      "inform \t\t 99.484903\n",
      "inform \t\t 98.764217\n",
      "inform \t\t 99.397391\n",
      "inform \t\t 93.002987\n",
      "inform \t\t 99.581331\n",
      "inform \t\t 99.690801\n",
      "inform \t\t 99.694037\n",
      "inform \t\t 99.260640\n",
      "inform \t\t 96.023971\n",
      "inform \t\t 99.322861\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 99.317753\n",
      "inform \t\t 98.342323\n",
      "inform \t\t 99.407911\n",
      "inform \t\t 99.891984\n",
      "inform \t\t 99.725014\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 98.853266\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 99.376410\n",
      "inform \t\t 99.376410\n",
      "inform \t\t 99.535924\n",
      "inform \t\t 99.376410\n",
      "inform \t\t 99.206358\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 99.846363\n",
      "inform \t\t 99.508792\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 99.225521\n",
      "inform \t\t 82.417566\n",
      "inform \t\t 92.564899\n",
      "inform \t\t 86.703306\n",
      "inform \t\t 99.443358\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 97.776103\n",
      "inform \t\t 99.695408\n",
      "inform \t\t 98.278493\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 92.177063\n",
      "inform \t\t 91.666561\n",
      "inform \t\t 99.582541\n",
      "inform \t\t 99.422175\n",
      "inform \t\t 99.509060\n",
      "inform \t\t 99.850130\n",
      "inform \t\t 99.376410\n",
      "inform \t\t 99.687093\n",
      "inform \t\t 99.474311\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 99.225521\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 95.610553\n",
      "deny \t\t 73.880702\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 89.914739\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 99.834788\n",
      "inform \t\t 99.433231\n",
      "inform \t\t 97.140759\n",
      "inform \t\t 98.959023\n",
      "inform \t\t 99.818206\n",
      "inform \t\t 92.906225\n",
      "inform \t\t 99.439442\n",
      "inform \t\t 98.058987\n",
      "inform \t\t 99.498981\n",
      "inform \t\t 99.507976\n",
      "inform \t\t 95.261353\n",
      "inform \t\t 97.381401\n",
      "inform \t\t 98.903799\n",
      "inform \t\t 91.947997\n",
      "inform \t\t 99.719882\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 98.120570\n",
      "inform \t\t 99.795938\n",
      "inform \t\t 91.606462\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 92.742938\n",
      "inform \t\t 99.597543\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 97.935450\n",
      "inform \t\t 99.865806\n",
      "inform \t\t 99.605221\n",
      "inform \t\t 99.262178\n",
      "inform \t\t 99.374121\n",
      "inform \t\t 81.594706\n",
      "inform \t\t 99.861670\n",
      "inform \t\t 99.573529\n",
      "inform \t\t 99.543399\n",
      "inform \t\t 98.120052\n",
      "inform \t\t 99.573529\n",
      "inform \t\t 99.512452\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 99.861670\n",
      "inform \t\t 99.769211\n",
      "inform \t\t 99.849427\n",
      "inform \t\t 98.783189\n",
      "inform \t\t 99.113464\n",
      "inform \t\t 99.326527\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 98.154414\n",
      "inform \t\t 99.376410\n",
      "inform \t\t 99.636590\n",
      "inform \t\t 99.622381\n",
      "inform \t\t 99.779058\n",
      "inform \t\t 95.348233\n",
      "inform \t\t 99.297380\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 99.439442\n",
      "inform \t\t 99.720263\n",
      "inform \t\t 95.864975\n",
      "inform \t\t 99.814367\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 99.532050\n",
      "inform \t\t 99.877542\n",
      "inform \t\t 99.851388\n",
      "inform \t\t 99.732047\n",
      "inform \t\t 98.487788\n",
      "inform \t\t 99.011993\n",
      "inform \t\t 97.289634\n",
      "inform \t\t 99.380809\n",
      "inform \t\t 98.973697\n",
      "inform \t\t 99.809879\n",
      "inform \t\t 99.903643\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 99.663866\n",
      "inform \t\t 94.259572\n",
      "inform \t\t 99.473912\n",
      "inform \t\t 95.968014\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 99.275756\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 99.679250\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 90.723014\n",
      "inform \t\t 99.503446\n",
      "inform \t\t 99.496764\n",
      "inform \t\t 99.065489\n",
      "inform \t\t 93.044084\n",
      "inform \t\t 67.468446\n",
      "inform \t\t 94.593966\n",
      "inform \t\t 99.372560\n",
      "inform \t\t 66.857398\n",
      "inform \t\t 83.219540\n",
      "inform \t\t 99.585634\n",
      "inform \t\t 62.150472\n",
      "inform \t\t 93.904108\n",
      "inform \t\t 99.398178\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 97.681057\n",
      "inform \t\t 99.736077\n",
      "inform \t\t 96.416157\n",
      "inform \t\t 99.598557\n",
      "inform \t\t 99.326527\n",
      "inform \t\t 96.864933\n",
      "inform \t\t 95.879632\n",
      "inform \t\t 99.269462\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 99.552429\n",
      "inform \t\t 97.771704\n",
      "inform \t\t 99.730492\n",
      "inform \t\t 81.191975\n",
      "inform \t\t 99.628794\n",
      "inform \t\t 99.443358\n",
      "inform \t\t 81.191975\n",
      "mood_ask \t\t 97.610658\n",
      "mood_ask \t\t 91.368937\n",
      "mood_ask \t\t 96.113217\n",
      "mood_ask \t\t 96.572167\n",
      "mood_ask \t\t 96.591395\n",
      "mood_ask \t\t 95.801181\n",
      "mood_ask \t\t 96.960145\n",
      "mood_ask \t\t 93.409497\n",
      "mood_ask \t\t 96.572167\n",
      "ask_whatspossible \t\t 32.451415\n",
      "mood_ask \t\t 46.398902\n",
      "ask_whatismyname \t\t 41.434446\n",
      "mood_ask \t\t 42.400506\n",
      "mood_ask \t\t 91.541111\n",
      "mood_ask \t\t 90.538621\n",
      "mood_ask \t\t 58.080924\n",
      "affirm \t\t 35.476154\n",
      "mood_ask \t\t 94.738692\n",
      "mood_ask \t\t 96.591395\n",
      "mood_ask \t\t 93.637037\n",
      "mood_ask \t\t 97.439390\n",
      "mood_ask \t\t 97.069561\n",
      "mood_ask \t\t 95.997387\n",
      "mood_ask \t\t 96.113217\n",
      "mood_ask \t\t 94.603986\n",
      "mood_ask \t\t 94.782084\n",
      "mood_ask \t\t 92.847085\n",
      "mood_ask \t\t 97.610658\n",
      "mood_ask \t\t 96.113217\n",
      "mood_ask \t\t 97.610658\n",
      "mood_ask \t\t 19.572946\n",
      "ask_whatspossible \t\t 32.451415\n",
      "ask_whatismyname \t\t 41.434446\n",
      "mood_ask \t\t 94.661111\n",
      "mood_ask \t\t 95.160407\n",
      "mood_ask \t\t 93.409497\n",
      "mood_ask \t\t 96.572167\n",
      "mood_ask \t\t 96.591395\n",
      "mood_ask \t\t 96.960145\n",
      "mood_ask \t\t 91.368937\n",
      "mood_ask \t\t 90.683544\n",
      "inform \t\t 81.191975\n",
      "affirm \t\t 43.150422\n",
      "mood_ask \t\t 39.830312\n",
      "affirm \t\t 50.372809\n",
      "mood_ask \t\t 97.666174\n",
      "mood_ask \t\t 94.026804\n",
      "mood_ask \t\t 90.683544\n",
      "mood_ask \t\t 91.933620\n",
      "mood_ask \t\t 56.144166\n",
      "mood_ask \t\t 95.415139\n",
      "mood_ask \t\t 94.702464\n",
      "mood_ask \t\t 95.160407\n",
      "mood_ask \t\t 97.610658\n",
      "mood_ask \t\t 19.572946\n",
      "ask_whatismyname \t\t 24.377367\n",
      "mood_ask \t\t 62.464893\n",
      "mood_ask \t\t 96.003753\n",
      "mood_ask \t\t 97.610658\n",
      "mood_ask \t\t 97.293788\n",
      "mood_ask \t\t 97.610658\n",
      "mood_ask \t\t 97.610658\n",
      "nice_talking_to_you \t\t 71.894079\n",
      "nice_talking_to_you \t\t 71.540016\n",
      "nice_talking_to_you \t\t 75.070310\n",
      "nice_talking_to_you \t\t 71.601927\n",
      "deny \t\t 39.120165\n",
      "feedback \t\t 89.466965\n",
      "feedback \t\t 74.127179\n",
      "feedback \t\t 86.214405\n",
      "feedback \t\t 90.754026\n",
      "feedback \t\t 98.674184\n",
      "feedback \t\t 91.805476\n",
      "feedback \t\t 95.335728\n",
      "feedback \t\t 93.902922\n",
      "feedback \t\t 97.387403\n",
      "feedback \t\t 78.372943\n",
      "feedback \t\t 97.229755\n",
      "feedback \t\t 81.568539\n",
      "feedback \t\t 93.256700\n",
      "feedback \t\t 96.565038\n",
      "feedback \t\t 87.744534\n",
      "feedback \t\t 97.918898\n",
      "feedback \t\t 90.816772\n",
      "feedback \t\t 81.411922\n",
      "feedback \t\t 93.364239\n",
      "feedback \t\t 97.818840\n",
      "feedback \t\t 99.124241\n",
      "feedback \t\t 97.301662\n",
      "feedback \t\t 98.059344\n",
      "feedback \t\t 94.439089\n",
      "feedback \t\t 95.579553\n",
      "ask_builder \t\t 80.934232\n",
      "ask_builder \t\t 95.774621\n",
      "ask_builder \t\t 97.917944\n",
      "ask_builder \t\t 83.999723\n",
      "ask_builder \t\t 53.296888\n",
      "ask_builder \t\t 94.445896\n",
      "ask_builder \t\t 94.969076\n",
      "ask_builder \t\t 94.477767\n",
      "ask_builder \t\t 91.276473\n",
      "ask_builder \t\t 96.978509\n",
      "ask_builder \t\t 91.710299\n",
      "ask_builder \t\t 97.917944\n",
      "ask_builder \t\t 88.854277\n",
      "ask_builder \t\t 97.607702\n",
      "ask_builder \t\t 93.288541\n",
      "ask_builder \t\t 97.937620\n",
      "ask_builder \t\t 98.371267\n",
      "ask_builder \t\t 98.183042\n",
      "ask_builder \t\t 98.155379\n",
      "ask_builder \t\t 89.007974\n",
      "ask_builder \t\t 98.754209\n",
      "ask_builder \t\t 96.282381\n",
      "ask_builder \t\t 90.676320\n",
      "ask_builder \t\t 95.602751\n",
      "ask_builder \t\t 94.061881\n",
      "ask_builder \t\t 85.569298\n",
      "ask_builder \t\t 88.854277\n",
      "ask_builder \t\t 96.943510\n",
      "ask_builder \t\t 97.755343\n",
      "ask_builder \t\t 91.128212\n",
      "ask_builder \t\t 97.621870\n",
      "ask_builder \t\t 98.083067\n",
      "ask_builder \t\t 57.164401\n",
      "ask_builder \t\t 90.676320\n",
      "how_is_it_going_to_help \t\t 27.395585\n",
      "ask_time \t\t 37.975276\n",
      "ask_builder \t\t 68.448436\n",
      "ask_builder \t\t 94.061881\n",
      "ask_builder \t\t 93.384427\n",
      "ask_whoisit \t\t 87.035900\n",
      "ask_builder \t\t 63.306135\n",
      "ask_builder \t\t 97.755343\n",
      "ask_builder \t\t 96.565700\n",
      "ask_builder \t\t 95.726031\n",
      "ask_builder \t\t 88.114351\n",
      "ask_builder \t\t 97.394109\n",
      "mood_ask \t\t 30.103236\n",
      "ask_howold \t\t 58.957899\n",
      "ask_howold \t\t 47.428218\n",
      "ask_time \t\t 62.645584\n",
      "ask_time \t\t 93.522030\n",
      "ask_time \t\t 52.569830\n",
      "ask_time \t\t 99.120587\n",
      "ask_time \t\t 97.345793\n",
      "ask_time \t\t 99.056011\n",
      "ask_time \t\t 98.837602\n",
      "ask_time \t\t 97.985005\n",
      "ask_time \t\t 95.008755\n",
      "ask_time \t\t 90.919650\n",
      "ask_time \t\t 99.346572\n",
      "ask_time \t\t 99.120587\n",
      "ask_time \t\t 99.272025\n",
      "ask_time \t\t 99.056011\n",
      "ask_time \t\t 98.281282\n",
      "ask_time \t\t 98.031104\n",
      "ask_time \t\t 99.199319\n",
      "ask_time \t\t 98.604035\n",
      "ask_time \t\t 98.837602\n",
      "ask_time \t\t 99.081635\n",
      "ask_time \t\t 36.115786\n",
      "ask_time \t\t 97.743124\n",
      "ask_time \t\t 99.389344\n",
      "ask_time \t\t 99.355620\n",
      "ask_time \t\t 99.120587\n",
      "ask_time \t\t 99.120587\n",
      "ask_time \t\t 99.167436\n",
      "ask_time \t\t 90.876144\n",
      "ask_time \t\t 98.765820\n",
      "ask_time \t\t 99.399096\n",
      "ask_time \t\t 99.167436\n",
      "ask_time \t\t 99.002773\n",
      "ask_time \t\t 90.876144\n",
      "ask_time \t\t 99.120587\n",
      "ask_time \t\t 99.056011\n",
      "ask_time \t\t 52.569830\n",
      "ask_time \t\t 99.389344\n",
      "ask_time \t\t 98.281282\n",
      "ask_time \t\t 93.510479\n",
      "ask_time \t\t 99.081635\n",
      "ask_time \t\t 97.834384\n",
      "ask_time \t\t 92.117256\n",
      "ask_time \t\t 97.345793\n",
      "ask_time \t\t 99.404413\n",
      "ask_time \t\t 98.913223\n",
      "ask_time \t\t 99.120587\n",
      "ask_time \t\t 95.994037\n",
      "good_day \t\t 48.034319\n",
      "ask_weather \t\t 97.479928\n",
      "ask_weather \t\t 86.357981\n",
      "ask_weather \t\t 99.066037\n",
      "ask_weather \t\t 92.373782\n",
      "ask_weather \t\t 91.123909\n",
      "ask_weather \t\t 97.119462\n",
      "ask_weather \t\t 95.750791\n",
      "ask_weather \t\t 99.200130\n",
      "ask_weather \t\t 85.786122\n",
      "ask_weather \t\t 94.175023\n",
      "ask_weather \t\t 87.359375\n",
      "ask_weather \t\t 97.744566\n",
      "ask_weather \t\t 87.282020\n",
      "ask_weather \t\t 87.098891\n",
      "mood_ask \t\t 33.995891\n",
      "ask_weather \t\t 78.940302\n",
      "ask_weather \t\t 98.876941\n",
      "ask_weather \t\t 92.125458\n",
      "ask_weather \t\t 97.982717\n",
      "ask_weather \t\t 98.653215\n",
      "ask_weather \t\t 99.225062\n",
      "ask_weather \t\t 83.318359\n",
      "ask_weather \t\t 96.050072\n",
      "ask_weather \t\t 98.871338\n",
      "ask_weather \t\t 98.048180\n",
      "ask_weather \t\t 92.125458\n",
      "ask_weather \t\t 98.048180\n",
      "ask_weather \t\t 92.125458\n",
      "ask_weather \t\t 98.871338\n",
      "ask_weather \t\t 92.125458\n",
      "ask_weather \t\t 86.461854\n",
      "ask_weather \t\t 99.094075\n",
      "ask_weather \t\t 76.581305\n",
      "ask_weather \t\t 99.226815\n",
      "ask_weather \t\t 74.744320\n",
      "ask_weather \t\t 91.994238\n",
      "ask_weather \t\t 99.221909\n",
      "ask_weather \t\t 92.125458\n",
      "ask_weather \t\t 99.069852\n",
      "ask_whatismyname \t\t 88.560516\n",
      "ask_whatismyname \t\t 93.973905\n",
      "ask_whatismyname \t\t 88.902396\n",
      "ask_whatismyname \t\t 92.108786\n",
      "ask_whatismyname \t\t 86.762834\n",
      "ask_whatismyname \t\t 93.067062\n",
      "ask_whatismyname \t\t 86.762834\n",
      "ask_whatismyname \t\t 95.027828\n",
      "ask_whatismyname \t\t 93.040121\n",
      "ask_whatismyname \t\t 88.871646\n",
      "ask_whatismyname \t\t 92.419434\n",
      "ask_whatismyname \t\t 95.347726\n",
      "ask_whatismyname \t\t 98.031908\n",
      "ask_whatismyname \t\t 97.594190\n",
      "ask_whatismyname \t\t 96.029371\n",
      "ask_whatismyname \t\t 86.762834\n",
      "ask_whatismyname \t\t 95.795995\n",
      "ask_whatismyname \t\t 86.762834\n",
      "ask_whatismyname \t\t 95.789146\n",
      "ask_whatismyname \t\t 92.108786\n",
      "ask_whatismyname \t\t 96.481973\n",
      "ask_whatismyname \t\t 95.795995\n",
      "ask_whatismyname \t\t 86.510372\n",
      "ask_whatismyname \t\t 96.092135\n",
      "ask_whatismyname \t\t 97.629762\n",
      "ask_whatismyname \t\t 95.305270\n",
      "ask_whatismyname \t\t 92.732799\n",
      "ask_whatismyname \t\t 91.279101\n",
      "ask_whatismyname \t\t 94.315231\n",
      "ask_whatismyname \t\t 95.179749\n",
      "ask_whatismyname \t\t 93.251818\n",
      "ask_whatismyname \t\t 88.478464\n",
      "ask_whatismyname \t\t 96.456218\n",
      "ask_whatismyname \t\t 94.580793\n",
      "ask_whatismyname \t\t 92.786682\n",
      "ask_whatismyname \t\t 88.871646\n",
      "ask_whatismyname \t\t 94.571358\n",
      "ask_whatismyname \t\t 94.081759\n",
      "ask_whatspossible \t\t 95.674914\n",
      "ask_whatspossible \t\t 91.304117\n",
      "ask_whatspossible \t\t 44.288355\n",
      "ask_whatspossible \t\t 74.953097\n",
      "ask_whatspossible \t\t 80.729622\n",
      "ask_whatspossible \t\t 74.953097\n",
      "ask_whatspossible \t\t 89.863920\n",
      "ask_whatspossible \t\t 70.512038\n",
      "how_is_it_going_to_help \t\t 68.386656\n",
      "ask_whatspossible \t\t 95.595026\n",
      "ask_whatspossible \t\t 75.063115\n",
      "ask_whatspossible \t\t 40.568027\n",
      "ask_whatspossible \t\t 96.343994\n",
      "ask_whatspossible \t\t 96.371621\n",
      "ask_whatspossible \t\t 90.524209\n",
      "ask_whatspossible \t\t 61.400449\n",
      "ask_whatspossible \t\t 91.010976\n",
      "ask_whatspossible \t\t 71.214688\n",
      "ask_whatspossible \t\t 29.793718\n",
      "ask_whatspossible \t\t 41.952243\n",
      "ask_whatspossible \t\t 80.729622\n",
      "ask_whatspossible \t\t 95.167577\n",
      "ask_whatspossible \t\t 58.598566\n",
      "ask_whatspossible \t\t 90.269023\n",
      "ask_whatspossible \t\t 95.674914\n",
      "ask_whatspossible \t\t 88.589400\n",
      "ask_whatspossible \t\t 95.682418\n",
      "ask_whatspossible \t\t 90.524209\n",
      "ask_time \t\t 30.617243\n",
      "ask_whatspossible \t\t 72.398090\n",
      "deny \t\t 34.079343\n",
      "deny \t\t 49.878851\n",
      "ask_whatspossible \t\t 78.672576\n",
      "ask_whatspossible \t\t 80.729622\n",
      "ask_whatismyname \t\t 58.483315\n",
      "ask_whatspossible \t\t 89.883983\n",
      "ask_whatspossible \t\t 70.866215\n",
      "ask_whoisit \t\t 94.524884\n",
      "ask_whoisit \t\t 68.168604\n",
      "ask_whoisit \t\t 68.864751\n",
      "ask_whoisit \t\t 97.678328\n",
      "ask_whoisit \t\t 94.071418\n",
      "ask_whoisit \t\t 95.910645\n",
      "ask_whoisit \t\t 94.945598\n",
      "ask_whoisit \t\t 75.046372\n",
      "ask_whoisit \t\t 94.524884\n",
      "ask_whoisit \t\t 94.524884\n",
      "ask_whatismyname \t\t 86.762834\n",
      "ask_whoisit \t\t 59.164298\n",
      "ask_whoisit \t\t 78.509569\n",
      "ask_whoisit \t\t 97.375429\n",
      "ask_whoisit \t\t 68.864751\n",
      "ask_whoisit \t\t 68.864751\n",
      "ask_whoisit \t\t 84.109861\n",
      "ask_whatismyname \t\t 86.762834\n",
      "ask_whoisit \t\t 94.524884\n",
      "ask_whoisit \t\t 90.151203\n",
      "ask_whatismyname \t\t 65.571362\n",
      "ask_whatismyname \t\t 51.151317\n",
      "ask_whatismyname \t\t 45.590261\n",
      "ask_whoisit \t\t 81.465423\n",
      "ask_whoisit \t\t 81.465423\n",
      "ask_whoisit \t\t 84.433901\n",
      "ask_whoisit \t\t 88.685375\n",
      "dialogue_tools \t\t 85.822451\n",
      "dialogue_tools \t\t 44.141871\n",
      "dialogue_tools \t\t 76.025867\n",
      "dialogue_tools \t\t 74.611032\n",
      "explain_generative_chatbot \t\t 11.302287\n",
      "explain_custom_bot \t\t 21.322571\n",
      "explain_custom_bot \t\t 19.057170\n",
      "explain_deep_learning \t\t 64.633191\n",
      "explain_deep_learning \t\t 64.633191\n",
      "explain_deep_learning \t\t 50.922424\n",
      "explain_deep_learning \t\t 76.717764\n",
      "explain_deep_learning \t\t 77.113479\n",
      "explain_feedback_bot \t\t 30.924070\n",
      "explain_retention_bot \t\t 21.932217\n",
      "explain_dialogue_machines \t\t 30.305013\n",
      "explain_retention_bot \t\t 22.208418\n",
      "explain_support_bot \t\t 23.373850\n",
      "explain_entities \t\t 67.878360\n",
      "explain_entities \t\t 68.488896\n",
      "explain_entities \t\t 68.488896\n",
      "explain_entities \t\t 68.250847\n",
      "explain_entities \t\t 71.056074\n",
      "explain_feedback_bot \t\t 43.362927\n",
      "explain_feedback_bot \t\t 33.484799\n",
      "explain_feedback_bot \t\t 24.989764\n",
      "explain_feedback_bot \t\t 34.882176\n",
      "explain_feedback_bot \t\t 40.820011\n",
      "explain_generative_chatbot \t\t 45.740324\n",
      "explain_generative_chatbot \t\t 45.268467\n",
      "explain_generative_chatbot \t\t 34.059009\n",
      "explain_generative_chatbot \t\t 42.997670\n",
      "explain_generative_chatbot \t\t 41.904211\n",
      "explain_intents \t\t 55.773479\n",
      "explain_intents \t\t 65.830719\n",
      "explain_intents \t\t 65.830719\n",
      "explain_intents \t\t 76.591307\n",
      "explain_intents \t\t 74.980199\n",
      "explain_keras \t\t 44.333079\n",
      "explain_keras \t\t 44.333079\n",
      "mood_ask \t\t 19.396076\n",
      "explain_keras \t\t 63.626224\n",
      "explain_keras \t\t 39.041510\n",
      "explain_keras \t\t 40.149298\n",
      "explain_lead_bot \t\t 26.849970\n",
      "explain_lead_bot \t\t 25.575408\n",
      "explain_lead_bot \t\t 42.126679\n",
      "explain_lead_bot \t\t 46.450981\n",
      "explain_lead_bot \t\t 47.007880\n",
      "explain_nlp \t\t 22.855259\n",
      "explain_nlp \t\t 22.855259\n",
      "explain_nlp \t\t 38.723016\n",
      "explain_nlp \t\t 28.259403\n",
      "explain_nlp \t\t 49.509048\n",
      "explain_nlp \t\t 24.453531\n",
      "explain_nlp \t\t 38.828012\n",
      "mood_ask \t\t 19.396076\n",
      "mood_ask \t\t 19.396076\n",
      "ask_whatismyname \t\t 23.815784\n",
      "explain_nltk \t\t 28.701851\n",
      "explain_tensorflow \t\t 30.846441\n",
      "explain_nlu \t\t 18.586710\n",
      "explain_nlu \t\t 18.586710\n",
      "explain_nlu \t\t 32.276469\n",
      "explain_nlp \t\t 23.996098\n",
      "explain_nlu \t\t 36.087105\n",
      "explain_nlu \t\t 26.221749\n",
      "explain_nlu \t\t 51.903760\n",
      "explain_retention_bot \t\t 37.877050\n",
      "explain_retention_bot \t\t 37.543812\n",
      "explain_support_bot \t\t 28.612277\n",
      "explain_retention_bot \t\t 34.394088\n",
      "explain_retention_bot \t\t 33.275515\n",
      "explain_retrieval_chatbot \t\t 41.331145\n",
      "explain_retrieval_chatbot \t\t 36.261576\n",
      "explain_generative_chatbot \t\t 18.140621\n",
      "explain_retrieval_chatbot \t\t 42.418870\n",
      "explain_retrieval_chatbot \t\t 41.725928\n",
      "explain_spacy \t\t 29.336131\n",
      "explain_spacy \t\t 29.336131\n",
      "explain_spacy \t\t 28.081030\n",
      "explain_spacy \t\t 27.608329\n",
      "explain_spacy \t\t 27.150506\n",
      "explain_support_bot \t\t 29.057232\n",
      "explain_support_bot \t\t 31.626248\n",
      "explain_support_bot \t\t 29.654899\n",
      "explain_support_bot \t\t 30.235744\n",
      "explain_support_bot \t\t 32.715547\n",
      "mood_ask \t\t 19.396076\n",
      "mood_ask \t\t 19.396076\n",
      "explain_tensorflow \t\t 25.144172\n",
      "explain_nltk \t\t 28.701851\n",
      "explain_tensorflow \t\t 30.846441\n",
      "explain_turing_test \t\t 75.469059\n",
      "explain_turing_test \t\t 41.180316\n",
      "explain_turing_test \t\t 63.776135\n",
      "explain_turing_test \t\t 86.971915\n",
      "explain_turing_test \t\t 86.159140\n",
      "ask_time \t\t 31.418341\n",
      "get_started \t\t 87.896794\n",
      "get_started \t\t 85.162520\n",
      "get_started \t\t 61.614388\n",
      "get_started \t\t 60.544908\n",
      "get_started \t\t 90.334463\n",
      "get_started \t\t 96.335489\n",
      "get_started \t\t 87.783659\n",
      "get_started \t\t 60.958529\n",
      "get_started \t\t 79.844642\n",
      "get_started \t\t 94.919527\n",
      "get_started \t\t 93.714660\n",
      "get_started \t\t 84.370589\n",
      "get_started \t\t 93.684965\n",
      "get_started \t\t 87.188548\n",
      "get_started \t\t 80.749410\n",
      "get_started \t\t 83.895779\n",
      "get_started \t\t 94.255304\n",
      "get_started \t\t 56.758094\n",
      "get_started \t\t 67.656481\n",
      "get_started \t\t 68.130988\n",
      "get_started \t\t 51.443994\n",
      "get_started \t\t 94.449311\n",
      "get_started \t\t 59.522980\n",
      "get_started \t\t 94.989640\n",
      "get_started \t\t 59.117192\n",
      "ask_whatismyname \t\t 24.821454\n",
      "how_is_it_going_to_help \t\t 68.386656\n",
      "how_is_it_going_to_help \t\t 61.115265\n",
      "get_started \t\t 82.880074\n",
      "how_is_it_going_to_help \t\t 61.115265\n",
      "how_is_it_going_to_help \t\t 46.375868\n",
      "ask_whatspossible \t\t 41.307628\n",
      "how_is_it_going_to_help \t\t 46.375868\n",
      "subscribe_newsletter \t\t 99.399132\n",
      "subscribe_newsletter \t\t 98.578608\n",
      "subscribe_newsletter \t\t 98.557186\n",
      "subscribe_newsletter \t\t 98.368949\n",
      "subscribe_newsletter \t\t 99.462730\n",
      "subscribe_newsletter \t\t 99.095345\n",
      "subscribe_newsletter \t\t 97.402769\n",
      "subscribe_newsletter \t\t 99.228632\n",
      "subscribe_newsletter \t\t 99.553335\n",
      "subscribe_newsletter \t\t 99.650204\n",
      "subscribe_newsletter \t\t 99.705291\n",
      "subscribe_newsletter \t\t 99.822253\n",
      "subscribe_newsletter \t\t 99.542350\n",
      "subscribe_newsletter \t\t 97.329783\n",
      "subscribe_newsletter \t\t 99.592650\n",
      "subscribe_newsletter \t\t 97.793454\n",
      "subscribe_newsletter \t\t 90.718341\n",
      "subscribe_newsletter \t\t 98.557186\n",
      "subscribe_newsletter \t\t 98.487860\n",
      "subscribe_newsletter \t\t 97.616106\n",
      "subscribe_newsletter \t\t 97.616106\n",
      "subscribe_newsletter \t\t 97.329783\n",
      "subscribe_newsletter \t\t 96.742320\n",
      "subscribe_newsletter \t\t 97.549599\n",
      "subscribe_newsletter \t\t 99.413800\n",
      "subscribe_newsletter \t\t 94.194478\n",
      "subscribe_newsletter \t\t 99.114615\n",
      "subscribe_newsletter \t\t 97.329783\n",
      "subscribe_newsletter \t\t 98.389870\n",
      "subscribe_newsletter \t\t 99.770689\n",
      "subscribe_newsletter \t\t 99.745005\n",
      "subscribe_newsletter \t\t 99.035454\n",
      "subscribe_newsletter \t\t 99.837649\n",
      "subscribe_newsletter \t\t 99.824703\n",
      "subscribe_newsletter \t\t 98.710316\n",
      "subscribe_newsletter \t\t 99.768281\n",
      "subscribe_newsletter \t\t 98.804408\n",
      "subscribe_newsletter \t\t 99.895573\n",
      "subscribe_newsletter \t\t 99.873918\n",
      "subscribe_newsletter \t\t 90.718341\n",
      "subscribe_newsletter \t\t 99.787152\n",
      "subscribe_newsletter \t\t 99.416417\n",
      "subscribe_newsletter \t\t 75.984210\n",
      "subscribe_newsletter \t\t 86.307365\n",
      "subscribe_newsletter \t\t 98.388308\n",
      "subscribe_newsletter \t\t 99.705291\n",
      "subscribe_newsletter \t\t 85.808253\n",
      "subscribe_newsletter \t\t 98.021531\n",
      "subscribe_newsletter \t\t 86.914235\n",
      "subscribe_newsletter \t\t 69.285059\n",
      "subscribe_newsletter \t\t 99.749714\n",
      "subscribe_newsletter \t\t 98.272508\n",
      "subscribe_newsletter \t\t 98.771721\n",
      "subscribe_newsletter \t\t 99.702054\n",
      "subscribe_newsletter \t\t 99.752921\n",
      "subscribe_newsletter \t\t 96.466833\n",
      "subscribe_newsletter \t\t 98.283285\n",
      "subscribe_newsletter \t\t 97.645640\n",
      "subscribe_newsletter \t\t 99.553335\n",
      "subscribe_newsletter \t\t 99.738711\n",
      "subscribe_newsletter \t\t 99.827647\n",
      "subscribe_newsletter \t\t 99.259478\n",
      "subscribe_newsletter \t\t 99.769092\n",
      "subscribe_newsletter \t\t 99.590981\n",
      "subscribe_newsletter \t\t 99.875927\n",
      "subscribe_newsletter \t\t 99.869257\n",
      "subscribe_newsletter \t\t 99.616903\n",
      "subscribe_newsletter \t\t 98.886150\n",
      "subscribe_newsletter \t\t 99.860853\n",
      "subscribe_newsletter \t\t 97.579986\n",
      "subscribe_newsletter \t\t 99.442446\n",
      "subscribe_newsletter \t\t 99.416417\n",
      "subscribe_newsletter \t\t 98.169827\n",
      "subscribe_newsletter \t\t 99.650061\n",
      "subscribe_newsletter \t\t 99.611330\n",
      "subscribe_newsletter \t\t 83.751023\n",
      "subscribe_newsletter \t\t 99.673009\n",
      "subscribe_newsletter \t\t 99.837863\n",
      "subscribe_newsletter \t\t 99.823296\n",
      "subscribe_newsletter \t\t 99.753499\n",
      "subscribe_newsletter \t\t 99.808818\n",
      "subscribe_newsletter \t\t 99.827647\n",
      "subscribe_newsletter \t\t 97.793454\n",
      "subscribe_newsletter \t\t 99.514633\n",
      "subscribe_newsletter \t\t 99.259478\n",
      "subscribe_newsletter \t\t 98.473257\n",
      "subscribe_newsletter \t\t 99.702054\n",
      "subscribe_newsletter \t\t 97.908670\n",
      "subscribe_newsletter \t\t 99.863070\n",
      "subscribe_newsletter \t\t 99.837649\n",
      "subscribe_newsletter \t\t 99.771595\n",
      "subscribe_newsletter \t\t 90.718341\n",
      "subscribe_newsletter \t\t 96.742320\n",
      "subscribe_newsletter \t\t 99.748367\n",
      "subscribe_newsletter \t\t 97.793454\n",
      "subscribe_newsletter \t\t 99.784529\n",
      "subscribe_newsletter \t\t 99.413800\n",
      "types_of_chatbots \t\t 49.189568\n",
      "types_of_chatbots \t\t 43.032494\n",
      "types_of_chatbots \t\t 77.519983\n",
      "types_of_chatbots \t\t 76.682216\n",
      "visit_website \t\t 56.998152\n",
      "visit_website \t\t 59.700805\n",
      "visit_website \t\t 47.410265\n",
      "visit_website \t\t 33.948362\n",
      "ask_whatismyname \t\t 61.111635\n",
      "when_will_you_beat_turing_test \t\t 80.211788\n",
      "when_will_you_beat_turing_test \t\t 69.696259\n",
      "why_did_you_make_a_bot \t\t 28.908849\n",
      "why_did_you_make_a_bot \t\t 53.055835\n",
      "why_machine_learning \t\t 80.485928\n",
      "why_machine_learning \t\t 79.801780\n",
      "why_machine_learning \t\t 64.732909\n",
      "why_machine_learning \t\t 77.933317\n",
      "explain_support_bot \t\t 21.450703\n",
      "why_make_a_bot \t\t 25.130570\n",
      "\n",
      " 1433 \n",
      "\n",
      "CPU times: user 4min 33s, sys: 3min 16s, total: 7min 50s\n",
      "Wall time: 3min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# max_index = int(len(x_train_final_input)/10 + 1)\n",
    "max_index = len(x_train_processed)\n",
    "\n",
    "for i in range(max_index):\n",
    "    predict_intent(x_train_processed[i])\n",
    "\n",
    "print(\"\\n\", max_index, \"\\n\")\n",
    "# TODO: CREATE A CONFUSION MATRIX TO QUICKLY CHECK THE FALSE POSITIVES AND FALSE POSITIVES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_intents(texts):\n",
    "    final_texts = []\n",
    "    for i in range(len(texts)):\n",
    "        text = preprocess_text(texts[i])\n",
    "        tokens = prepare_text_for_prediction(text)\n",
    "        final_texts.append(tokens)\n",
    "\n",
    "    pred_intents = []\n",
    "    for i in range(len(final_texts)):\n",
    "        predict_class = model.predict(final_texts[i])\n",
    "        \n",
    "        intent = encoder.inverse_transform(predict_class)\n",
    "        index_number = np.argmax(predict_class)\n",
    "        intent_value = predict_class[0, index_number]\n",
    "        pred_intents.append((intent[0], intent_value))\n",
    "        \n",
    "    return pred_intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('greet', 0.9671258), ('greet', 0.9567531), ('greet', 0.96930164), ('greet', 0.9567531), ('greet', 0.6586009), ('greet', 0.6238077), ('greet', 0.94389), ('greet', 0.9671258), ('bye', 0.52806836), ('greet', 0.9766517), ('greet', 0.88771784), ('inform', 0.81191975), ('greet', 0.70120573), ('inform', 0.81191975), ('greet', 0.9457526), ('greet', 0.971114), ('greet', 0.89354163), ('greet', 0.92160153), ('greet', 0.9567531), ('inform', 0.81191975)]\n",
      "\n",
      " 1433 \n",
      "\n",
      "CPU times: user 3.95 s, sys: 2.81 s, total: 6.76 s\n",
      "Wall time: 2.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# max_index = int(len(x_train_final_input)/10 + 1)\n",
    "max_index = len(x_train_processed)\n",
    "\n",
    "pred = predict_intents(x_train_processed[:20])\n",
    "\n",
    "print(pred)\n",
    "print(\"\\n\", max_index, \"\\n\")\n",
    "# TODO: CREATE A CONFUSION MATRIX TO QUICKLY CHECK THE FALSE POSITIVES AND FALSE POSITIVES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_pred = []\n",
    "for i in range(len(X_test)):\n",
    "    pred_intent, pred_intent_value = predict_intent(X_test[i])\n",
    "    label_tokens = np.array([y_test[i]])\n",
    "    true_intent = encoder.inverse_transform(label_tokens)\n",
    "    if pred_intent != true_intent:\n",
    "        incorrect_pred.append([tokens_to_string(X_test[i]), pred_intent[0], true_intent[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence                                          || Prediction      || True Intent\n"
     ]
    }
   ],
   "source": [
    "print(\"{:50}|| {:15} || {}\".format(\"Sentence\", \"Prediction\", \"True Intent\"))\n",
    "for i in range(len(incorrect_pred)):\n",
    "    print(\"{:50}: {:15}\\t{}\".format(incorrect_pred[i][0], incorrect_pred[i][1], incorrect_pred[i][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((len(X_test) - len(incorrect_pred)) / len(X_test) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_embedding = model.get_layer('embedding_layer')\n",
    "weights_embedding = layer_embedding.get_weights()[0]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "inverse_map = dict(zip(word_index.values(), word_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def print_sorted_words(word, metric='cosine'):\n",
    "    \"\"\"\n",
    "    Print the words in the vocabulary sorted according to their\n",
    "    embedding-distance to the given word.\n",
    "    Different metrics can be used, e.g. 'cosine' or 'euclidean'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the token (i.e. integer ID) for the given word.\n",
    "    token = tokenizer.word_index[word]\n",
    "\n",
    "    # Get the embedding for the given word. Note that the\n",
    "    # embedding-weight-matrix is indexed by the word-tokens\n",
    "    # which are integer IDs.\n",
    "    embedding = weights_embedding[token]\n",
    "\n",
    "    # Calculate the distance between the embeddings for\n",
    "    # this word and all other words in the vocabulary.\n",
    "    distances = cdist(weights_embedding, [embedding], metric=metric).T[0]\n",
    "\n",
    "    # Get an index sorted according to the embedding-distances.\n",
    "    # These are the tokens (integer IDs) for words in the vocabulary.\n",
    "    sorted_index = np.argsort(distances)\n",
    "\n",
    "    # Sort the embedding-distances.\n",
    "    sorted_distances = distances[sorted_index]\n",
    "\n",
    "    # Sort all the words in the vocabulary according to their\n",
    "    # embedding-distance. This is a bit excessive because we\n",
    "    # will only print the top and bottom words.\n",
    "    sorted_words = [inverse_map[token] for token in sorted_index\n",
    "                    if token != 0 and token in inverse_map]\n",
    "\n",
    "    # Helper-function for printing words and embedding-distances.\n",
    "    def _print_words(words, distances):\n",
    "        for word, distance in zip(words, distances):\n",
    "            print(\"{0:.3f} - {1}\".format(distance, word))\n",
    "\n",
    "    # Number of words to print from the top and bottom of the list.\n",
    "    k = 10\n",
    "\n",
    "    print(\"Distance from '{0}':\".format(word))\n",
    "\n",
    "    # Print the words with smallest embedding-distance.\n",
    "    _print_words(sorted_words[0:k], sorted_distances[0:k])\n",
    "\n",
    "#     print(\"...\")\n",
    "\n",
    "    # Print the words with highest embedding-distance. \n",
    "#     _print_words(sorted_words[-k:], sorted_distances[-k:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance from 'machine':\n",
      "0.000 - machine\n",
      "0.387 - tool\n",
      "0.428 - system\n",
      "0.439 - type\n",
      "0.457 - use\n",
      "0.471 - factory\n",
      "0.481 - model\n",
      "0.490 - design\n",
      "0.492 - software\n",
      "0.496 - robot\n"
     ]
    }
   ],
   "source": [
    "print_sorted_words('machine', metric='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_FILE = \"glove.6B.100d.txt\"\n",
    "GLOVE_VEC_SIZE = 100\n",
    "VOCAB_SIZE = 20000\n",
    "EMBEDDING_DIM = GLOVE_VEC_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_words = {}\n",
    "\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_words[word] = embedding_vector\n",
    "    if index > VOCAB_SIZE - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_words = [\n",
    "    \"knowledge\", \"information\" \"organisation\", \"chatbot\", \"bot\", \"signup, newsletter\", \"subscriber\", \n",
    "    \"nlu\", \"nlp\", \"machine\", \"learning\", \"deep\", \"logic\", \"subscription\", \"development\", \"built\",\n",
    "    \"understanding\", \"nltk\", \"spacy\", \"natural\", \"tensorflow\", \"keras\", \"turing\", \"computer\",\n",
    "    \"dialogue\", \"show\", \"possible\", \"explain\", \"intent\", \"entity\", \"entities\", \"recognize\", \"look\",\n",
    "    \"weather\", \"sky\", \"sunny\", \"programmer\", \"classification\", \"listen\", \"artificial\", \"intelligence\",\n",
    "    \"ai\", \"ml\", \"test\", \"website\", \"setup\", \"start\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_and_labels_emb(embedding_words, word_list):\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    for w, t in embedding_words.items():\n",
    "        if w in word_list:\n",
    "            tokens.append(t)\n",
    "            labels.append(w)\n",
    "    return labels, tokens\n",
    "\n",
    "def tsne_fit_and_plot(tokens, labels):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=42)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anurags/miniforge3/envs/tensorflow_exp/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/anurags/miniforge3/envs/tensorflow_exp/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:982: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA60AAAOFCAYAAACBWgSEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAACRpElEQVR4nOzde1xVVf7/8fcWCVEMNTXFmsB+3uFwO6hImEiKpXm3Mq3Myu6WMzHmpEWOTTPJpNnMaDpjZllZXjMbMxVHvGVAiDfMS5SJqWiSICCX/fsDPV9JVFDgbOD1fDx8cM7a66z92efxmIY3a+21DdM0BQAAAACAFdVxdgEAAAAAAFwKoRUAAAAAYFmEVgAAAACAZRFaAQAAAACWRWgFAAAAAFgWoRUAAAAAYFl1nV1AWTVt2tT09vZ2dhkAAAAAgAqWmJiYYZpms9KOVZvQ6u3trYSEBGeXAQAAAACoYIZh/HCpYywPBgAAAABYFqEVAAAAAGBZhFYAAAAAgGURWgEAAAAAlkVoBQAAAABYFqEVAAAAAGBZhFYAAAAAgGURWgEAAAAAlkVoBQAAAABYFqEVAAAAAGBZhFYAAAAAgGURWgEAAAAAlkVoBQAAAABYFqEVAAAAAGBZhFYAAAAAgGURWgEAAAAAlkVoBQAAAABYFqEVAAAAAGBZhFYAAAAAgGURWgEAAAAAlkVoBQAAAABYFqEVAAAAAGBZhFYAAAAAgGVdc2g1DONmwzDiDMPYbRjGLsMwnjvX3sQwjK8Mw9h37mfjc+2GYRgzDMPYbxhGimEYQddaAwAAAACgZqqImdYCSX8wTbOjpK6SnjYMo6OkFyWtNU2zjaS1595L0p2S2pz7N0bSzAqoAQAAAABQA11zaDVN84hpmknnXp+WtEdSK0kDJL13rtt7kgaeez1A0nyz2FZJjQzDaHmtdQAAAAAAap4KvafVMAxvSYGSvpZ0o2maR84d+lnSjedet5J06IKP/XSuDQBgQd26dbtin+nTp+vMmTNXfY7169dr8+bNV/15AABQc1VYaDUMw0PSYknPm6b564XHTNM0JZlXMeYYwzASDMNIOH78eAVVCgAoj7KESUIrAACoLBUSWg3DcFVxYF1gmuaSc81Hzy/7Pffz2Ln2w5JuvuDjN51ru4hpmrNN07Sbpmlv1qxZRZQKACgnDw8PScXBskePHho6dKjat2+vESNGyDRNzZgxQ+np6YqIiFBERIQkafXq1QoNDVVQUJCGDRumrKwsSZK3t7deeeUVBQUFyc/PT6mpqUpLS9OsWbM0bdo0BQQEKD4+3mnXCgAArKcidg82JP1H0h7TNN+84NBnkh469/ohScsvaH/w3C7CXSVlXrCMGABgYd9++62mT5+u3bt36+DBg9q0aZPGjh0rLy8vxcXFKS4uThkZGZoyZYrWrFmjpKQk2e12vfnm//3fQ9OmTZWUlKQnn3xSsbGx8vb21hNPPKFx48YpOTlZ4eHhTrxCAABgNXUrYIwwSQ9I2mEYRvK5tj9J+qukTwzDeETSD5LuOXfsC0l3Sdov6YykhyugBgBAFejcubNuuukmSVJAQIDS0tJ02223leizdetW7d69W2FhYZKks2fPKjQ01HF88ODBkqTg4GAtWbJEAAAAl3PNodU0zY2SjEscjiylvynp6Ws9LwCg6rm5uTleu7i4qKCg4KI+pmmqV69e+uijjy47xqU+DwAAcKEK3T0YAFA7NWzYUKdPn5Ykde3aVZs2bdL+/fslSdnZ2fruu+/K/HkAAIALEVoBANdszJgx6tOnjyIiItSsWTPNmzdPw4cPl81mU2hoqFJTUy/7+bvvvltLly5lIyYAAHARo3i1rvXZ7XYzISHB2WUAAAAAACqYYRiJpmnaSztWERsxAQBQbtnfHtOvX6ap8FSeXBq56foobzUIbO7ssgAAgMUQWgEAVS7722M6tWSfzPwiSVLhqTydWrJPkgiuAACgBO5pBQBUuV+/THME1vPM/CL9+mWacwoCAACWRWgFAFS5wlN55WoHAAC1F6EVAFDlXBq5lasdAADUXoRWAECVuz7KW4Zryf8LMlzr6Poob+cUBAAALIuNmAAAVe78ZkvsHgwAAK6E0AoAcIoGgc0JqQAA4IpYHgwAAAAAsCxCKwAAAADAsgitAAAAAADLIrQCAAAAACyL0AoAAAAAsCxCKwAAAADAsgitAAAAAADLIrQCAAAAACyrrrMLAAAAtUNMTIw8PDz066+/qnv37rrjjjsu2XfUqFHq16+fhg4dWoUVAgCsiNAKAACq1OTJk51dAgCgGmF5MAAAqDSvvfaa2rZtq9tuu0179+6VVDyLumjRIknFATYkJES+vr4aM2aMTNO8aIy1a9cqMDBQfn5+Gj16tPLy8iRJX3zxhdq3b6/g4GCNHTtW/fr1k1Q8oxsbG+v4vK+vr9LS0iRJH3zwgTp37qyAgAA9/vjjKiwsrMzLBwBUAEIrAACoFImJifr444+VnJysL774Qt98881FfZ555hl988032rlzp3JycvT555+XOJ6bm6tRo0Zp4cKF2rFjhwoKCjRz5kzl5ubq8ccf13//+18lJibq+PHjV6xnz549WrhwoTZt2qTk5GS5uLhowYIFFXa9AIDKQWgFAACVIj4+XoMGDVL9+vV1/fXXq3///hf1iYuLU5cuXeTn56d169Zp165dJY7v3btXPj4+atu2rSTpoYce0oYNG5SamqrWrVvLx8dHkjR8+PAr1rN27VolJiYqJCREAQEBWrt2rQ4ePFgBVwoAqEzc0woAAJwiNzdXTz31lBISEnTzzTcrJiZGubm51zxu3bp1VVRUVOI8kmSaph566CG9/vrr13wOAEDVYaYVAABUiu7du2vZsmXKycnR6dOntWLFihLHz4fJpk2bKisry3Gf64XatWuntLQ07d+/X5L0/vvv6/bbb1e7du108OBBx72qCxcudHzG29tbSUlJkqSkpCR9//33kqTIyEgtWrRIx44dkySdPHlSP/zwQ8VeNACgwjHTCgAAKkVQUJDuvfde+fv7q3nz5goJCSlxvFGjRnrsscfk6+urFi1aXHRckurVq6d3331Xw4YNU0FBgUJCQvTEE0/Izc1N//rXv9SnTx81aNCgxGeHDBmi+fPnq1OnTurSpYtjaXHHjh01ZcoU9e7dW0VFRXJ1ddU///lP3XLLLZX7RQAArolR2i59VmS3282EhARnlwEAACwiKytLHh4eMk1TTz/9tNq0aaNx48Y5uywAwFUwDCPRNE17aceYaQUAANXSnDlz9N577+ns2bMKDAzU448/ftn+3339s7YsP6Csk3nyaOKm0AG3qm2XFlVULQDgajHTCgAAarzvvv5ZcQtSVXD2/zZoqntdHUWMaE9wBQALuNxMKxsxAQCAGm/L8gMlAqskFZwt0pblB5xUEQCgrAitAACgxss6mVeudgCAdRBaAQBAjefRxK1c7QAA6yC0AgCAGi90wK2qe13JX3vqXldHoQNudVJFAICyYvdgAABQ453fbIndgwGg+iG0AgCAWqFtlxaEVACohlgeDAAAAACwLEIrAAAAAMCyCK0AAAAAAMsitAIAAAAALIvQCgAAAACwLEIrAAAAAMCyCK0AAAAAAMsitAIAAAAALIvQCgAAAACwLEIrAAAAAMCyCK0AAAAAAMsitAIAAAAALIvQCgAAAACwLEIrAAAAAMCyCK0AAAAAAMsitAIAAAAALIvQCgAAAACwLEIrAAAAAMCyCK0AAAAAAMsitAIAAAAALIvQCgAAAACwLEIrAAAAAMCyCK0AAAAAAMsitAIAAAAALIvQCgAAAACwLEIrAAAAAMCyCK0AAAAAAMsitAIAAAAALIvQCgAAAACwLEIrAAAAAMCyCK0AAAAAAMsitAIAAAAALIvQCgAAAACwLEIrUI3FxMQoNja2QsdMSEjQ2LFjK218AAAAoDzqOrsAANZRUFAgu90uu91+zWOZpinTNFWnDn8bAwAAwNXjt0nAYrKzs9W3b1/5+/vL19dXCxculLe3tzIyMiQVz4T26NHD0X/79u0KDQ1VmzZtNGfOHEnSkSNH1L17dwUEBMjX11fx8fGSpFWrVikoKEj+/v6KjIyUVDyb+sADDygsLEwPPPCA1q9fr379+l12fEmaOnWqQkJCZLPZ9Morr0iS0tLS1K5dOz344IPy9fXVoUOHKvW7AgAAQM3HTCtgMatWrZKXl5dWrlwpScrMzNT48eMv2T8lJUVbt25Vdna2AgMD1bdvX3300UeKiorSSy+9pMLCQp05c0bHjx/XY489pg0bNsjHx0cnT550jLF7925t3LhR7u7uWr9+/RXH37lzp/bt26dt27bJNE31799fGzZs0O9+9zvt27dP7733nrp27Vop3w8AAABqF0IrYDF+fn76wx/+oPHjx6tfv34KDw+/bP8BAwbI3d1d7u7uioiI0LZt2xQSEqLRo0crPz9fAwcOVEBAgNavX6/u3bvLx8dHktSkSRPHGP3795e7u3uZx9+4caNWr16twMBASVJWVpb27dun3/3ud7rlllsIrAAAAKgwhFbAYtq2baukpCR98cUXmjhxoiIjI1W3bl0VFRVJknJzc0v0Nwzjovfdu3fXhg0btHLlSo0aNUq///3v1bhx40ues0GDBpc8Vtr4pmlqwoQJevzxx0scS0tLu+xYAAAAQHlxTytgMenp6apfv75Gjhyp6OhoJSUlydvbW4mJiZKkxYsXl+i/fPly5ebm6sSJE1q/fr1CQkL0ww8/6MYbb9Rjjz2mRx99VElJSeratas2bNig77//XpJKLA++nNLGj4qK0ty5c5WVlSVJOnz4sI4dO1aB3wIAAABQjJlWwGJ27Nih6Oho1alTR66urpo5c6ZycnL0yCOPaNKkSSU2YZIkm82miIgIZWRkaNKkSfLy8tJ7772nqVOnytXVVR4eHpo/f76aNWum2bNna/DgwSoqKlLz5s311VdfXbGe0sb38vLSnj17FBoaKkny8PDQBx98IBcXl8r4SgAAAFCLGaZpOruGMrHb7WZCQoKzywAAAAAAVDDDMBJN0yz1uYvMtAK4Jnvi4xT/8XydPpGhhjc0Vfh9D6pDeISzywIAAEANQWgFcNX2xMdp9ex/qOBsniTpdMZxrZ79D0kiuAIAAKBCsBETgKsW//F8R2A9r+BsnuI/nu+kigAAAFDTEFoBXLXTJzLK1Q4AAACUF6EVwFVreEPTcrUDAAAA5UVoBXDVwu97UHWvcyvRVvc6N4Xf96CTKgIAAEBNw0ZMAK7a+c2W2D0YAAAAlYXQCuCadAiPIKQCAACg0rA8GAAAAABgWYRWAAAAAIBlEVoBAAAAAJZVIaHVMIy5hmEcMwxj5wVtMYZhHDYMI/ncv7suODbBMIz9hmHsNQwjqiJqAAAAAADUPBU10zpPUp9S2qeZphlw7t8XkmQYRkdJ90nqdO4z/zIMw6WC6gAAAAAA1CAVElpN09wg6WQZuw+Q9LFpmnmmaX4vab+kzhVRBwAAAGqvtLQ0+fr6lrn/9OnTdebMmUqsCEBFqOx7Wp8xDCPl3PLhxufaWkk6dEGfn861AQAAAFWG0ApUD5UZWmdKulVSgKQjkv5e3gEMwxhjGEaCYRgJx48fr+DyAAAAUNMUFBRoxIgR6tChg4YOHaozZ85o7dq1CgwMlJ+fn0aPHq28vDzNmDFD6enpioiIUEQEzxsHrKzSQqtpmkdN0yw0TbNI0hz93xLgw5JuvqDrTefaShtjtmmadtM07c2aNausUgEAAFBD7N27V0899ZT27Nmj66+/Xm+++aZGjRqlhQsXaseOHSooKNDMmTM1duxYeXl5KS4uTnFxcc4uG8BlVFpoNQyj5QVvB0k6v7PwZ5LuMwzDzTAMH0ltJG2rrDoAAABQe9x8880KCwuTJI0cOVJr166Vj4+P2rZtK0l66KGHtGHDBmeWCKCc6lbEIIZhfCSph6SmhmH8JOkVST0MwwiQZEpKk/S4JJmmucswjE8k7ZZUIOlp0zQLK6IOAAAA1G6GYZR436hRI504ccJJ1QCoCBW1e/Bw0zRbmqbpaprmTaZp/sc0zQdM0/QzTdNmmmZ/0zSPXND/NdM0bzVNs51pmv+tiBoAAACAH3/8UVu2bJEkffjhh7Lb7UpLS9P+/fslSe+//75uv/12SVLDhg11+vRpp9UKoGwqe/dgAAAAoMq0a9dO//znP9WhQwf98ssvGjdunN59910NGzZMfn5+qlOnjp544glJ0pgxY9SnTx82YgIszjBN09k1lIndbjcTEhKcXQYAAAAAoIIZhpFomqa9tGMVck8rAAAAYHXLvj2sqV/uVfqpHHk1cld0VDsNDGzl7LIAXAGhFQAAADXesm8Pa8KSHcrJL97/8/CpHE1YskOSCK6AxXFPKwAAAGq8qV/udQTW83LyCzX1y71OqghAWRFaAQAAUOOln8opVzsA6yC0AgAAoMbzauRernYA1kFoBQAAQI0XHdVO7q4uJdrcXV0UHdXOSRUBKCs2YgIAAECNd36zJXYPBqofQisAAABqhYGBrQipQDXE8mAAAAAAgGURWgEAAAAAlkVoBQAAAABYFqEVAAAAAGBZhFYAAAAAgGURWgEAAAAAlkVoBQAAAABYFqEVAAAAAGBZhFYAAAAAgGURWgEAAAAAlkVoBQAAAABYFqEVAAAAAGBZhFYAAAAAgGURWgEAAAAAlkVoBQAAAABYFqEVAGAZMTExio2NdXYZAADAQgitAAAAAADLIrQCAJzqtddeU9u2bXXbbbdp7969kqQDBw6oT58+Cg4OVnh4uFJTUyVJx48f15AhQxQSEqKQkBBt2rRJUvEM7QMPPKDQ0FC1adNGc+bMcdr1AACAilXX2QUAAGqvxMREffzxx0pOTlZBQYGCgoIUHBysMWPGaNasWWrTpo2+/vprPfXUU1q3bp2ee+45jRs3Trfddpt+/PFHRUVFac+ePZKklJQUbd26VdnZ2QoMDFTfvn3l5eXl5CsEAADXitAKAHCa+Ph4DRo0SPXr15ck9e/fX7m5udq8ebOGDRvm6JeXlydJWrNmjXbv3u1o//XXX5WVlSVJGjBggNzd3eXu7q6IiAht27ZNAwcOrLqLAQAAlYLQCgCwlKKiIjVq1EjJycmlHtu6davq1at30THDMC77HgAAVE/c0woAcJru3btr2bJlysnJ0enTp7VixQrVr19fPj4++vTTTyVJpmlq+/btkqTevXvr7bffdnz+wmC7fPly5ebm6sSJE1q/fr1CQkKq9FoAAEDlILQCAJwmKChI9957r/z9/XXnnXc6guaCBQv0n//8R/7+/urUqZOWL18uSZoxY4YSEhJks9nUsWNHzZo1yzGWzWZTRESEunbtqkmTJnE/KwAANYRhmqazaygTu91uJiQkOLsMAIAFxcTEyMPDQy+88IKzSwEAAFfBMIxE0zTtpR3jnlYAQLWWkpKiLVu2yDRNubi4KDIyUjabzdllAQCACkJoBQBUWykpKVqxYoVCQ0MlSZmZmVqxYoUkEVwBAKghuKcVAFBtrV27Vvn5+SXa8vPztXbtWidVBAAAKhqhFQBQbWVmZparHQAAVD+EVgBAteXp6VmudgAAUP0QWgEA1VZkZKRcXV1LtLm6uioyMtJJFQEAgIrGRkwAgGrr/GZLa9euVWZmpjw9Pdk9GACAGobQCgCo1mw2GyEVAIAajOXBAAAAAADLIrQCAAAAACyL0AoAAAAAsCxCKwAAAADAsgitAAAAAADLIrQCAAAAACyL0AoAAAAAsCxCKwAAAADAsgitAAAAAADLIrQCAAAAACyL0AoAAAAAsCxCKwAAAADAsgitAAAAAADLIrQCAAAAACyL0AoAAAAAsCxCKwAAAADAsgitAADA0qZPn64zZ86U+3Pz5s1Tenp6JVQEAKhKhFYAAGBpVxNaCwsLCa0AUEMQWgEAgGVkZ2erb9++8vf3l6+vr1599VWlp6crIiJCERERkqQnn3xSdrtdnTp10iuvvOL4rLe3t8aPH6+goCB99NFHSkhI0IgRIxQQEKCcnBxnXRJQo8XExCg2NrbCxuvWrZvjdXR0tDp16qTo6GjNmjVL8+fPL/d4p06d0r/+9S/H+/T0dA0dOrRCakXVqevsAgAAAM5btWqVvLy8tHLlSklSZmam3n33XcXFxalp06aSpNdee01NmjRRYWGhIiMjlZKSIpvNJkm64YYblJSUJEn697//rdjYWNntdudcDIBy27x5s+P17NmzdfLkSbm4uFz1eOdD61NPPSVJ8vLy0qJFi665TlQtZloBAIBl+Pn56auvvtL48eMVHx8vT0/Pi/p88sknCgoKUmBgoHbt2qXdu3c7jt17771VWS5Q68yfP182m03+/v564IEHShybM2eOQkJC5O/vryFDhjiW9X/66afy9fWVv7+/unfvLknatWuXOnfurICAANlsNu3bt0+S5OHhIUnq37+/srKyFBwcrIULF5aY0d2/f7/uuOMO+fv7KygoSAcOHFBWVpYiIyMVFBQkPz8/LV++XJL04osv6sCBAwoICFB0dLTS0tLk6+srScrNzdXDDz8sPz8/BQYGKi4uTlLx/fCDBw9Wnz591KZNG/3xj3+s5G8VV8JMKwAAsIy2bdsqKSlJX3zxhSZOnKjIyMgSx7///nvFxsbqm2++UePGjTVq1Cjl5uY6jjdo0KCqSwZqjV27dmnKlCnavHmzmjZtqpMnT2rGjBmO44MHD9Zjjz0mSZo4caL+85//6Nlnn9XkyZP15ZdfqlWrVjp16pQkadasWXruuec0YsQInT17VoWFhSXO9dlnn8nDw0PJycmSipchnzdixAi9+OKLGjRokHJzc1VUVKTrrrtOS5cu1fXXX6+MjAx17dpV/fv311//+lft3LnTMU5aWppjnH/+858yDEM7duxQamqqevfure+++06SlJycrG+//VZubm5q166dnn32Wd18880V+4WizJhpBQAAlpGenq769etr5MiRio6OVlJSkho2bKjTp09Lkn799Vc1aNBAnp6eOnr0qP773/9ecqwLPwfg2q1bt07Dhg1zLNVv0qRJieM7d+5UeHi4/Pz8tGDBAu3atUuSFBYWplGjRmnOnDmOcBoaGqq//OUv+tvf/qYffvhB7u7uZarh9OnTOnz4sAYNGiRJqlevnurXry/TNPWnP/1JNptNd9xxhw4fPqyjR49edqyNGzdq5MiRkqT27dvrlltucYTWyMhIeXp6ql69eurYsaN++OGHMn5LqAzMtAIAAMvYsWOHoqOjVadOHbm6umrmzJnasmWL+vTpIy8vL8XFxSkwMFDt27fXzTffrLCwsEuONWrUKD3xxBNyd3fXli1byvxLMYCrM2rUKC1btkz+/v6aN2+e1q9fL6l4VvXrr7/WypUrFRwcrMTERN1///3q0qWLVq5cqbvuukvvvPOOevbsedXnXrBggY4fP67ExES5urrK29u7xCqM8nJzc3O8dnFxUUFBwVWPhWtHaAUAAJYRFRWlqKioEm12u13PPvus4/28efNK/eyFy/4kaciQIRoyZEhFlwjUWj179tSgQYP0+9//XjfccINOnjxZ4vjp06fVsmVL5efna8GCBWrVqpUk6cCBA+rSpYu6dOmi//73vzp06JAyMzPVunVrjR07Vj/++KNSUlLKFFobNmyom266ScuWLdPAgQOVl5enwsJCZWZmqnnz5nJ1dVVcXJxjZvRyKy7Cw8O1YMEC9ezZU999951+/PFHtWvXzrGZG6yD5cEAAKDGWHlwpXov6i3bezb1XtRbKw+udHZJQI3RqVMnvfTSS7r99tvl7++v3//+9yWO//nPf1aXLl0UFham9u3bO9qjo6Pl5+cnX19fdevWTf7+/vrkk0/k6+urgIAA7dy5Uw8++GCZ63j//fc1Y8YM2Ww2devWTT///LNGjBihhIQE+fn5af78+Y7z33DDDQoLC5Ovr6+io6NLjPPUU0+pqKhIfn5+uvfeezVv3rwSM6ywDsM0TWfXUCZ2u91MSEhwdhkAAMCiVh5cqZjNMcot/L8lgfVc6immW4z6tu7rxMoAAFdiGEaiaZqlPqOM5cEAAKBGeCvprRKBVZJyC3P1VtJbhFYAZbInPk7xH8/X6RMZanhDU4Xf96A6hEc4u6xaj9AKAABqhJ+zfy5XOwBcaE98nFbP/ocKzuZJkk5nHNfq2f+QJIKrk3FPK0rl7e2tjIwMZ5cBAECZtWjQolztAHCh+I/nOwLreQVn8xT/8XwnVYTzCK0AAKBGeC7oOdVzqVeirZ5LPT0X9JyTKgJQnZw+UfqEzaXaUXUIrVB2drb69u0rf39/+fr6auHChZKkt99+W0FBQfLz81Nqaqok6eTJkxo4cKBsNpu6du2qlJQUSZKfn59OnTol0zR1ww03aP784r9IPfjgg/rqq6+cc2EAgFqlb+u+iukWo5YNWsqQoZYNWrIJE4Aya3hD03K1o+oQWqFVq1bJy8tL27dv186dO9WnTx9JUtOmTZWUlKQnn3xSsbGxkqRXXnlFgYGBSklJ0V/+8hfH9uRhYWHatGmTdu3apdatWys+Pl6StGXLFnXr1s05FwYAqHX6tu6r1UNXK+WhFK0euprACqDMwu97UHWvK/nIm7rXuSn8vrI/jgeVg9AK+fn56auvvtL48eMVHx8vT09PSdLgwYMlScHBwY4Htm/cuFEPPPCApOIHTJ84cUK//vqrwsPDtWHDBm3YsEFPPvmkduzYocOHD6tx48Zq0KCBU64LAAAAKKsO4RHqPeYZNWzaTDIMNWzaTL3HPMMmTBbA7sFQ27ZtlZSUpC+++EITJ05UZGSkJDkeruzi4qKCgoLLjtG9e3f985//1I8//qjXXntNS5cu1aJFixQeHl7p9QMAAAAVoUN4BCHVgphphdLT01W/fn2NHDlS0dHRSkpKumTf8PBwLViwQJK0fv16NW3aVNdff71uvvlmZWRkaN++fWrdurVuu+02xcbGqnv37lV1GQAAAABqIGZaoR07dig6Olp16tSRq6urZs6cqaFDh5baNyYmRqNHj5bNZlP9+vX13nvvOY516dJFhYWFkorD7YQJE3TbbbdVyTUAAAAAqJkM0zSdXUOZ2O12MyEhwdllAAAAAAAqmGEYiaZp2ks7xkwrKsyybw9r6pd7lX4qR16N3BUd1U4DA1s5uywAAAAA1RihFRVi2beHNWHJDuXkFy8PPnwqRxOW7JAkgisAAACAq8ZGTKgQU7/c6wis5+XkF2rql3udVBEAAACAmoDQigqRfiqnXO0AAAAAUBaEVlQIr0bu5WoHAAAAgLIgtKJCREe1k7urS4k2d1cXRUe1c1JFAAAAAGoCNmJChTi/2RK7BwMAAACoSIRWVJiBga0IqQAAAAAqFMuDAQAAAACWVSGh1TCMuYZhHDMMY+cFbU0Mw/jKMIx95342PtduGIYxwzCM/YZhpBiGEVQRNQAAAAAAap6KmmmdJ6nPb9pelLTWNM02ktaeey9Jd0pqc+7fGEkzK6gGAAAAAEANUyGh1TTNDZJO/qZ5gKT3zr1+T9LAC9rnm8W2SmpkGEbLiqgDAAAAAFCzVOY9rTeapnnk3OufJd147nUrSYcu6PfTuTYAAAAAAEqoko2YTNM0JZnl/ZxhGGMMw0gwDCPh+PHjlVAZAAAAAMDKKjO0Hj2/7Pfcz2Pn2g9LuvmCfjeda7uIaZqzTdO0m6Zpb9asWSWWCgAAAACwosoMrZ9Jeujc64ckLb+g/cFzuwh3lZR5wTJiAAAAAAAc6lbEIIZhfCSph6SmhmH8JOkVSX+V9IlhGI9I+kHSPee6fyHpLkn7JZ2R9HBF1AAAAAAAqHkqJLSapjn8EociS+lrSnq6Is4LABd69NFH9fvf/14dO3Z0dikAAACoIBUSWgHgPNM0ZZqm6tSpkn3eSvj3v/9d5ecEAABA5ar63yoB1DhpaWlq166dHnzwQfn6+urPf/6zQkJCZLPZ9Morrzj6zZ8/XzabTf7+/nrggQccn+3Zs6dsNpsiIyP1448/SpIOHDigrl27ys/PTxMnTpSHh4ckaf369erRo4eGDh2q9u3ba8SIESpewCH16NFDCQkJ+uyzzxQQEKCAgAC1a9dOPj4+kqTExETdfvvtCg4OVlRUlI4c4XZ6AAAAqyO0AqgQ+/bt01NPPaVp06bp8OHD2rZtm5KTk5WYmKgNGzZo165dmjJlitatW6ft27frrbfekiQ9++yzeuihh5SSkqIRI0Zo7NixkqTnnntOzz33nHbs2KGbbrqpxLm+/fZbTZ8+Xbt379bBgwe1adOmEsf79++v5ORkJScny9/fXy+88ILy8/P17LPPatGiRUpMTNTo0aP10ksvVc2XAwAAgKtGaAVQIW655RZ17dpVq1ev1urVqxUYGKigoCClpqZq3759WrdunYYNG6amTZtKkpo0aSJJ2rJli+6//35J0gMPPKCNGzc62ocNGyZJjuPnde7cWTfddJPq1KmjgIAApaWllVrTG2+8IXd3dz399NPau3evdu7cqV69eikgIEBTpkzRTz/9VBlfBQAAACoQ97QCqBANGjSQVHxP64QJE/T444+XOP72229X2Lnc3Nwcr11cXFRQUHBRnzVr1ujTTz/Vhg0bHHV16tRJW7ZsqbA6AAAAUPmYaQVQoaKiojR37lxlZWVJkg4fPqxjx46pZ8+e+vTTT3XixAlJ0smTJyVJ3bp108cffyxJWrBggcLDwyVJXbt21eLFiyXJcbysfvjhBz399NP69NNP5e7uLklq166djh8/7git+fn52rVr1zVeLQAAACobM60AKlTv3r21Z88ehYaGSpI8PDz0wQcfqFOnTnrppZd0++23y8XFRYGBgZo3b57efvttPfzww5o6daqaNWumd999V5I0ffp0jRw5Uq+99pr69OkjT0/PMtcwb948nThxQgMHDpQkeXl56YsvvtCiRYs0duxYZWZmqqCgQM8//7w6depU4d8BAAAAKo5xftdNq7Pb7WZCQoKzywBQRc6cOSN3d3cZhqGPP/5YH330kZYvX+7ssgAAAFAJDMNINE3TXtoxZloBWFJiYqKeeeYZmaapRo0aae7cudc03rJvD2vql3uVfipHXo3cFR3VTgMDW1VQtQAAAKgshFYAlhQeHq7t27dXyFjLvj2sCUt2KCe/UJJ0+FSOJizZIUkEVwAAAItjIyYANd7UL/c6Aut5OfmFmvrlXidVBAAAgLIitAKo8dJP5ZSrHQAAANZBaAVQ43k1ci9XOwAAAKyD0AqgxouOaid3V5cSbe6uLoqOauekigAAAFBWbMQEoMY7v9kSuwcDAABUP4RWALXCwMBWhFQAAIBqiOXBAAAAAADLIrQCAAAAACyL0AoAAAAAsCxCKwAAAADAsgitAAAAAADLIrQCAAAAACyL0AoAAAAAsCxCKwAAAADAsgitAAAAAADLIrQCAAAAACyL0AoAAAAAsCxCKwAAAADAsgitAAAAAADLIrQCAAAAACyL0AoAAAAAsCxCKwAAAADAsgitAAAAsIRly5Zp9+7djvcvv/yy1qxZI0maPn26zpw546zSADgRoRUAAACW8NvQOnnyZN1xxx2SCK1AbUZoBQAAQKX54IMP1LlzZwUEBOjxxx9XYWGhPDw89NJLL8nf319du3bV0aNHtXnzZn322WeKjo5WQECADhw4oFGjRmnRokWaMWOG0tPTFRERoYiICM2dO1fPP/+84xxz5szRuHHjnHeRACoVoRUAAACVYs+ePVq4cKE2bdqk5ORkubi4aMGCBcrOzlbXrl21fft2de/eXXPmzFG3bt3Uv39/TZ06VcnJybr11lsd44wdO1ZeXl6Ki4tTXFyc7rnnHq1YsUL5+fmSpHfffVejR4921mUCqGR1nV0AAAAAaqa1a9cqMTFRISEhkqScnBw1b95c1113nfr16ydJCg4O1ldffVWucT08PNSzZ099/vnn6tChg/Lz8+Xn51fh9QOwBkIrAAAAKoVpmnrooYf0+uuvl2iPjY2VYRiSJBcXFxUUFJR77EcffVR/+ctf1L59ez388MMVUi8Aa2J5MAAAACpFZGSkFi1apGPHjkmSTp48qR9++OGS/Rs2bKjTp0+X6ViXLl106NAhffjhhxo+fHjFFg7AUgitAAAAqBQdO3bUlClT1Lt3b9lsNvXq1UtHjhy5ZP/77rtPU6dOVWBgoA4cOFDi2JgxY9SnTx9FREQ42u655x6FhYWpcePGlXYNAJzPME3T2TWUid1uNxMSEpxdBgAAACyiX79+GjdunCIjI51dCoBrZBhGomma9tKOMdMKAACA6iPlE536Swe1vcFF7j/FK7LZCWdXBKCSsRETAAAAqoeUT6QVY9UoP0ffPetR3LZibPFP2z3OqwtApWKmFQAAANXD2slSfk7Jtvyc4nYANRahFQBQ7Xl7eysjI+Oyfe666y6dOnWqagoCUDkyfypfO4AageXBAIBa4YsvvnB2CQCuledNUuah0tsB1FjMtAIAnOqDDz5Q586dFRAQoMcff1xff/21bDabcnNzlZ2drU6dOmnnzp1av369unfvrr59+6pdu3Z64oknVFRUdNF4AwcOVHBwsDp16qTZs2c72s/PxqalpalDhw567LHH1KlTJ/Xu3Vs5OTkXjQPAgiJfllzdS7a5uhe3A6ixCK0AAKfZs2ePFi5cqE2bNik5OVkuLi7au3ev+vfvr4kTJ+qPf/yjRo4cKV9fX0nStm3b9Pbbb2v37t06cOCAlixZctGYc+fOVWJiohISEjRjxgydOHHxzqL79u3T008/rV27dqlRo0ZavHhxpV8rgApgu0e6e4bkebMko/jn3TPYhAmo4VgeDABwmrVr1yoxMVEhISGSpJycHDVv3lwvv/yyQkJCVK9ePc2YMcPRv3PnzmrdurUkafjw4dq4caOGDh1aYswZM2Zo6dKlkqRDhw5p3759uuGGG0r08fHxUUBAgCQpODhYaWlplXSFACqc7R5CKlDLEFoBAE5jmqYeeughvf766yXajxw5oqysLOXn5ys3N1cNGjSQJBmGUaLfb9+vX79ea9as0ZYtW1S/fn316NFDubm5F53Xzc3N8drFxYXlwQAAWBjLgwEAThMZGalFixbp2LFjkqSTJ0/qhx9+0OOPP64///nPGjFihMaPH+/ov23bNn3//fcqKirSwoULddttt5UYLzMzU40bN1b9+vWVmpqqrVu3Vun1AACAisdMKwDAaTp27KgpU6aod+/eKioqkqurqwYMGCBXV1fdf//9KiwsVLdu3bRu3TrVqVNHISEheuaZZ7R//35FRERo0KBBJcbr06ePZs2apQ4dOqhdu3bq2rWrk64MAABUFMM0TWfXUCZ2u91MSEhwdhkAACdZv369YmNj9fnnnzu7FAAAUMEMw0g0TdNe2jFmWgEAtcrin0/q9YNHdDgvX63cXDWhdUsNadHE2WUBAIBLILQCAKqFHj16qEePHtc0xuKfT+qFvYeUU1S8yuinvHy9sPeQJBFcAQCwKDZiAgDUGq8fPOIIrOflFJl6/eARJ1UEAACuhNAKAKg1Dufll6sdAAA4H6EVAFBrtHJzLVc7AABwPkIrAKDWmNC6pdzrGCXa3OsYmtC6pZMqAgAAV8JGTACAWuP8ZkvsHgwAQPVBaAUA1CpDWjQhpAIAUI2wPBgAAAAAYFmEVgAAAACAZRFaAQAAAACWRWgFAAAAAFgWoRUAAAAAYFmEVgAAAACAZRFaAQAAAACWRWgFAAAAAFgWoRUAAAAAYFmEVgAAAACAZRFaAQAAAACWRWgFAAAAAFgWoRUAAAAAYFmEVgAAAACAZRFaAQAAAACWRWgFAAAAAFgWoRUAAAAAYFmEVgAAUKq0tDT5+vpW+nl69OihhISEMvdfv369+vXrV4kVAQCshNAKAAAAALAsQisAALiigwcPKjAwUFOnTtXgwYPVp08ftWnTRn/84x8dfT766CP5+fnJ19dX48ePlyR9+umn+v3vfy9Jeuutt9S6dWvHeGFhYRedZ/Xq1QoNDVVQUJCGDRumrKwsSdKqVavUvn17BQUFacmSJY7+x48fV69evdSpUyc9+uijuuWWW5SRkSFJ+uCDD9S5c2cFBATo8ccfV2FhYeV8OQCASkVoBQAAl7V3714NGTJE8+bNU7NmzZScnKyFCxdqx44dWrhwoQ4dOqT09HSNHz9e69atU3Jysr755hstW7ZM4eHhio+PlyTFx8frhhtu0OHDhxUfH6/u3buXOE9GRoamTJmiNWvWKCkpSXa7XW+++aZyc3P12GOPacWKFUpMTNTPP//s+Myrr76qnj17ateuXRo6dKh+/PFHSdKePXu0cOFCbdq0ScnJyXJxcdGCBQuq7ksDAFSYus4uAAAAWNfx48c1YMAALVmyRB07dtS3336ryMhIeXp6SpI6duyoH374QSdOnFCPHj3UrFkzSdKIESO0YcMGDRw4UFlZWTp9+rQOHTqk+++/Xxs2bFB8fLwGDx5c4lxbt27V7t27HTOwZ8+eVWhoqFJTU+Xj46M2bdpIkkaOHKnZs2dLkjZu3KilS5dKkvr06aPGjRtLktauXavExESFhIRIknJyctS8efNK/rYAAJWB0AoAAC7J09NTv/vd77Rx40Z17NhRkuTm5uY47uLiooKCgsuO0a1bN7377rtq166dwsPDNXfuXG3ZskV///vfS/QzTVO9evXSRx99VKI9OTm53HWbpqmHHnpIr7/+erk/CwCwFpYHAwCAS7ruuuu0dOlSzZ8/Xx9++OEl+3Xu3Fn/+9//lJGRocLCQn300Ue6/fbbJUnh4eGKjY1V9+7dFRgYqLi4OLm5uTlma8/r2rWrNm3apP3790uSsrOz9d1336l9+/ZKS0vTgQMHJKlEqA0LC9Mnn3wiqfh+2F9++UWSFBkZqUWLFunYsWOSpJMnT+qHH36ooG8FAFCVCK0AAOCyGjRooM8//1zTpk3Tr7/+Wmqfli1b6q9//asiIiLk7++v4OBgDRgwQFJxaD106JC6d+8uFxcX3XzzzbrtttsuGqNZs2aaN2+ehg8fLpvN5lgaXK9ePc2ePVt9+/ZVUFBQiWW+r7zyilavXi1fX199+umnatGihRo2bKiOHTtqypQp6t27t2w2m3r16qUjR45UzhcEAKhUhmmazq6hTOx2u1meZ7gBAICaLy8vTy4uLqpbt662bNmiJ5988qqWEwMAnMswjETTNO2lHeOeVgAAUG39+OOPuueee1RUVKTrrrtOc+bM0Xdf/6wtyw8o62SePJq4KXTArWrbpYWzSwUAXCVCKwAAqLbatGmjb7/91vH+u69/VtyCVBWcLZIkZZ3MU9yCVEkiuAJANVXp97QahpFmGMYOwzCSDcNIONfWxDCMrwzD2HfuZ+PKrgMAANR8W5YfcATW8wrOFmnL8gNOqggAcK2qaiOmCNM0Ay5Yo/yipLWmabaRtPbcewAAgGuSdTKvXO0AAOtz1u7BAyS9d+71e5IGOqkOAABQg3g0cStXOwDrmjdvntLT051dBiygKkKrKWm1YRiJhmGMOdd2o2ma5/ed/1nSjVVQB1CCh4eHJCk9PV1Dhw69ZL9Tp07pX//6V1WVBQC4BqEDblXd60r+elP3ujoKHXCrkyoCcLUIrTivKkLrbaZpBkm6U9LThmF0v/CgWfzMnVKfu2MYxhjDMBIMw0g4fvx4FZSK2sjLy0uLFi265HFCKwBUH227tFDEiPaOmVWPJm6KGNGeTZgAi8jOzlbfvn3l7+8vX19fLVy4UImJibr99tsVHBysqKgoHTlyRIsWLVJCQoJGjBihgIAA5eTkyNvbWxkZGZKkhIQE9ejRQ5IUExOjBx54QKGhoWrTpo3mzJnjxCtEZaj03YNN0zx87ucxwzCWSuos6ahhGC1N0zxiGEZLSccu8dnZkmZLxc9prexaUTulpaWpX79+2rlzp3bt2qWHH35YZ8+eVVFRkRYvXqxJkybpwIEDCggIUK9evTR16lRNnTpVn3zyifLy8jRo0CC9+uqrSktL05133qnbbrtNmzdvVqtWrbR8+XK5u7s7+xIBoFZp26UFIRWwqFWrVsnLy0srV66UJGVmZurOO+/U8uXL1axZMy1cuFAvvfSS5s6dq3/84x+KjY2V3V7qoztLSElJ0datW5Wdna3AwED17dtXXl5elX05qCKVOtNqGEYDwzAann8tqbeknZI+k/TQuW4PSVpemXUAZTVr1iw999xzSk5OVkJCgm666Sb99a9/1a233qrk5GRNnTpVq1ev1r59+7Rt2zYlJycrMTFRGzZskCTt27dPTz/9tHbt2qVGjRpp8eLFTr4iAAAA6/Dz89NXX32l8ePHKz4+XocOHdLOnTvVq1cvBQQEaMqUKfrpp5/KPe6AAQPk7u6upk2bKiIiQtu2bauE6uEslT3TeqOkpYZhnD/Xh6ZprjIM4xtJnxiG8YikHyTdU8l1AGUSGhqq1157TT/99JMGDx6sNm3aXNRn9erVWr16tQIDAyVJWVlZ2rdvn373u9/Jx8dHAQEBkqTg4GClpaVVYfUAAADW1rZtWyUlJemLL77QxIkT1bNnT3Xq1Elbtmy54mfr1q2roqLiR1rl5uaWOHYub1zyPaq3Sp1pNU3zoGma/uf+dTJN87Vz7SdM04w0TbONaZp3mKZ5sjLrAMrq/vvv12effSZ3d3fdddddWrdu3UV9TNPUhAkTlJycrOTkZO3fv1+PPPKIJMnN7f92p3RxcVFBQUGV1Q4AAGB16enpql+/vkaOHKno6Gh9/fXXOn78uCO05ufna9euXZKkhg0b6vTp047Pent7KzExUZIuWs22fPly5ebm6sSJE1q/fr1CQkKq6IpQFSr9nlagOjl48KBat26tsWPH6scff1RKSor8/f1L/AczKipKkyZN0ogRI+Th4aHDhw/L1dXViVUDAABUDzt27FB0dLTq1KkjV1dXzZw5U3Xr1tXYsWOVmZmpgoICPf/88+rUqZNGjRqlJ554Qu7u7tqyZYteeeUVPfLII5o0aZJjE6bzbDabIiIilJGRoUmTJnE/aw1DaAUu8Mknn+j999+Xq6urWrRooT/96U9q0qSJwsLC5OvrqzvvvFNTp07Vnj17FBoaKqn40TkffPCBXFxcnFw9AACAtUVFRSkqKuqi9vP7g1xoyJAhGjJkiON9eHi4vvvuu1LHtdlsmj9/fsUVCksxip84Y312u91MSEhwdhlAqb77+mdtWX5AWSfz5NHETaEDbmXnSgAAgMqW8oliop+Rh3laL9x5qxT5smRju5zqyDCMRNM0S90qmplW4Bp99/XPiluQqoKzxRsDZJ3MU9yCVEkiuAIAAFSWlE+kFWMVE5on6Top85C0YmzxMYJrjVKpGzEBtcGW5QccgfW8grNF2rL8gJMqAgAAqAXWTpbyc0q25ecUt6NGIbQC1yjrZF652gEAAFABMi/xPNdLtaPaIrQC18ijiVu52gEAAFABPG8qXzuqLUIrcI1CB9yquteV/J9S3evqKHTArU6qCAAAoBaIfFlydS/Z5upe3I4ahY2YgGt0frMldg8GAACoQuc3W1o7uXhJsOdN7B5cQ/HIGwAAAACAU13ukTcsDwYAAAAAWBahFQAAAABgWYRWAAAAAIBlEVoBAAAAAJZFaAUAAAAAWBahFQAAAABgWYRWAAAAAIBlEVoBAAAAAJZFaAUAAAAAWBahFQAAAABgWYRWAAAAAIBlEVoB1Ere3t7KyMgoU9+EhASNHTtWkrR+/Xpt3ry5MksDAADABeo6uwAAsDq73S673S6pOLR6eHioW7duTq4KAACgdmCmFUC1NnXqVM2YMUOSNG7cOPXs2VOStG7dOo0YMUKrV69WaGiogoKCNGzYMGVlZTk++8Ybb8jPz0+dO3fW/v37JUmffvqpfH195e/vr+7du0sqDqr9+vVTWlqaZs2apWnTpikgIEDx8fE6fvy4hgwZopCQEIWEhGjTpk1V/A0AAADUbIRWANVaeHi44uPjJRUv483KylJ+fr7i4+Nls9k0ZcoUrVmzRklJSbLb7XrzzTcdn/X09NSOHTv0zDPP6Pnnn5ckTZ48WV9++aW2b9+uzz77rMS5vL299cQTT2jcuHFKTk5WeHi4nnvuOY0bN07ffPONFi9erEcffbTKrh0AAKA2YHkwgGotODhYiYmJ+vXXX+Xm5qagoCAlJCQoPj5e/fv31+7duxUWFiZJOnv2rEJDQx2fHT58uOPnuHHjJElhYWEaNWqU7rnnHg0ePPiK51+zZo12797teP/rr78qKytLHh4eFXmZAAAAtRahFUC15urqKh8fH82bN0/dunWTzWZTXFyc9u/fLx8fH/Xq1UsfffRRqZ81DOOi17NmzdLXX3+tlStXOgLx5RQVFWnr1q2qV69exV0UAAAAHFgeDKDaCw8PV2xsrLp3767w8HDNmjVLgYGB6tq1qzZt2uS4XzU7O1vfffed43MLFy50/Dw/A3vgwAF16dJFkydPVrNmzXTo0KES52rYsKFOnz7teN+7d2+9/fbbjvfJycmVdZkAAAC1EqEVQLUXHh6uI0eOKDQ0VDfeeKPq1aun8PBwNWvWTPPmzdPw4cNls9kUGhqq1NRUx+d++eUX2Ww2vfXWW5o2bZokKTo6Wn5+fvL19VW3bt3k7+9f4lx33323li5d6tiIacaMGUpISJDNZlPHjh01a9asKr12AACAms4wTdPZNZSJ3W43ExISnF0GADisPLhSbyW9pZ+zf1aLBi30XNBz6tu6r7PLAgAAqHYMw0g0TdNe2jHuaQWAq7Dy4ErFbI5RbmGuJOlI9hHFbI6RJIIrAABABWJ5MABchbeS3nIE1vNyC3P1VtJbTqoIAACgZiK0AsBV+Dn753K1AwAA4OoQWgHgKrRo0KJc7QAAALg6hFYAuArPBT2nei4ln81az6Wengt6zkkVAQAA1ExsxAQAV+H8ZkvsHgwAAFC5CK0AcJX6tu5LSAUAAKhkLA8GAAAAAFgWoRUAAAAAYFmEVgAAAACAZRFaAQAAAACWRWgFAAAAAFgWoRUAAAAAYFmEVgAAAACAZRFaAQAAAACWRWgFAAAAAFgWoRUAAAAAYFmEVgAAAACAZRFaAQAAAACWRWgFAAAAAFgWoRUAAAAAYFmEVgAAAACAZRFaAQAAAACWRWgFAAAAAFgWoRUAAAAAYFmEVgAAAACAZRFaAQAAAACWRWgFAAAAAFgWoRUAAEne3t7KyMhwdhkAAOA3CK0AAAAAAMsitAIAap3s7Gz17dtX/v7+8vX11cKFCx3HcnJydOedd+qdd95RmzZtdPz4cUlSUVGR/t//+3+O9wAAoGoQWgEAtc6qVavk5eWl7du3a+fOnerTp48kKSsrS3fffbeGDx+uxx9/XCNHjtSCBQskSWvWrJG/v7+aNWvmzNIB/MaoUaO0aNEiZ5cBoBLVqtBa2l/Wvb299cc//lF+fn7q3Lmz9u/fL0lasWKFunTposDAQN1xxx06evSopOJfaB5++GH5+fnJZrNp8eLFmjt3rp5//nnHeebMmaNx48Y54xIBAGXg5+enr776SuPHj1d8fLw8PT0lSQMGDNDDDz+sBx98UJI0evRozZ8/X5I0d+5cPfzww06rGQCA2qpWhdZL/WXd09NTO3bs0DPPPOMIn7fddpu2bt2qb7/9Vvfdd5/eeOMNSdKf//xnR/+UlBT17NlT99xzj1asWKH8/HxJ0rvvvqvRo0c75RoBAFfWtm1bJSUlyc/PTxMnTtTkyZMlSWFhYVq1apVM05Qk3Xzzzbrxxhu1bt06bdu2TXfeeaczywZqtbS0NHXo0EGPPfaYOnXqpN69eysnJ6dEn0tNRgCo3mpVaL3UX9aHDx/u+LllyxZJ0k8//aSoqCj5+flp6tSp2rVrl6Ti5WFPP/20Y8zGjRvLw8NDPXv21Oeff67U1FTl5+fLz8+viq8OAFBW6enpql+/vkaOHKno6GglJSVJkiZPnqzGjRuX+O/8o48+qpEjR2rYsGFycXFxVskAJO3bt09PP/20du3apUaNGmnx4sUX9SltMgJA9VarQuul/rJuGIajz/nXzz77rJ555hnt2LFD77zzjnJzcy879qOPPqp58+bp3XffZfkYAFjcjh071LlzZwUEBOjVV1/VxIkTHcfeeust5eTk6I9//KMkqX///o5bQwA4l4+PjwICAiRJwcHBSktLu6hPaZMRAKq3us4uoCqlp6erSZMmGjlypBo1aqR///vfkqSFCxfqxRdf1MKFCxUaGipJyszMVKtWrSRJ7733nmOMXr166Z///KemT58uSfrll1/UuHFjdenSRYcOHVJSUpJSUlKq9sIAAOUSFRWlqKioEm0X/vL77rvvOl5v375d/v7+at++fVWVB+AS3NzcHK9dXFwuWh4slT4ZAaB6q1UzrZf6y/ovv/wim82mt956S9OmTZMkxcTEaNiwYQoODlbTpk0dY0ycOFG//PKLfH195e/vr7i4OMexe+65R2FhYWrcuHHVXhgAoMKlpKSoX79+uuOOOxQQEMAfJIFq4vwjrC6cjABQvdWqmdbS/rIuSdHR0frb3/5Wom3AgAEaMGDARX09PDxKzLxeaOPGjewaDAA1QEpKilasWCG73S673S6peFd5SbLZbM4sDcAVnJ+McHNz00cffeTscgBUgFoVWivDyoMrFRsfq83jN6uRTyPl+lz+3lcAgPWtXbvWsSP8efn5+Vq7di2hFXASb29v7dy50/H+hRdeKLVfaZMRAKq3Wh9aS7uBv6xWHlypmM0xylWu2v6trSQpZnOMJKlv674VUB0AwBkyMzPL1Q7A+TJXrFDB0aP6LrSbfrn5ZjUf97w8777b2WUBqAC16p7WivZW0lvKLSw5s5pbmKu3kt5yUkUAgIpw/pFoZW0H4FyZK1boyKSX9dUt3mrs4qKC9HQdmfSyMs8t6wdQvRFar8HP2T+Xqx0AUD1ERkbK1dW1RJurq6siIyOdVBGAyzk2bbrM3zye0MzN1bFp051TEIAKRWi9Bi0atChXOwCgerDZbLr77rsdM6uenp66++67uZ8VsKiCI0fK1Q6geqn197Rei+eCniu+p/WCJcL1XOrpuaDnnFgVAKAi2Gw2QipQTdRt2VIF6emltgOo/phpvQZ9W/dVTLcYtWzQUoYMtWzQUjHdYtiECQAAoAo1H/e8jHr1SrQZ9eqp+bjnnVMQgArFTOs16tu6LyEVAADAic7vEnxs2nQVHDmiui1bsnswUIMQWgEAAFDted59NyEVqKFYHgwAAAAAsCxCKwAAAADAsgitAAAAAADLIrQCAAAAACyL0AoAAAAAsCxCKwAAAADAsgitAAAAAADLIrQCAAAAACyL0AoAAAAAsCxCKwAAAADAsgitAAAAAADLclpoNQyjj2EYew3D2G8YxovOqgMAANRu8+bN0zPPPOPsMgAAl+CU0GoYhoukf0q6U1JHScMNw+jojFoAAAAAANblrJnWzpL2m6Z50DTNs5I+ljTASbUAAIAaKi0tTe3bt9eoUaPUtm1bjRgxQmvWrFFYWJjatGmjbdu2ObtEAMAVOCu0tpJ06IL3P51rAwAAqFD79+/XH/7wB6Wmpio1NVUffvihNm7cqNjYWP3lL39xdnkAgCuw9EZMhmGMMQwjwTCMhOPHjzu7HAAAUA35+PjIz89PderUUadOnRQZGSnDMOTn56e0tDRnlwcAuAJnhdbDkm6+4P1N59pKME1ztmmadtM07c2aNauy4gAAQM3h5ubmeF2nTh3H+zp16qigoMBZZQEAyshZofUbSW0Mw/AxDOM6SfdJ+sxJtQAAAAAALKquM05qmmaBYRjPSPpSkoukuaZp7nJGLQAAAAAA6zJM03R2DWVit9vNhIQEZ5cBAAAAAKhghmEkmqZpL+2YpTdiAgAAqGiZK1ZoX89I7enQUft6RipzxQpnlwQAuAynLA8GAABwhswVK3Rk0ssyc3MlSQXp6Toy6WVJkufddzuzNADAJTDTCgAAao1j06Y7Aut5Zm6ujk2b7pyCAABXRGgFAAC1RsGRI+VqBwA4H6EVAADUGnVbtixXOwDA+QitAACg1mg+7nkZ9eqVaDPq1VPzcc87pyAAwBWxERMAAKg1zm+2dGzadBUcOaK6LVuq+bjn2YQJACyM0AoAAGoVz7vvJqQCQDXC8mAAAAAAgGURWgEAAAAAlkVoBQAAAABYFqEVAAAAAGBZhFYAAAAAgGURWgEAAAAAlkVoBQAAAABYFqEVAAAAQLW2bNky7d69u0LH9PDwqNDxcPUIrQAAAACqtasJrQUFBZVUDSoaoRUAAACApaSlpalDhw567LHH1KlTJ/Xu3Vs5OTmaM2eOQkJC5O/vryFDhujMmTPavHmzPvvsM0VHRysgIEAHDhxQjx49lJCQIEnKyMiQt7e3JGnevHnq37+/evbsqcjISGVlZSkyMlJBQUHy8/PT8uXLnXjVuBRCKwAAAADL2bdvn55++mnt2rVLjRo10uLFizV48GB988032r59uzp06KD//Oc/6tatm/r376+pU6cqOTlZt95662XHTUpK0qJFi/S///1P9erV09KlS5WUlKS4uDj94Q9/kGmaVXSFKKu6zi4AAAAAAH7Lx8dHAQEBkqTg4GClpaVp586dmjhxok6dOqWsrCxFRUWVe9xevXqpSZMmkiTTNPWnP/1JGzZsUJ06dXT48GEdPXpULVq0qMhLwTUitAIAAACwHDc3N8drFxcX5eTkaNSoUVq2bJn8/f01b948rV+/vtTP1q1bV0VFRZKk3NzcEscaNGjgeL1gwQIdP35ciYmJcnV1lbe390X94XwsDwYAAABQLZw+fVotW7ZUfn6+FixY4Ghv2LChTp8+7Xjv7e2txMRESdKiRYsuOV5mZqaaN28uV1dXxcXF6Ycffqi84nHVCK0AAAAAqoU///nP6tKli8LCwtS+fXtH+3333aepU6cqMDBQBw4c0AsvvKCZM2cqMDBQGRkZlxxvxIgRSkhIkJ+fn+bPn19iTFiHUV1uNLbb7eb5HcAAAAAAADWHYRiJpmnaSzvGTCuqjQu3Lr8W69ev1+bNmyugomIxMTGKjY2VJL388stas2ZNhY0NAACAynfk5+XatClca9f9P23aFK4jP/PoGythIybUWIWFhXJxcbmoff369fLw8FC3bt0q/JyTJ0+u8DEBAABQeY78vFypqS+pqChHkpSbl67U1JckSS1bDHBmaTiHmVZUmbS0NPn6+jrex8bGKiYmRj169ND48ePVuXNntW3bVvHx8ZKknJwc3XffferQoYMGDRqknJwcx2dXr16t0NBQBQUFadiwYcrKypJUfNP9+PHjFRQUpE8//VQzZsxQx44dZbPZdN999yktLU2zZs3StGnTFBAQoPj4eK1YsUJdunRRYGCg7rjjDh09elRS8Qzq6NGj1aNHD7Vu3VozZsxwnP+1115T27Ztddttt2nv3r2O9lGjRjlu9vf29tYrr7zieFh1amqqJOn48ePq1auXOnXqpEcffVS33HLLZe+1AAAAQOU5eCDWEVjPKyrK0cEDsU6qCL/FTCssoaCgQNu2bdMXX3yhV199VWvWrNHMmTNVv3597dmzRykpKQoKCpIkZWRkaMqUKVqzZo0aNGigv/3tb3rzzTf18ssvS5JuuOEGJSUlSZK8vLz0/fffy83NTadOnVKjRo30xBNPyMPDQy+88IIk6ZdfftHWrVtlGIb+/e9/64033tDf//53SVJqaqri4uJ0+vRptWvXTk8++aRSUlL08ccfKzk5WQUFBQoKClJwcHCp19W0aVMlJSXpX//6l2JjY/Xvf/9br776qnr27KkJEyZo1apV+s9//lPZXy8AAAAuITfvSLnaUfUIrbCEwYMHS/q/B0dL0oYNGzR27FhJks1mk81mkyRt3bpVu3fvVlhYmCTp7NmzCg0NdYx17733Ol7bbDaNGDFCAwcO1MCBA0s9908//aR7771XR44c0dmzZ+Xj4+M41rdvX7m5ucnNzU3NmzfX0aNHFR8fr0GDBql+/fqSpP79+5fpupYsWSJJ2rhxo5YuXSpJ6tOnjxo3bly2LwkAAAAVrp5bS+XmpZfaDmtgeTCqzIUPeZZKPuj5/MOjXVxcVFBQcNlxTNNUr169lJycrOTkZO3evbvEbOWFD4xeuXKlnn76aSUlJSkkJKTUsZ999lk988wz2rFjh955551S6yprbb9VnusCAABA1Wt96wuqU8e9RFudOu5qfesLTqoIv0VoRZW58cYbdezYMZ04cUJ5eXn6/PPPL9u/e/fu+vDDDyVJO3fuVEpKiiSpa9eu2rRpk/bv3y9Jys7O1nfffXfR54uKinTo0CFFRETob3/7mzIzM5WVlXXRw6czMzPVqlUrSdJ77713xevo3r27li1bppycHJ0+fVorVqwo2xdwTlhYmD755BNJxffm/vLLL+X6PAAAACpOyxYD1L79a6rn5iXJUD03L7Vv/xqbMFkIy4NRZVxdXfXyyy+rc+fOatWq1RUf3vzkk0/q4YcfVocOHdShQwfHfaPNmjXTvHnzNHz4cOXl5UmSpkyZorZt25b4fGFhoUaOHKnMzEyZpqmxY8eqUaNGuvvuuzV06FAtX75cb7/9tmJiYjRs2DA1btxYPXv21Pfff3/ZuoKCgnTvvffK399fzZs3V0hISLm+h1deeUXDhw/X+++/r9DQULVo0UINGzYs1xgAAACoOC1bDCCkWphhmqazaygTu91uVsQzOgFny8vLk4uLi+rWrastW7boySefVHJysrPLAgAAAJzGMIxE0zTtpR1jphWoQnvi47Ro1j8064s1quPiokbNmmvu/PedXRYAAABgWYRWoIrsiY/T6tn/UP2CPP2+d7gkqe51bvLIzXJyZQAAAIB1sRETUEXiP56vgrN5JdoKzuYp/uP5TqoI1+rUqVP617/+dVWfnT59us6cOVPBFQEAANQ8hFagipw+kVGudlgfoRUAAKDysTwYqCINb2iq0xnHS21H9fTiiy/qwIEDCggIUK9evdS8eXN98sknysvL06BBg/Tqq68qOztb99xzj3766ScVFhZq0qRJOnr0qNLT0xUREaGmTZsqLi7O2ZcCAABgWYRWoIqE3/egVs/+R4klwnWvc1P4fQ86sSpci7/+9a/auXOnkpOTtXr1ai1atEjbtm2TaZrq37+/NmzYoOPHj8vLy0srV66UVPxcYE9PT7355puKi4tT06b80QIAAOByWB4MVJEO4RHqPeYZNWzaTDIMNWzaTL3HPKMO4RHOLg0VYPXq1Vq9erUCAwMVFBSk1NRU7du3T35+fvrqq680fvx4xcfHy9PT09mlAgAAVCvMtAJVqEN4BCG1hjJNUxMmTNDjjz9+0bGkpCR98cUXmjhxoiIjI/Xyyy87oUIAAIDqiZlWALhKDRs21OnTpyVJUVFRmjt3rrKyih9hdPjwYR07dkzp6emqX7++Ro4cqejoaCUlJV30WQAAAFwaM60AcJVuuOEGhYWFydfXV3feeafuv/9+hYaGSpI8PDz0wQcfaP/+/YqOjladOnXk6uqqmTNnSpLGjBmjPn36yMvLi42YAAAALsMwTdPZNZSJ3W43ExISnF0GAAAAAKCCGYaRaJqmvbRjzLQCQBVZ/PNJvX7wiA7n5auVm6smtG6pIS2aOLssAAAASyO0AkAVWPzzSb2w95ByiopXt/yUl68X9h6SJIIrAADAZbAREwBUgdcPHnEE1vNyiky9fvCIkyoCAACoHgitAFAFDufll6sdAAAAxQitAFAFWrm5lqsdAAAAxQitAFAFJrRuKfc6Rok29zqGJrRu6aSKAAAAqgc2YgKAKnB+syV2DwYAACgfQisAVJEhLZoQUgEAAMqJ5cEAAAAAAMsitAIAAAAALIvQCgAAAACwLEIrAAAAAMCyCK0AAAAAAMsitAIAAAAALIvQCgAAAACwLEIrAAAAAMCyCK0AAAAAAMsitAIAAAAALIvQCgAAAACwLEIrAAAAAMCyCK0AAAAAAMsitAIAAAAALIvQCgAAAACwLEIrAAAAAMCyCK0AAAAAAMsitAIAAAAALIvQCgAAAACwLEIrAKBaiomJUWxsbIWOmZCQoLFjx162T1pamj788MNyfQYAAFy9us4uAAAAKygoKJDdbpfdbr9sv/Oh9f7775ekMn0GAABcPWZaAQCWkZ2drb59+8rf31++vr5auHChvL29lZGRIal4VrNHjx6O/tu3b1doaKjatGmjOXPmSJKOHDmi7t27KyAgQL6+voqPj5ckrVq1SkFBQfL391dkZKSk4tnaBx54QGFhYXrggQe0fv169evXr8Sx347/4osvKj4+XgEBAZo2bVqJz5w8eVIDBw6UzWZT165dlZKS4hhr9OjR6tGjh1q3bq0ZM2ZU/pcJAEANwUwrAMAyVq1aJS8vL61cuVKSlJmZqfHjx1+yf0pKirZu3ars7GwFBgaqb9+++uijjxQVFaWXXnpJhYWFOnPmjI4fP67HHntMGzZskI+Pj06ePOkYY/fu3dq4caPc3d21fv36K47/17/+VbGxsfr8888lqcRnXnnlFQUGBmrZsmVat26dHnzwQSUnJ0uSUlNTFRcXp9OnT6tdu3Z68skn5erqWjFfHAAANRgzrQAAy/Dz89NXX32l8ePHKz4+Xp6enpftP2DAALm7u6tp06aKiIjQtm3bFBISonfffVcxMTHasWOHGjZsqK1bt6p79+7y8fGRJDVp0sQxRv/+/eXu7l7m8S9n48aNeuCBByRJPXv21IkTJ/Trr79Kkvr27Ss3Nzc1bdpUzZs319GjR8v8vQAAUJsRWgEAltG2bVslJSXJz89PEydO1OTJk1W3bl0VFRVJknJzc0v0Nwzjovfdu3fXhg0b1KpVK40aNUrz58+/7DkbNGhwyWOljX+13NzcHK9dXFxUUFBw1WMBAFCbEFoBAJaRnp6u+vXra+TIkYqOjlZSUpK8vb2VmJgoSVq8eHGJ/suXL1dubq5OnDih9evXKyQkRD/88INuvPFGPfbYY3r00UeVlJSkrl27asOGDfr+++8lqcTy4MspbfyGDRvq9OnTpfYPDw/XggULJBUvG27atKmuv/76q/06AACAuKcVAGAhO3bsUHR0tOrUqSNXV1fNnDlTOTk5euSRRzRp0qQSmzBJks1mU0REhDIyMjRp0iR5eXnpvffe09SpU+Xq6ioPDw/Nnz9fzZo10+zZszV48GAVFRWpefPm+uqrr65YT2njN2vWTC4uLvL399eoUaMUGBjo6H9+wyWbzab69evrvffeq+ivCACAWscwTdPZNZSJ3W43ExISnF0GAKCWiImJkYeHh1544QVnlwIAQI1nGEaiaZqlPkOOmVYAACpR9rfH9OuXaSo8lSeXRm66PspbDQKbO7ssAACqDUIrAACliImJueYxsr89plNL9snML95IqvBUnk4t2SdJBFcAAMqIjZgAAKgkv36Z5gis55n5Rfr1yzTnFAQAQDVEaAUAoJIUnsorVzsAALgYoRUAgEri0sitXO0AAOBihFYAACrJ9VHeMlxL/l+t4VpH10d5O6cgAACqIUIrAACVpEFgczUa3MYxs+rSyE2NBrdhEybAIubNm6f09HTHe29vb2VkZDixIgClYfdgAAAqUYPA5oRUwKLmzZsnX19feXl5XfNYBQUFqluXX62BysBMKwAAAKqFqVOnasaMGZKkcePGqWfPnpKkdevWacSIEVq9erVCQ0MVFBSkYcOGKSsrS5I0efJkhYSEyNfXV2PGjJFpmlq0aJESEhI0YsQIBQQEKCcnR5L09ttvKygoSH5+fkpNTZUkZWdna/To0ercubMCAwO1fPlyScWht3///urZs6ciIyOr+usAao1KC62GYcQYhnHYMIzkc//uuuDYBMMw9huGsdcwjKjKqgEAAAA1R3h4uOLj4yVJCQkJysrKUn5+vuLj42Wz2TRlyhStWbNGSUlJstvtevPNNyVJzzzzjL755hvt3LlTOTk5+vzzzzV06FDZ7XYtWLBAycnJcnd3lyQ1bdpUSUlJevLJJxUbGytJeu2119SzZ09t27ZNcXFxio6OVnZ2tiQpKSlJixYt0v/+9z8nfCNA7VDZaximmaYZe2GDYRgdJd0nqZMkL0lrDMNoa5pmYSXXAgAAgGosODhYiYmJ+vXXX+Xm5qagoCAlJCQoPj5e/fv31+7duxUWFiZJOnv2rEJDQyVJcXFxeuONN3TmzBmdPHlSnTp10t13313qOQYPHuw415IlSyRJq1ev1meffeYIsbm5ufrxxx8lSb169VKTJk0q9bqB2s4ZC+8HSPrYNM08Sd8bhrFfUmdJW5xQCwAAAKoJV1dX+fj4aN68eerWrZtsNpvi4uK0f/9++fj4qFevXvroo49KfCY3N1dPPfWUEhISdPPNNysmJka5ubmXPIeb27mN01xcVFBQIEkyTVOLFy9Wu3btSvT9+uuv1aBBgwq+SgC/Vdn3tD5jGEaKYRhzDcNofK6tlaRDF/T56VwbAAAAcFnh4eGKjY1V9+7dFR4erlmzZikwMFBdu3bVpk2btH//fknF96F+9913joDatGlTZWVladGiRY6xGjZsqNOnT1/xnFFRUXr77bdlmqYk6dtvv62EKwNwKdcUWg3DWGMYxs5S/g2QNFPSrZICJB2R9PerGH+MYRgJhmEkHD9+/FpKBQAAQA0QHh6uI0eOKDQ0VDfeeKPq1aun8PBwNWvWTPPmzdPw4cNls9kUGhqq1NRUNWrUSI899ph8fX0VFRWlkJAQx1ijRo3SE088UWIjptJMmjRJ+fn5stls6tSpkyZNmlQVlwrgHOP8X4wq9SSG4S3pc9M0fQ3DmCBJpmm+fu7Yl5JiTNO87PJgu91uJiQkVHqtAAAAAICqZRhGomma9tKOVebuwS0veDtI0s5zrz+TdJ9hGG6GYfhIaiNpW2XVAQAAAFSYlE+kab5STKPinymfOLsioMarzI2Y3jAMI0CSKSlN0uOSZJrmLsMwPpG0W1KBpKfZORgAAACWl/KJtGKslH9uKXHmoeL3kmS7x3l1ATVclSwPrggsDwYAAIBTTfMtDqq/5XmzNG7nxe0Ayswpy4MBAACAGiXzp/K1A6gQhFYAAIBzPDw8rupz69evV79+/Sq4GliO503lawdQIQitAAAAQFlEviy5updsc3UvbgdQaQitAAAAv2GapqKjo+Xr6ys/Pz8tXLjwsu0X+uabbxQYGKgDBw5UddmobLZ7pLtnFN/DKqP4590z2IQJqGSVuXswAABAtbRkyRIlJydr+/btysjIUEhIiLp3767NmzeX2n7e5s2b9eyzz2r58uX63e9+58QrQKWx3UNIBaoYM60AAAC/sXHjRg0fPlwuLi668cYbdfvtt+ubb765ZLsk7dmzR2PGjNGKFSsIrABQgQitAAAAFaBly5aqV6+evv32W2eXAgA1CqEVAADgN8LDw7Vw4UIVFhbq+PHj2rBhgzp37nzJdklq1KiRVq5cqQkTJmj9+vXOvQAAqEG4pxUAAOA3Bg0apC1btsjf31+GYeiNN95QixYtLtmempoqSbrxxhv1+eef684779TcuXPVpUsXJ18JAFR/hmmazq6hTOx2u5mQkODsMgAAAAAAFcwwjETTNO2lHWOmFQAA4Cod+Xm5Dh6IVW7eEdVza6nWt76gli0GOLssAKhRCK0AAABX4cjPy5Wa+pKKinIkSbl56UpNfUmSCK4AUIHYiAkAAOAqHDwQ6wis5xUV5ejggVgnVQQANROhFQAA4Crk5h0pVzsA4OoQWgEAAK5CPbeW5WoHAFwdQisAAMBVaH3rC6pTx71EW5067mp96wtOqggAaiY2YgIAALgK5zdbYvdgAKhchFYAAICr1LLFAEIqAFQylgcDAAAAACyL0AoAAAAAsCxCKwAAAADAsgitAAAAAADLIrQCAAAAACyL0AoAAAAAsCxCKwAAAADAsgitAAAAAADLIrQCAAAAACyL0AoAAAAAsCxCKwAAAADAsgitAAAAAADLIrRWsOnTp+vMmTOO93fddZdOnTolSZoxY4Y6dOigESNG6LPPPtNf//rXy47VrVu3K57Pw8PjmuoFAAAAACszTNN0dg1lYrfbzYSEBGeXcVmFhYW69dZblZCQoKZNm150vH379lqzZo1uuummCjunh4eHsrKyKmw8AAAAAKhqhmEkmqZpL+0YM63lMHDgQAUHB6tTp06aPXu2pOLQ+Ic//EH+/v567bXXlJ6eroiICEVEREiSvL29lZGRoSeeeEIHDx7UnXfeqWnTpmnevHl65plnJElHjx7VoEGD5O/vL39/f23evNkxtiRlZWUpMjJSQUFB8vPz0/Lly51w9QAAAABQ9eo6u4DqZO7cuWrSpIlycnIUEhKiIUOGKDs7W126dNHf//53R5+4uLiLZlpnzZqlVatWOY7NmzfPcWzs2LG6/fbbtXTpUhUWFl40c1qvXj0tXbpU119/vTIyMtS1a1f1799fhmFU+jUDAAAAgDMRWsthxowZWrp0qSTp0KFD2rdvn1xcXDRkyJBrGnfdunWaP3++JMnFxUWenp4ljpumqT/96U/asGGD6tSpo8OHD+vo0aNq0aLFNZ0XAAAAAKyO0FpG69ev15o1a7RlyxbVr19fPXr0UG5ururVqycXF5dKPfeCBQt0/PhxJSYmytXVVd7e3srNza3UcwIAAACAFXBPaxllZmaqcePGql+/vlJTU7V169ZS+zVs2FCnT58u19iRkZGaOXOmpOLNnDIzMy86d/PmzeXq6qq4uDj98MMPV3cRAAAAAFDNEFrLqE+fPiooKFCHDh304osvqmvXrqX2GzNmjPr06ePYiKks3nrrLcXFxcnPz0/BwcHavXt3ieMjRoxQQkKC/Pz8NH/+fLVv3/6argUAAABA7XThIzmrCx55AwAAAABwqss98oZ7WquZlJQUrV27VpmZmfL09FRkZKRsNpuzywIAAABgMQMHDtShQ4eUm5ur5557TmPGjJG3t7cSEhIuetqJlRFaq5GUlBStWLFC+fn5korvdV2xYoUkEVwBAAAAlFDaIzurI+5prUbWrl3rCKzn5efna+3atU6qCAAAAIBVzZgxQ/7+/uratavjkZ3VETOt1chvdxW+UjsAAACA2ulSj+ysjphprUY8PT3L1Q4AAACgdirrIzurA0JrNRIZGSlXV9cSba6uroqMjHRSRQAAAACsqKyP7KwOWB5cjZzfbIndgwEAAABcjpubm/773/9e1J6Wllb1xVwjQms1Y7PZCKkAAAAAymTZt4c19cu9Sj+VI69G7oqOaqeBga2cXVa5EFoBAAAAoAZa9u1hTViyQzn5hZKkw6dyNGHJDkmqVsGVe1oB4BK6det2xT7Tp0/XmTNnrtivR48eSkhIkCR5e3srIyOjzOcAAAC4GlO/3OsIrOfl5Bdq6pd7nVTR1SG0AsAlbN68+Yp9yhpar+UcAAAAVyP9VE652q2K0AoAl+Dh4SGp+DlnPXr00NChQ9W+fXuNGDFCpmlqxowZSk9PV0REhCIiIiRJq1evVmhoqIKCgjRs2DBlZWWV6RxFRUV66qmn1L59e/Xq1Ut33XWXFi1aJElKTEzU7bffruDgYEVFRenIkSOSimdvx48fr86dO6tt27aKj4+XJBUWFuqFF16Qr6+vbDab3n777cuOAwAAaiavRu7larcqQisAlMG3336r6dOna/fu3Tp48KA2bdqksWPHysvLS3FxcYqLi1NGRoamTJmiNWvWKCkpSXa7XW+++WaZxl+yZInS0tK0e/duvf/++9qyZYskKT8/X88++6wWLVqkxMREjR49Wi+99JLjcwUFBdq2bZumT5+uV199VZI0e/ZspaWlKTk5WSkpKRoxYsQVxwEAADVPdFQ7ubu6lGhzd3VRdFQ7J1V0ddiICQDKoHPnzrrpppskSQEBAUpLS9Ntt91Wos/WrVu1e/duhYWFSZLOnj2r0NDQMo2/ceNGDRs2THXq1FGLFi0cM7d79+7Vzp071atXL0nFs6gtW7Z0fG7w4MGSpODgYMcW9mvWrNETTzyhunWL/xPfpEkT7dy587LjAACAmuf8ZkvsHgygynh4eFxxuemlPProo/r973+vjh07VnBVtYObm5vjtYuLiwoKCi7qY5qmevXqpY8++qjCzmuapjp16uSYeb1UXZeqqazjAACAmmlgYKtqF1J/i+XBQC3x73//m8BaCRo2bKjTp09Lkrp27apNmzZp//79kqTs7Gx99913ZRonLCxMixcvVlFRkY4ePar169dLktq1a6fjx4+XWC68a9euy47Vq1cvvfPOO44Qe/LkyasaBwAAwAoIrUA1ZJqmoqOj5evrKz8/Py1cuFDS5TfzufCRK6tWrVJQUJD8/f0VGRnptOuoCcaMGaM+ffooIiJCzZo107x58zR8+HDZbDaFhoYqNTW1TOMMGTJEN910kzp27KiRI0cqKChInp6euu6667Ro0SKNHz9e/v7+CggIuOKOw48++qh+97vfyWazyd/fXx9++OFVjQMAAGAFhmmazq6hTOx2u3n+F26gtjq/PHjx4sWaNWuWVq1apYyMDIWEhOjrr7/Wpk2bNHfuXH3++ec6duyYOnTooDlz5mjo0KHq0aOHYmNjdcsttygoKEgbNmyQj4+PTp48qSZNmjj70iApKytLHh4eOnHihDp37qxNmzapRYsWFTL2kZ+X6+CBWOXmHVE9t5ZqfesLatliQIWMDQAAcK0Mw0g0TdNe2jHuaQWqoY0bN2r48OFycXHRjTfeqNtvv13ffPPNJTfzudDWrVvVvXt3+fj4SBKB1UL69eunU6dO6ezZs5o0aVKFBtbU1JdUVFT8TLbcvHSlphbvHExwBQAAVkdoBQCLOH8fa0U7eCDWEVjPKyrK0cEDsYRWAABgedzTClRD4eHhWrhwoQoLC3X8+HFt2LBBnTt3vuRmPhfq2rWrNmzYoO+//15S8SY9qNly846Uqx0AAMBKmGkFqqFBgwZpy5Yt8vf3l2EYeuONN9SiRQsNGTJEa9euVceOHXXzzTc7NvO5ULNmzTR79mwNHjxYRUVFat68ub766isnXQmqQj23lsrNSy+1HQAAwOrYiAmoYS61mU/2t8f065dpKjyVJ5dGbro+ylsNAps7u1xUgd/e0ypJdeq4q33711geDAAALIGNmIBapLTNfLK/PaZTS/bJzC+SJBWeytOpJfskieBaC5wPpuweDAAAqiNCK1DDlHYf669fpjkC63lmfpF+/TKN0FpLtGwxgJAKAACqJTZiAmqBwlN55WoHAAAArILQCtQCLo3cytUOAAAAWAWhFagFro/yluFa8n/uhmsdXR/l7ZyCAAAAgDLinlagFjh/3yq7BwMAAKC6IbQCtUSDwOaEVAAAAFQ7LA8GAAAAAFgWoRUAAAAAYFmEVgAAAACAZRFaAQAAAACWRWgFAAAAAFgWoRUAAAAAYFmEVgAAAACAZRFaAQAAAACWRWgFAAAAAFgWoRUAAAAAYFmEVgAAAACAZRFaAQAAAACWRWgFAAAAAFgWoRUAAAAAYFmEVgAAAACAZRFaAQAAAACWRWgFAAAAAFgWoRUAAAAAYFmEVgAAAACAZRFaAQAAAACWRWgFAAAAAFjWNYVWwzCGGYaxyzCMIsMw7L85NsEwjP2GYew1DCPqgvY+59r2G4bx4rWcHwAAAABQs13rTOtOSYMlbbiw0TCMjpLuk9RJUh9J/zIMw8UwDBdJ/5R0p6SOkoaf6wsAAAAAwEXqXsuHTdPcI0mGYfz20ABJH5ummSfpe8Mw9kvqfO7YftM0D5773Mfn+u6+ljoAAAAAADVTZd3T2krSoQve/3Su7VLtpTIMY4xhGAmGYSQcP368UgoFAAAAAFjXFWdaDcNYI6lFKYdeMk1zecWX9H9M05wtabYk2e12szLPBQAAAACwniuGVtM077iKcQ9LuvmC9zeda9Nl2gEAAAAAKKGylgd/Juk+wzDcDMPwkdRG0jZJ30hqYxiGj2EY16l4s6bPKqkGAAAsIzs7W3379pW/v798fX21cOFCeXt7KyMjQ5KUkJCgHj16SJJiYmI0evRo9ejRQ61bt9aMGTMkSWlpaerQoYMee+wxderUSb1791ZOTo4OHDigoKAgx7n27dtX4j0AANXZtT7yZpBhGD9JCpW00jCMLyXJNM1dkj5R8QZLqyQ9bZpmoWmaBZKekfSlpD2SPjnXFwCAGm3VqlXy8vLS9u3btXPnTvXp0+ey/VNTU/Xll19q27ZtevXVV5Wfny+pOJA+/fTT2rVrlxo1aqTFixfr1ltvlaenp5KTkyVJ7777rh5++OHKviQAAKrENYVW0zSXmqZ5k2mabqZp3miaZtQFx14zTfNW0zTbmab53wvavzBNs+25Y69dy/kBAKgu/Pz89NVXX2n8+PGKj4+Xp6fnZfv37dtXbm5uatq0qZo3b66jR49Kknx8fBQQECBJCg4OVlpamiTp0Ucf1bvvvqvCwkItXLhQ999/f2VeDgAAVaaylgcDAIALtG3bVklJSfLz89PEiRM1efJk1a1bV0VFRZKk3NzcEv3d3Nwcr11cXFRQUHDZ9iFDhui///2vPv/8cwUHB+uGG26o7EsCAKBKEFoBAKgC6enpql+/vkaOHKno6GglJSXJ29tbiYmJkqTFixdf0/j16tVTVFSUnnzySZYGAwBqlCvuHgwAAK7djh07FB0drTp16sjV1VUzZ85UTk6OHnnkEU2aNMmxCdO1GDFihJYuXarevXtfe8EAAFiEYZrV4/GndrvdTEhIcHYZAABYVmxsrDIzM/XnP//Z2aUAAFAuhmEkmqZpL+0Yy4MBAKjmFv98Uo1vj9SEmbO1IjRKi38+6eySAACoMCwPBgCgGlv880m9sPeQ3GL+LjdJP0t6Ye8hSdKQFk2cWhsAABWBmVYAAKqx1w8eUU5RyVt9copMvX7wiJMqAgCgYhFaAQCoxg7n5ZerHQCA6obQCgBANdbKzbVc7QAAVDeEVgAAqrEJrVvKvY5Ros29jqEJrVs6qSIAACoWGzEBAFCNnd9s6fWDR3Q4L1+t3Fw1oXVLNmECANQYhFYAAKq5IS2aEFIBADUWy4MBAACqiYSEBI0dO1aSFBMTo9jY2Iv6pKWlydfXt6pLA4BKw0wrAABANWG322W3251dBgBUKWZaAQAAKlhaWprat2+vESNGqEOHDho6dKjOnDmjtWvXKjAwUH5+fho9erTy8vIkSS+++KI6duwom82mF154QZL06aefytfXV/7+/urevbskaf369erXr5/jPNu3b1doaKjatGmjOXPmXFRHYWGhoqOjFRISIpvNpnfeeacKrh4AKhYzrQAAAJVg7969+s9//qOwsDCNHj1ab775pt555x2tXbtWbdu21YMPPqiZM2fqgQce0NKlS5WamirDMHTq1ClJ0uTJk/Xll1+qVatWjrbfSklJ0datW5Wdna3AwED17du3xPH//Oc/8vT01DfffKO8vDyFhYWpd+/e8vHxqeSrB4CKw0wrAABAJbj55psVFhYmSRo5cqTWrl0rHx8ftW3bVpL00EMPacOGDfL09FS9evX0yCOPaMmSJapfv74kKSwsTKNGjdKcOXNUWFhY6jkGDBggd3d3NW3aVBEREdq2bVuJ46tXr9b8+fMVEBCgLl266MSJE9q3b18lXjUAVDxmWgEAACqBYZR8fm6jRo104sSJi/rVrVtX27Zt09q1a7Vo0SL94x//0Lp16zRr1ix9/fXXWrlypYKDg5WYmHjFc/z2vWmaevvttxUVFVUBVwQAzsFMKwAAQCX48ccftWXLFknShx9+KLvdrrS0NO3fv1+S9P777+v2229XVlaWMjMzddddd2natGnavn27JOnAgQPq0qWLJk+erGbNmunQoUMXnWP58uXKzc3ViRMntH79eoWEhJQ4HhUVpZkzZyo/P1+S9N133yk7O7syLxsAKhwzrQAAAJWgXbt2+uc//6nRo0erY8eOmjFjhrp27aphw4apoKBAISEheuKJJ3Ty5EkNGDBAubm5Mk1Tb775piQpOjpa+/btk2maioyMlL+/v/73v/+VOIfNZlNERIQyMjI0adIkeXl5KS0tzXH80UcfVVpamoKCgmSappo1a6Zly5ZV4bcAKxs1apT69eunoUOHOrsU4LIM0zSdXUOZ2O12MyEhwdllAAAAXFFaWpr69eunnTt3OrsU4JIIrbASwzASTdMs9ZleLA8GAACoiVI+kab5SjGNin+mfOLsiuAkaWlp6tChgx577DF16tRJvXv3Vk5OTok+3t7eysjIkCQlJCSoR48eTqgUKB2hFQAAoIJ5e3s7d5Y15RNpxVgp85Aks/jnirEE11ps3759evrpp7Vr1y41atRIixcvdnZJQJkRWgEAAGqatZOl/JIzacrPKW5HreTj46OAgABJUnBwcIl7nwGrI7QCAADUNJk/la8dNZ6bm5vjtYuLiwoKCkocr1u3roqKiiRJubm5VVobcCWEVgAAgJrG86bytaPW8/b2djwLmKXDsBpCKwAAQE0T+bLk6l6yzdW9uB0oxSuvvKLnnntOdrtdLi4uzi4HKIFH3gAAANREKZ8U38Oa+VPxDGvky5LtHmdXBQClutwjb+pWdTEAAACoArZ7CKkok+++/llblh9Q1sk8eTRxU+iAW9W2SwtnlwU4EFoBAACAWuq7r39W3IJUFZwt3oQp62Se4hakShLBFZbBPa0AAABALbVl+QFHYD2v4GyRtiw/4KSKgIsRWgEAAIBaKutkXrnaAWcgtAIAAAC1lEcTt3K1A85AaAUAAABqqdABt6rudSUjQd3r6ih0wK1Oqgi4GBsxAQAAALXU+c2W2D0YVkZoBQAAAGqxtl1aEFJhaSwPBgAAAABYFqEVAAAAAGBZhFYAAAAAgGURWgEAAAAAlkVoBQAAAABYFqEVwP9v786jrKruvI0/m2KoIIiI2rJQAbtBpiooqkAQQdAwmCACUdE2r/D6Joq03egyDQKtOPRgt7xqHGhjlixinCViRJM3SNJRiNIFBSUiQwrsShwqIiIloCBV7PePulSKUMVgDfdQPJ+1anHv3mf43bvXWfd+OefsK0mSJCWWoVWSJEmSlFiGVkmSJElSYhlaJUmSJEmJZWiVJEmSJCWWoVWSJEmSlFiGVkmSJElSYhlaJUmSJEmJZWiVJEmSJCWWoVWSJEmSlFiGVkmSJElSYhlaJUmSJEmJZWiVJEmSJCWWoVWSJEmSlFiGVkmSJElSYhlaJUmSJEmJZWiVJEmSJCWWoVWSJEmSlFiGVkmSJElSYhlaJUmSJEmJZWiVJEmSJCWWoVWSJEmSlFiGVkmSJElSYhlaJUmSJEmJZWiVJEmSJCWWoVWSJEmSlFiGVklKo+3btzN37tyjXu/2229nyZIl9VCRJElSsoQYY7prOCJ5eXlx5cqV6S5DkupUcXExo0ePZu3atUe8Tnl5ORkZGfVYlSRJUsMKIRTEGPOq6/NMqySl0a233srmzZvp06cP/fr1Y/To0ZV9N954I/PnzwegU6dOTJ8+nb59+/LCCy8wadIkFixYUNk3e/Zs+vbtS1ZWFhs2bADgk08+Yfjw4fTs2ZPvfe97dOzYka1btzb4a5QkSaoNQ6skpdE999zDX//1X1NYWMi99957yGXbtWvHqlWruPLKKw/qO+WUU1i1ahU33HADc+bMAeDOO+/kwgsv5N133+Wyyy7jj3/8Y728BkmSpPpkaJWkY8SECRNq7Bs/fjwAubm5FBcXA7Bs2bLKgDtq1Cjatm1b7zVKkiTVNUOrJCVE06ZN2bdvX+Xz3bt3H9B/wgkn1LhuixYtAMjIyKCsrKx+CpQkSUoDQ6skpVHr1q3ZsWMHAB07dmTdunXs2bOH7du38+tf/7pW2x40aBDPP/88AIsXL+azzz6rdb2SJEkNrWm6C5Ck41m7du0YNGgQvXr14uKLL+aKK66gV69edO7cmZycnFpte/bs2Vx11VX89Kc/ZeDAgZx++um0bt26jiqXJElqGP7kjSQ1Ui+tf4mH1zzMlt1byPwgk61PbeW9de+luyxJkqSDHOonbzzTKkmN0Kvvvcptr97Gpoc2QYTQNNBpUidefe9Vvn32t9NdniRJ0hEztEpSI/TDVT+EU+Fv7vqbg9oNrZIk6VjiREyS1Aj9adefjqpdkiQpqQytktQInX7C6UfVLkmSlFSGVklqhKb2nUpmRuYBbZkZmUztOzVNFUmSJH093tMqSY3Q/vtWf7jqh/xp1584/YTTmdp3qvezSpKkY46hVZIaqW+f/W1DqiRJOuZ5ebAkSZIkKbEMrZIkSZKkxDK0SpIkSZISy9AqSZIkSUosQ6skSZIkKbEMrZIkSZKkxKpVaA0hXB5CeDeEsC+EkFelvVMI4csQQmHq79EqfbkhhHdCCJtCCA+GEEJtapBqo1WrVvW+j0cffZQnnnii3vcjSZIkNUa1/Z3WtcB44EfV9G2OMfappv0/ge8D/w38AhgF/LKWdUhpVV5eTkZGRrV9kydPbuBqJEmSpMajVmdaY4zrY4wbj3T5EEJ74MQY4/IYYwSeAMbWpgaprtx7773069eP7OxsZs+eXdk+duxYcnNz6dmzJ4899lhle6tWrbjlllvo3bs3b731Fq1atWLWrFn07t2bAQMG8PHHHwNwxx13MGfOHACGDh3K9OnT6d+/P127dmXp0qUAfPHFF1xxxRX06NGDcePGce6557Jy5coGfPWSJElSMtXnPa2dQwirQwivhxAGp9o6AB9UWeaDVJuUVosXL6aoqIj8/HwKCwspKCjgjTfeAGDevHkUFBSwcuVKHnzwQT799FMAdu3axbnnnsvbb7/N+eefz65duxgwYABvv/02Q4YM4cc//nG1+yorKyM/P58HHniAO++8E4C5c+fStm1b1q1bx913301BQUHDvHBJkiQp4Q57eXAIYQlwejVds2KMP69htRLgrBjjpyGEXOClEELPoy0uhHAdcB3AWWeddbSrS0ds8eLFLF68mJycHAB27txJUVERQ4YM4cEHH2ThwoUAvP/++xQVFdGuXTsyMjL4zne+U7mN5s2bM3r0aAByc3N57bXXqt3X+PHjK5cpLi4GYNmyZUydOhWAXr16kZ2dXS+vU5IkSTrWHDa0xhi/ebQbjTHuAfakHheEEDYDXYEPgTOqLHpGqq2m7TwGPAaQl5cXj7YO6UjFGJkxYwbXX3/9Ae2//e1vWbJkCW+99RYtW7Zk6NCh7N69G4DMzMwD7mNt1qwZ++cVy8jIoKysrNp9tWjR4rDLSJIkSapQL5cHhxBODSFkpB6fDXQB3osxlgCfhxAGpGYNvgao6Wyt1GBGjhzJvHnz2LlzJwAffvghW7ZsobS0lLZt29KyZUs2bNjA8uXL62X/gwYN4vnnnwdg3bp1vPPOO/WyH0mSJOlYU6vZg0MI44CHgFOBV0MIhTHGkcAQ4K4Qwl5gHzA5xrgttdoUYD7wDSpmDXbmYKXdiBEjWL9+PQMHDgQqJll68sknGTVqFI8++ijdu3fnnHPOYcCAAfWy/ylTpjBx4kR69OhBt27d6NmzJ23atKmXfUmSJEnHklAxiW/y5eXlRWdTVWNVXl7O3r17yczMZPPmzXzzm99k48aNNG/ePN2lSZIkSfUuhFAQY8yrrq+2v9MqqZbWL/0vXnvice792SvQpAkt27Rl7ty5BlZJkiQJQ6uUVuuX/heLH3uYsq/2cNPw8wFo2rwFnVplprkySZIkKRnq83daJR3G0mefoOyrPQe0lX21h6XPPpGmiiRJkqRkMbRKabTj061H1S5JkiQdbwytUhq1bnfKUbVLkiRJxxtDq5RGg6+8hqbNWxzQ1rR5CwZfeU2aKpIkSZKSxYmYpDTqPngYUHFv645Pt9K63SkMvvKaynZJkiTpeGdoldKs++BhhlRJkiSpBl4eLEmSJElKLEOrJEmSJCmxDK2SJEmSpMQytEqSJEmSEsvQKkmSJElKLEOrJEmSJCmxDK2SJEmSpMQytEqSJEmSEsvQKkmSJElKLEOrJEmSJCmxDK2SJEmSpMQytEqSJEmSEsvQKkmSqtWpUye2bt16UPvLL7/MPffck4aKJEnHo6bpLkCSJB1bxowZw5gxY9JdhiTpOOGZVkmSGoHi4mK6devGpEmT6Nq1K1dffTVLlixh0KBBdOnShfz8fPLz8xk4cCA5OTmcd955bNy4EYDy8nJ+8IMf0KtXL7Kzs3nooYcqt/vQQw/Rt29fsrKy2LBhAwDz58/nxhtvBGDSpEn8wz/8A+eddx5nn302CxYsqFz33nvvpV+/fmRnZzN79uwGfDckSY2JoVWSpEZi06ZN3HLLLWzYsIENGzbw9NNPs2zZMubMmcO//uu/0q1bN5YuXcrq1au56667mDlzJgCPPfYYxcXFFBYWsmbNGq6++urKbZ5yyimsWrWKG264gTlz5lS735KSEpYtW8Yrr7zCrbfeCsDixYspKioiPz+fwsJCCgoKeOONN+r/TZAkNTpeHixJUiPRuXNnsrKyAOjZsycXXXQRIQSysrIoLi6mtLSUiRMnUlRURAiBvXv3ArBkyRImT55M06YVXwtOPvnkym2OHz8egNzcXF588cVq9zt27FiaNGlCjx49+Pjjj4GK0Lp48WJycnIA2LlzJ0VFRQwZMqR+XrwkqdEytEqS1Ei0aNGi8nGTJk0qnzdp0oSysjJuu+02hg0bxsKFCykuLmbo0KFHvM2MjAzKysoOu98YY+W/M2bM4Prrr/+6L0eSJMDLgyVJOm6UlpbSoUMHoOK+1P2GDx/Oj370o8pQum3btlrva+TIkcybN4+dO3cC8OGHH7Jly5Zab1eSdPwxtEqSdJyYNm0aM2bMICcn54Czpt/73vc466yzyM7Opnfv3jz99NO13teIESP427/9WwYOHEhWVhaXXXYZO3bsqPV2JUnHn7D/Mp6ky8vLiytXrkx3GZIkSZKkOhZCKIgx5lXX55lWSZJUp0oXLaLowotY370HRRdeROmiRekuSZJ0DHMiJkmSVGdKFy2i5Lbbibt3A1D20UeU3HY7AG0uuSSdpUmSjlGeaZUkSXVmy/0PVAbW/eLu3Wy5/4H0FCRJOuYZWiVJUp0pKyk5qnZJkg7H0CpJkupM0/btj6pdkqTDMbRKkqQ6c9rNNxEyMw9oC5mZnHbzTekpSJJ0zHMiJkmSVGf2T7a05f4HKCspoWn79px2801OwiRJ+toMrZIkqU61ueQSQ6okqc54ebAkSZIkKbEMrZIkSZKkxDK0SpIkSZISy9AqSZIkSUosQ6skSZIkKbEMrZIkSZKkxDK0SpIkSZISy9AqSZIkSUosQ6skSZIkKbEMrZIkSZKkxDK0SpIkSZISy9AqSZJ0HCkvL6/V+mVlZXVUSd2o7euRlHyGVkmSpEaiuLiYbt26cfXVV9O9e3cuu+wyvvjiCzp16sT06dPp27cvL7zwAs888wxZWVn06tWL6dOnV67/+OOP07VrV/r378/3v/99brzxRgAmTZrE5MmTOffcc5k2bRr5+fkMHDiQnJwczjvvPDZu3AjA/PnzGTt2LMOHD6dTp048/PDD3HfffeTk5DBgwAC2bdsGwNChQ7n55pvJy8uje/furFixgvHjx9OlSxf+6Z/+qbKeJ598kv79+9OnTx+uv/76yoDaqlUrbrnlFnr37s1bb73VUG+vpDQxtEqSJDUiGzduZMqUKaxfv54TTzyRuXPnAtCuXTtWrVrFkCFDmD59Or/5zW8oLCxkxYoVvPTSS3z00UfcfffdLF++nN/97nds2LDhgO1+8MEHvPnmm9x3331069aNpUuXsnr1au666y5mzpxZudzatWt58cUXWbFiBbNmzaJly5asXr2agQMH8sQTT1Qu17x5c1auXMnkyZO59NJLeeSRR1i7di3z58/n008/Zf369Tz33HP87ne/o7CwkIyMDJ566ikAdu3axbnnnsvbb7/N+eef3wDvqqR0apruAiRJklR3zjzzTAYNGgTAd7/7XR588EEAJkyYAMCKFSsYOnQop556KgBXX301b7zxBgAXXHABJ598MgCXX345v//97yu3e/nll5ORkQFAaWkpEydOpKioiBACe/furVxu2LBhtG7dmtatW9OmTRsuueQSALKyslizZk3lcmPGjKls79mzJ+3btwfg7LPP5v3332fZsmUUFBTQr18/AL788ktOO+00ADIyMvjOd75TZ++ZpGQztEqSJDUiIYRqn59wwgm12m7V9W+77TaGDRvGwoULKS4uZujQoZV9LVq0qHzcpEmTyudNmjQ54H7Yqu1/uU5ZWRkxRiZOnMi//du/HVRLZmZmZYCW1Ph5ebAkSVIj8sc//rHyPs+nn376oMtn+/fvz+uvv87WrVspLy/nmWee4YILLqBfv368/vrrfPbZZ5SVlfGzn/2sxn2UlpbSoUMHoOI+1vpw0UUXsWDBArZs2QLAtm3b+MMf/lAv+5KUbIZWSZKkRuScc87hkUceoXv37nz22WfccMMNB/S3b9+ee+65h2HDhtG7d29yc3O59NJL6dChAzNnzqR///4MGjSITp060aZNm2r3MW3aNGbMmEFOTk69zSbco0cP/vmf/5kRI0aQnZ3N8OHDKSkpqZd9SUq2EGNMdw1HJC8vL65cuTLdZUiSJCVWcXExo0ePZu3atV9r/Z07d9KqVSvKysoYN24c1157LePGjavjKiXpYCGEghhjXnV93tMqSZIkAO644w6WLFnC7t27GTFiBGPHjk13SZV2rd7C578qpnz7HjJOasGJIztxQs5p6S5LUgPwTKskSZISbdfqLWx/sYi4d19lW2jWhJPGdzG4So3Eoc60ek+rJEmSEu3zXxUfEFgB4t59fP6r4vQUJKlBGVolSZKUaOXb9xxVu6TGxdAqSZKkRMs4qcVRtUtqXAytkiRJSrQTR3YiNDvwa2to1oQTR3ZKT0GSGpSzB0uSJCnR9k+25OzB0vHJ0CpJkqTEOyHnNEOqdJzy8mBJkiRJUmIZWiVJkiRJiWVolSRJkiQllqFVkiRJkpRYhlZJkiRJUmIZWiVJkiRJiWVolSRJkiQllqFVkiRJkpRYhlZJkiRJUmIZWiVJkiRJiWVolSRJkiQllqFVkiRJkpRYhlZJkiRJUmIZWiVJkiRJiWVolSRJkiQllqFVkiRJkpRYhlZJkiRJUmIZWiVJkiRJiWVolSRJkiQllqFVkiRJkpRYhlZJkiRJUmIZWiVJkiRJiWVolSRJkiQllqFVkiRJkpRYhlZJkiRJUmLVKrSGEO4NIWwIIawJISwMIZxUpW9GCGFTCGFjCGFklfZRqbZNIYRba7N/SZIkSVLjVtszra8BvWKM2cDvgRkAIYQewJVAT2AUMDeEkBFCyAAeAS4GegBXpZaVJEmSJOkgtQqtMcbFMcay1NPlwBmpx5cCz8YY98QY/wfYBPRP/W2KMb4XY/wKeDa1rCRJkiRJB6nLe1qvBX6ZetwBeL9K3weptpraJUmSJEk6SNPDLRBCWAKcXk3XrBjjz1PLzALKgKfqsrgQwnXAdQBnnXVWXW5akiRJknQMOGxojTF+81D9IYRJwGjgohhjTDV/CJxZZbEzUm0cor26fT8GPAaQl5cXa1pOkiRJktQ41Xb24FHANGBMjPGLKl0vA1eGEFqEEDoDXYB8YAXQJYTQOYTQnIrJml6uTQ2SJEmSpMbrsGdaD+NhoAXwWggBYHmMcXKM8d0QwvPAOiouG/67GGM5QAjhRuBXQAYwL8b4bi1rkCRJkiQ1UuHPV/QmW15eXly5cmW6y5CkenHHHXfQqlUrfvCDHyRye3WhsLCQjz76iG9961vpLkWSJCVMCKEgxphXXV9dzh4sSVKNCgsL+cUvfpHuMiRJ0jHG0CpJafIv//IvdO3alfPPP5+NGzcCsHnzZkaNGkVubi6DBw9mw4YNlJaW0rFjR/bt2wfArl27OPPMM9m7d2+1y/+lwsJCBgwYQHZ2NuPGjeOzzz4DYOjQoUydOpU+ffrQq1cv8vPzgYqztBMnTmTw4MF07NiRF198kWnTppGVlcWoUaPYu3cvAAUFBVxwwQXk5uYycuRISkpKKrc7ffp0+vfvT9euXVm6dClfffUVt99+O8899xx9+vThueeeq/f3V5IkNQ6GVklKg4KCAp599tnKs48rVqwA4LrrruOhhx6ioKCAOXPmMGXKFNq0aUOfPn14/fXXAXjllVcYOXIkzZo1q3b5v3TNNdfw7//+76xZs4asrCzuvPPOyr4vvviCwsJC5s6dy7XXXlvZvnnzZn7zm9/w8ssv893vfpdhw4bxzjvv8I1vfINXX32VvXv38vd///csWLCAgoICrr32WmbNmlW5fllZGfn5+TzwwAPceeedNG/enLvuuosJEyZQWFjIhAkT6uutlSRJjUxtJ2KSJH0NS5cuZdy4cbRs2RKAMWPGsHv3bt58800uv/zyyuX27NkDwIQJE3juuecYNmwYzz77LFOmTGHnzp01Lr9faWkp27dv54ILLgBg4sSJByx/1VVXATBkyBA+//xztm/fDsDFF19Ms2bNyMrKory8nFGjRgGQlZVFcXExGzduZO3atQwfPhyA8vJy2rdvX7nd8ePHA5Cbm0txcXGt3y9JknT8MrRKUkLs27ePk046icLCwoP6xowZw8yZM9m2bRsFBQVceOGF7Nq1q8blj1Rq5veDnrdo0QKAJk2a0KxZs8r2Jk2aUFZWRoyRnj178tZbb1W73f3rZ2RkUFZW9rXrkyRJ8vJgSUqDIUOG8NJLL/Hll1+yY8cOFi1aRMuWLencuTMvvPACADFG3n77bQBatWpFv379mDp1KqNHjyYjI4MTTzyxxuX3a9OmDW3btmXp0qUA/PSnP6086wpU3lu6bNky2rRpQ5s2bY6o/nPOOYdPPvmkMrTu3buXd9899C+YtW7dmh07dhzR9iVJkvYztEpSGvTt25cJEybQu3dvLr74Yvr16wfAU089xeOPP07v3r3p2bMnP//5zyvXmTBhAk8++eQB94Meavn9fvKTn/CP//iPZGdnU1hYyO23317Zl5mZSU5ODpMnT+bxxx8/4vqbN2/OggULmD59Or1796ZPnz68+eabh1xn2LBhrFu3zomYJEnSUfF3WiXpODV06FDmzJlDXl61P4kmSZLUYPydVklSWpQuWkTRhRexvnsPii68iNJFi9JdkiRJOsY4EZMkHad++9vf1uv2SxctouS224m7dwNQ9tFHlNxWcWlym0suqdd9S5KkxsMzrZKkerHl/gcqA+t+cfduttz/QHoKkiRJxyRDqySpXpSVlBxVuyRJUnUMrZKketG0ffujapckSaqOoVWSVC9Ou/kmQmbmAW0hM5PTbr4pPQVJkqRjkhMxSZLqxf7Jlrbc/wBlJSU0bd+e026+yUmYJEnSUTG0SpLqTZtLLjGkSpKkWvHyYEmSJElSYhlaJUmSJEmJZWiVJEmSJCWWoVWSJEmSlFiGVkmSJElSYhlaJUmSJEmJZWiVJEmSJCWWoVWSJEmSlFiGVkmSJElSYhlaJUmSJEmJZWiVJEmSJCWWoVWSJEmSlFiGVkmSJElSYhlaJUmSJEmJZWiVJEmSJCWWoVWSJEmSlFiGVkmSJElSYhlaJUmSJEmJZWiVJEmSJCWWoVWSJEmSlFiGVkmSJElSYhlaJUmSJEmJZWiVJEmSJCWWoVWSJEmSlFiGVkmSJElSYhlaJUmSJEmJZWiVJEmSJCWWoVWSJEmSlFiGVkmSJElSYhlaJUmSJEmJZWiVJEmSJCWWoVWSJEmSlFiGVkmSJElSYhlaJUmSJEmJZWiVJEmSJCVWiDGmu4YjEkL4BPhDuus4QqcAW9NdhA7JMUo+xyj5HKPkc4ySzzFKPsco+Ryj5DuSMeoYYzy1uo5jJrQeS0IIK2OMeemuQzVzjJLPMUo+xyj5HKPkc4ySzzFKPsco+Wo7Rl4eLEmSJElKLEOrJEmSJCmxDK3147F0F6DDcoySzzFKPsco+Ryj5HOMks8xSj7HKPlqNUbe0ypJkiRJSizPtEqSJEmSEsvQWkshhHtDCBtCCGtCCAtDCCdV6ZsRQtgUQtgYQhhZpX1Uqm1TCOHWtBR+HAkhXB5CeDeEsC+EkFelvVMI4csQQmHq79EqfbkhhHdSY/RgCCGkp/rjQ01jlOrzOEqYEMIdIYQPqxw736rSV+14qeF5jCRTCKE49flSGEJYmWo7OYTwWgihKPVv23TXeTwJIcwLIWwJIayt0lbtmIQKD6aOqzUhhL7pq/z4UcMY+VmUICGEM0MI/xVCWJf6Tjc11V4nx5KhtfZeA3rFGLOB3wMzAEIIPYArgZ7AKGBuCCEjhJABPAJcDPQArkotq/qzFhgPvFFN3+YYY5/U3+Qq7f8JfB/okvobVf9lHteqHSOPo0S7v8qx8wuoebzSWeTxymMk8Yaljp39/0l3K/DrGGMX4Nep52o48zn4c76mMbmYP383uI6K7wuqf/Op/ruYn0XJUQbcEmPsAQwA/i41FnVyLBlaaynGuDjGWJZ6uhw4I/X4UuDZGOOeGOP/AJuA/qm/TTHG92KMXwHPppZVPYkxro8xbjzS5UMI7YETY4zLY8VN308AY+urPh1yjDyOji01jZcansfIseVS4Cepxz/Bz5wGFWN8A9j2F801jcmlwBOxwnLgpNT3BtWjGsaoJn4WpUGMsSTGuCr1eAewHuhAHR1Lhta6dS3wy9TjDsD7Vfo+SLXV1K706BxCWB1CeD2EMDjV1oGKcdnPMUofj6PkujF1Oc+8KpcyOi7J4VgkVwQWhxAKQgjXpdr+KsZYknr8J+Cv0lOaqqhpTDy2ksXPogQKIXQCcoD/po6OpaZ1X2bjE0JYApxeTdesGOPPU8vMouK0+FMNWZsqHMkYVaMEOCvG+GkIIRd4KYTQs96KPM59zTFSmhxqvKi4hOduKr583w38Xyr+007S4Z0fY/wwhHAa8FoIYUPVzhhjDCH40w4J4pgklp9FCRRCaAX8DLgpxvh51WlhanMsGVqPQIzxm4fqDyFMAkYDF8U//4bQh8CZVRY7I9XGIdr1NR1ujGpYZw+wJ/W4IISwGehKxXicUWVRx6gOfJ0xwuMobY50vEIIPwZeST091HipYTkWCRVj/DD175YQwkIqLlv8OITQPsZYkro8bktaixTUPCYeWwkRY/x4/2M/i5IhhNCMisD6VIzxxVRznRxLXh5cSyGEUcA0YEyM8YsqXS8DV4YQWoQQOlNxk3E+sALoEkLoHEJoTsWN4i83dN2CEMKp+2/MDyGcTcUYvZe6hOHzEMKAUPHfQ9cAnglMD4+jBPqLe07GUTGRFtQ8Xmp4HiMJFEI4IYTQev9jYAQVx8/LwMTUYhPxMycJahqTl4FrUjOfDgBKq1z6qAbkZ1GypL4zPw6sjzHeV6WrTo4lz7TW3sNACyou8QFYHmOcHGN8N4TwPLCOisuG/y7GWA4QQrgR+BWQAcyLMb6bntKPDyGEccBDwKnAqyGEwhjjSGAIcFcIYS+wD5gcY9x/k/8UKmaq+wYV9yn/8qANq87UNEYeR4n1HyGEPlRcklUMXA9wqPFSw4oxlnmMJNJfAQtT3xeaAk/HGP9fCGEF8HwI4f8AfwCuSGONx50QwjPAUOCUEMIHwGzgHqofk18A36Jicp8vgP/d4AUfh2oYo6F+FiXKIOB/Ae+EEApTbTOpo2Mp/PlqVkmSJEmSksXLgyVJkiRJiWVolSRJkiQllqFVkiRJkpRYhlZJkiRJUmIZWiVJkiRJiWVolSRJkiQllqFVkiRJkpRYhlZJkiRJUmL9f4q8Go0kXtUpAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1152x1152 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels, tokens = get_tokens_and_labels_emb(embedding_words, popular_words)\n",
    "tsne_fit_and_plot(tokens=tokens, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE Dimensionality reduction\n",
    "As we can see, reducing a 100 dimension vector so much doesn't seem to make much sense, and thus it's better to look at the words closest to target word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance from 'weather':\n",
      "0.000 - weather\n",
      "0.292 - rain\n",
      "0.363 - cold\n",
      "0.367 - sunny\n",
      "0.394 - humid\n",
      "0.402 - forecast\n",
      "0.406 - temperature\n",
      "0.410 - meteorological\n",
      "0.423 - cool\n",
      "0.436 - hot\n"
     ]
    }
   ],
   "source": [
    "print_sorted_words('weather', metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance from 'classification':\n",
      "0.000 - classification\n",
      "0.497 - type\n",
      "0.525 - system\n",
      "0.530 - configuration\n",
      "0.556 - proper\n",
      "0.557 - course\n",
      "0.559 - humid\n",
      "0.572 - race\n",
      "0.576 - list\n",
      "0.578 - different\n"
     ]
    }
   ],
   "source": [
    "print_sorted_words('classification', metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance from 'programmer':\n",
      "0.000 - programmer\n",
      "0.383 - software\n",
      "0.430 - engineer\n",
      "0.475 - developer\n",
      "0.499 - creator\n",
      "0.505 - scientist\n",
      "0.518 - setup\n",
      "0.519 - designer\n",
      "0.532 - master\n",
      "0.551 - microsoft\n"
     ]
    }
   ],
   "source": [
    "print_sorted_words('programmer', metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
